{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set your user name as a case, pointing to your path to documents and tinyir.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "// set your case once\n",
    "val (doc_dir: String, files_path: String) = System.getProperties().get(\"user.name\").toString match {\n",
    "    case \"Yarden-\"  => (\"../documents\", \"../\")\n",
    "    case \"Max\"  => (\"/Users/Max/Coding/ETH/Information_Retrieval_AS16/scala_practice/documents\", \"/Users/Max/Coding/ETH/Information_Retrieval_AS16/scala_practice/files/\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classpath.addPath(files_path + \"tinyir-1.1.jar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import scala.xml.XML\n",
    "import ch.ethz.dal.tinyir._\n",
    "import com.github.aztek.porterstemmer.PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scala.io.Source  // for importing txt files\n",
    "import java.io._  // for saving txt files\n",
    "// import scala.collection.mutable.HashMap  //HashMap used for counting elements in linear time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// import scala.util.Random\n",
    "import scala.collection.mutable.{Map => MutMap, HashMap => MutHashMap}\n",
    "// enables \"mutable lists\"\n",
    "// import scala.collection.mutable.ListBuffer  \n",
    "import scala.collection.mutable.{Set => MutSet}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val timeit = new util.StopWatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def token_filter(text_body: String) = {\n",
    "    processing.StopWords.filterOutSW(\n",
    "        processing.Tokenizer.tokenize(text_body.\n",
    "                                      replaceAll(\"\\\\P{L}+\", \" \"))\n",
    "    ).\n",
    "    map(x => PorterStemmer.stem(x)).filter(_.trim.nonEmpty).toList\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class xml_doc (file_path: String) {\n",
    "    def get_doc(): xml.Elem = {\n",
    "        XML.loadFile(file_path: String)\n",
    "    }    \n",
    "    \n",
    "    def text() = {\n",
    "        (get_doc() \\\\ \"DOC\" \\\\ \"TEXT\").text\n",
    "    }\n",
    "    \n",
    "    def head() = {\n",
    "        (get_doc() \\\\ \"DOC\" \\\\ \"HEAD\").text\n",
    "    }\n",
    "\n",
    "    def id() = {\n",
    "        (get_doc() \\\\ \"DOC\" \\\\ \"DOCNO\").text.trim\n",
    "    }\n",
    "    \n",
    "    def tokens() = {\n",
    "        token_filter(head() ++ text())\n",
    "    }\n",
    "    \n",
    "    def hash_tokens() = {\n",
    "        tokens().map(x => x.hashCode())\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def list_docs (path: String) = {  // : Array[java.io.File]\n",
    "        new java.io.File(path).listFiles.map(x => x.toString())\n",
    "    }\n",
    "val numPattern = \"[0-9]+\".r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val token_hash = MutHashMap[String, Int]() // token -> hash\n",
    "\n",
    "def create_hash_doc_subset(star_count: Int, end_count: Int,\n",
    "                           file_list: Array[String],\n",
    "                           token_hash_map: MutHashMap[String, Int] = token_hash) = {\n",
    "    val id_htoken = MutHashMap[Int, List[Int]]() // forward index, docID to tokens\n",
    "    val htoken_id = MutHashMap[Int, List[Int]]()  // inverse index, tokens to docID\n",
    "    val id_name = MutHashMap[Int, String]()  // inverse index, tokens to docID\n",
    "    val name_id = MutHashMap[String, Int]()  // inverse index, tokens to docID\n",
    "    var counter = star_count\n",
    "    while (counter < end_count){\n",
    "        var cur_doc = new xml_doc(file_list(counter))\n",
    "        // get token from XML, then hash, or create hashes \"on the fly\"\n",
    "        var cur_htoken = cur_doc.tokens.map(x => token_hash_map.getOrElseUpdate(x, token_hash_map.size))\n",
    "        id_htoken += counter -> cur_htoken\n",
    "        \n",
    "        // update the inverse mapping, from (hashed) tokens to docID\n",
    "        cur_htoken.distinct.foreach(\n",
    "            (token: Int) => htoken_id(token) = htoken_id.getOrElseUpdate(token, List[Int]()) ++ List(counter)\n",
    "        )\n",
    "        \n",
    "        id_name(counter) = cur_doc.id\n",
    "        name_id(cur_doc.id) = counter\n",
    "        \n",
    "        counter += 1\n",
    "        if (counter % 100 == 0) println(s\"iteration $counter\")\n",
    "    }\n",
    "    (id_htoken, htoken_id, token_hash_map, id_name, name_id)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def write_int_to_intList(data: MutHashMap[Int, List[Int]], filename: String) = {\n",
    "\n",
    "    val bw = new BufferedWriter(new FileWriter(new File(filename)))\n",
    "    val iter = data.keys.iterator\n",
    "    while(iter.hasNext){\n",
    "        var elem = iter.next()\n",
    "        var values = data(elem).toList\n",
    "//         if(values.length>0){\n",
    "            bw.write(elem+\" \"+values.mkString(\" \"))\n",
    "            bw.newLine\n",
    "//         }    \n",
    "    }   \n",
    "    bw.close()\n",
    "}\n",
    "\n",
    "def write_int_string(data: MutHashMap[Int, String], filename: String) = {\n",
    "\n",
    "    val bw = new BufferedWriter(new FileWriter(new File(filename)))\n",
    "    val iter = data.keys.iterator\n",
    "    while(iter.hasNext){\n",
    "        var elem = iter.next()\n",
    "        var values = data(elem).toList\n",
    "        if(values.length>0){\n",
    "            bw.write(elem+\" \"+values.mkString(\"\"))\n",
    "            bw.newLine\n",
    "        }    \n",
    "    }   \n",
    "    bw.close()\n",
    "}\n",
    "\n",
    "def write_string_int(data: MutHashMap[String, Int], filename: String) = {\n",
    "\n",
    "    val bw = new BufferedWriter(new FileWriter(new File(filename)))\n",
    "    val iter = data.keys.iterator\n",
    "    while(iter.hasNext){\n",
    "        var elem = iter.next()\n",
    "        bw.write(elem+\" \"+data(elem).toString)\n",
    "        bw.newLine\n",
    "    }   \n",
    "    bw.close()\n",
    "}\n",
    "\n",
    "def write_int_to_int(data: MutHashMap[Int, Int], filename: String) = {\n",
    "\n",
    "    val bw = new BufferedWriter(new FileWriter(new File(filename)))\n",
    "    val iter = data.keys.iterator\n",
    "    while(iter.hasNext){\n",
    "        var elem = iter.next()\n",
    "        var value = data(elem)\n",
    "            bw.write(elem+\" \"+value)\n",
    "            bw.newLine\n",
    "    }   \n",
    "    bw.close()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_mutmap_int_intList(path: String, mutmap: MutHashMap[Int, List[Int]]) = {\n",
    "    val lines = Source.fromFile(path).getLines.toList\n",
    "    for (line <- lines){\n",
    "        val line_split = line.split(\" \", -1).filter(_.trim.length > 0)\n",
    "        mutmap(line_split.head.toInt) = \n",
    "            line_split.tail.map(x => x.toInt).toList\n",
    "    }\n",
    "}\n",
    "\n",
    "def load_mutmap_int_string(path: String, mutmap: MutHashMap[Int, String]) = {\n",
    "    val lines = Source.fromFile(path).getLines.toList\n",
    "    for (line <- lines){\n",
    "        val line_split = line.split(\" \") // .filter(_.trim.length > 0)\n",
    "        mutmap(line_split.head.toInt) = \n",
    "            line_split.last\n",
    "    }\n",
    "}\n",
    "\n",
    "def load_mutmap_string_int(path: String, mutmap: MutHashMap[String, Int]) = {\n",
    "    val lines = Source.fromFile(path).getLines.toList\n",
    "    for (line <- lines){\n",
    "//         val line_split = line.split(\" \", -1)\n",
    "        val line_split = line.split(\" \") // .filter(_.trim.length > 0)\n",
    "        mutmap(line_split.head) = \n",
    "            line_split.last.toInt\n",
    "    }\n",
    "}\n",
    "\n",
    "def load_mutmap_int_int(path: String, mutmap: MutHashMap[Int, Int]) = {\n",
    "    val lines = Source.fromFile(path).getLines.toList\n",
    "    for (line <- lines){\n",
    "        val line_split = line.split(\" \", -1).filter(_.trim.length > 0)\n",
    "        mutmap(line_split.head.toInt) = \n",
    "            line_split.last.toInt\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val mb = 1024*1024\n",
    "val runtime = Runtime.getRuntime\n",
    "def print_memory() = {\n",
    "    println(s\"Used Memory:  \" + (runtime.totalMemory - runtime.freeMemory) / mb)\n",
    "    println(s\"Free Memory:  \" + runtime.freeMemory / mb)\n",
    "    println(s\"Total Memory: \" + runtime.totalMemory / mb)\n",
    "    println(s\"Max Memory:   \" + runtime.maxMemory / mb)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val train_list = list_docs(doc_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val PATH_id_htoken = files_path + \"id_htoken.txt\"\n",
    "val PATH_htoken_id = files_path + \"htoken_id.txt\"\n",
    "val PATH_id_name = files_path + \"id_name.txt\"\n",
    "val PATH_name_id = files_path + \"name_id.txt\"\n",
    "val PATH_token_hash = files_path + \"token_hash.txt\"\n",
    "\n",
    "val PATH_prun_htoken_collectfreq = files_path + \"prun_htoken_collectfreq.txt\"\n",
    "val PATH_prun_htoken_id = files_path + \"prun_htoken_id.txt\"\n",
    "val PATH_prun_id_htoken = files_path + \"prun_id_htoken.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data files and creating maps\n",
    "# # not run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "// time it\n",
    "timeit.start\n",
    "\n",
    "val (id_htoken, htoken_id, token_hash, \n",
    "     id_name, name_id) = create_hash_doc_subset(0, 100000, train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// time it\n",
    "timeit.uptonow / 60.0\n",
    "// 87.56585954758334 , in minutes with 6GB\n",
    "// 67.88821773650001 , in minutes with 7GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_memory()\n",
    "\n",
    "// Used Memory:  3981\n",
    "// Free Memory:  1814\n",
    "// Total Memory: 5796\n",
    "// Max Memory:   5796"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_int_to_intList(id_htoken, PATH_id_htoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_int_to_intList(htoken_id, PATH_htoken_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_int_string(id_name, PATH_id_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_string_int(name_id, PATH_name_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "write_string_int(token_hash, PATH_token_hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// time it\n",
    "timeit.start\n",
    "\n",
    "val id_htoken: MutHashMap[Int, List[Int]] = MutHashMap[Int, List[Int]]()\n",
    "val htoken_id: MutHashMap[Int, List[Int]] = MutHashMap[Int, List[Int]]()\n",
    "val id_name: MutHashMap[Int, String] = MutHashMap[Int, String]()\n",
    "val token_hash: MutHashMap[String, Int] = MutHashMap[String, Int]()\n",
    "val name_id: MutHashMap[String, Int] = MutHashMap[String, Int]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "load_mutmap_int_intList(PATH_id_htoken, id_htoken)\n",
    "load_mutmap_int_intList(PATH_htoken_id, htoken_id)\n",
    "load_mutmap_int_string(PATH_id_name, id_name)\n",
    "load_mutmap_string_int(PATH_token_hash, token_hash)\n",
    "load_mutmap_string_int(PATH_name_id, name_id)\n",
    "\n",
    "// confirm load successful\n",
    "// test_load_mutmap_id_htoken == id_htoken\n",
    "// test_load_mutmap_htoken_id == htoken_id\n",
    "// test_load_mutmap_id_name == id_name\n",
    "// test_load_mutmap_token_hash == token_hash\n",
    "// test_load_mutmap_name_id == name_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// time it\n",
    "timeit.uptonow / 60.0\n",
    "// 1.4485827969833334 , in minutes\n",
    "// 1.3195113773833334 , in minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_memory()\n",
    "\n",
    "// Used Memory:  3468\n",
    "// Free Memory:  649\n",
    "// Total Memory: 4117\n",
    "// Max Memory:   5461"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prune vocabulary, collection and document frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "// htoken_id.mapValues(v => v.length).size\n",
    "// 1356183\n",
    "// htoken_id.mapValues(v => v.length).filter(_._2 > 5 - 1).size\n",
    "// 176866\n",
    "// reduction factor of ~7.67\n",
    "\n",
    "val prun_threshold = 5\n",
    "val pruned_token_set = htoken_id.mapValues(v => v.length).\n",
    "    filter(_._2 > prun_threshold - 1).keys.toSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "// time it\n",
    "timeit.start\n",
    "\n",
    "val prun_htoken_collectfreq: MutHashMap[Int, Int] = \n",
    "    MutHashMap(\n",
    "        id_htoken.flatMap{ case (k,v) => v.filter(pruned_token_set.contains(_)) }.\n",
    "        groupBy(identity).mapValues(_.size)\n",
    "        .toSeq:_*)\n",
    "\n",
    "prun_htoken_collectfreq.size\n",
    "\n",
    "timeit.uptonow / 60.0\n",
    "// 7.0919255263  , in minutes\n",
    "// 0.39257662956666667 , in minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "// time it\n",
    "timeit.start\n",
    "\n",
    "val prun_htoken_id: MutHashMap[Int, List[Int]] = \n",
    "    MutHashMap(\n",
    "        htoken_id.filterKeys(\n",
    "            pruned_token_set.contains(_)\n",
    "        ).toSeq:_*)\n",
    "\n",
    "prun_htoken_id.size\n",
    "\n",
    "timeit.uptonow / 60.0\n",
    "// 0.012546176999999999 , in minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// time it\n",
    "timeit.start\n",
    "\n",
    "val prun_id_htoken: MutHashMap[Int, List[Int]] = \n",
    "    MutHashMap(\n",
    "//         id_htoken.flatMap{ case (k,v) => (k, v.filter(pruned_token_set.contains(_))) }.\n",
    "        id_htoken.mapValues{ v => v.filter(pruned_token_set.contains(_)) }.\n",
    "        toSeq:_*)\n",
    "\n",
    "prun_id_htoken.size\n",
    "\n",
    "timeit.uptonow / 60.0\n",
    "// 0.20076514550000002 , in minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save pruned results to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_int_to_int(prun_htoken_collectfreq, PATH_prun_htoken_collectfreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_int_to_intList(prun_htoken_id, PATH_prun_htoken_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_int_to_intList(prun_id_htoken, PATH_prun_id_htoken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load maps (pruned)\n",
    "## # start from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// time it\n",
    "timeit.start\n",
    "\n",
    "val prun_htoken_collectfreq: MutHashMap[Int, Int] = MutHashMap[Int, Int]()\n",
    "val prun_id_htoken: MutHashMap[Int, List[Int]] = MutHashMap[Int, List[Int]]()\n",
    "val prun_htoken_id: MutHashMap[Int, List[Int]] = MutHashMap[Int, List[Int]]()\n",
    "val id_name: MutHashMap[Int, String] = MutHashMap[Int, String]()\n",
    "val token_hash: MutHashMap[String, Int] = MutHashMap[String, Int]()\n",
    "val name_id: MutHashMap[String, Int] = MutHashMap[String, Int]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "load_mutmap_int_int(PATH_prun_htoken_collectfreq, prun_htoken_collectfreq)\n",
    "load_mutmap_int_intList(PATH_prun_id_htoken, prun_id_htoken)\n",
    "load_mutmap_int_intList(PATH_prun_htoken_id, prun_htoken_id)\n",
    "load_mutmap_int_string(PATH_id_name, id_name)\n",
    "load_mutmap_string_int(PATH_token_hash, token_hash)\n",
    "load_mutmap_string_int(PATH_name_id, name_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// time it\n",
    "timeit.uptonow / 60.0\n",
    "// 2.1530588158666664 , in minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_memory()\n",
    "\n",
    "// Used Memory:  2569\n",
    "// Free Memory:  1782\n",
    "// Total Memory: 4352\n",
    "// Max Memory:   5461"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Queries & Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "// requires: having added tinyir to classpath, having added the qrels, i.e. \"relevance-judgements.csv\" in root \n",
    "// builds truth, an object, whose only method .judgements(\"query-ID\") returns the set of all document-IDs deemed \n",
    "// relevant to that query, note that these document-IDs are provided as List[String]\n",
    "// observe that query-ID is a string of an integer between 51 and 90 -> 40 queries in total\n",
    "import ch.ethz.dal.tinyir.lectures._\n",
    "val truth = new TipsterGroundTruth(files_path + \"/relevance-judgements.csv\")\n",
    "\n",
    "// how to use it, example:\n",
    "truth.judgements(\"51\")\n",
    "// observe that the size of relevant documents varies between queries, with the minimum being 52 and the maximum 894\n",
    "truth.judgements.values.map(x => x.size).min\n",
    "truth.judgements.values.map(x => x.size).max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// requires: having added the file \"questions-descriptions.txt\" to source\n",
    "// This cell will build a list (can be Stream if required) of query tokens. \n",
    "// Note that the 16 is hard-coded to ignore the first 15 characters of these <title> line, which all read \n",
    "// \"<title> Topic: \"\n",
    "import scala.io.Source\n",
    "val numPattern = \"[0-9]+\".r\n",
    "\n",
    "val title = Source.fromFile(files_path +\"questions-descriptions.txt\").getLines().filter(_.startsWith(\"<title>\"))\n",
    "                .map(_.substring(16).trim).map(x => token_filter(x)).toList\n",
    "\n",
    "val num = Source.fromFile(files_path +\"questions-descriptions.txt\").getLines().filter(_.startsWith(\"<num>\"))   \n",
    "                .map(x => numPattern.findFirstIn(x.toString).get.substring(1)).toList\n",
    "\n",
    "val query = num zip title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Term-Frequency Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// DEFINE AUXILLARY FUNCTIONS\n",
    "\n",
    "// get inverse-document frequency (idf)\n",
    "// is defined as the logarithmically scaled inverse fraction of the documents that contain the word, \n",
    "// obtained by dividing the total number of documents by the number of documents containing the term, and then \n",
    "// taking the logarithm of that quotient.\n",
    "\n",
    "def hash_query(query: (String, List[String])) = {\n",
    "    (query._1, query._2.map(x => token_hash.getOrElse(x,-1)).filter(prun_htoken_id.keys.toSet.contains(_)).toSet)\n",
    "}\n",
    "\n",
    "val corpus_size = prun_id_htoken.size\n",
    "def get_idf(query: Set[Int]) = {\n",
    "    query.map(x => x -> Math.log(corpus_size / prun_htoken_id(x).size)).toMap\n",
    "}\n",
    "\n",
    "// get term frequency in a specific document (doc)\n",
    "def get_tf(query: Set[Int],doc: Int) = {\n",
    "    prun_id_htoken(doc).filter(query.contains(_)).groupBy(identity).mapValues(_.size)\n",
    "}\n",
    "\n",
    "// get tf-idf is defined as tf-idf = tf * idf\n",
    "def get_tf_idf(query: Set[Int],doc:Int) = {\n",
    "    get_tf(query,doc).map(x => x._1 -> x._2 * get_idf(query).getOrElse(x._1,0.toDouble))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Handle a Query --> take in a query, produce a ranking\n",
    "def handle(query: (String, List[String])) = {\n",
    "    val hashed_query = hash_query(query)\n",
    "    val doc_set = hashed_query._2.map(x => prun_htoken_id(x)).flatten.toSet\n",
    "    val ranking = doc_set.map(x => x -> get_tf_idf(hashed_query._2,x).values.sum).toSeq.sortBy(-_._2)\n",
    "                    .take(100).map(x => x._1).toList    \n",
    "    (query._1,ranking)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// test it out.\n",
    "handle(query(35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Does it work on mass-answering queries?\n",
    "// It takes nearly 8 minutes though, so about 12 seconds per query on average. \n",
    "// Potential speed improvements: Write a function that reduces document collection in the first place.\n",
    "timeit.start\n",
    "val answers = query.map(x => handle(x))\n",
    "timeit.uptonow / 60.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This was used to generate the code (trial-and-error traces) - have a look in case you want to understand some functions better, but in theory it can be deleted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// it works!\n",
    "\n",
    "for(i <- query){\n",
    "println(i._1)\n",
    "handle(i)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// trouble-maker 33 -> 84 and 36 -> 87\n",
    "query(33)\n",
    "query(36)\n",
    "query(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "token_hash(\"inanci\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "token_hash.filter(prun_htoken_id.keys.toSet.contains(_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prun_htoken_id.keys.toSet.contains(282592)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "token_hash(\"ltern\") // these two words are not in the seen vocabulary (pruned)\n",
    "token_hash(\"rimin\") // these two words are not in the seen vocabulary (pruned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// now it works despite, gives -1 to unseen tokens as a hash, then filters out all negative hashes.\n",
    "hash_query(query(33))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Get documents for a query. Let's consider a sample query for trial-and-error purposes. Convert to hashes\n",
    "val squery = (query(3)._1, query(3)._2.map(x => token_hash(x)).toSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// retrieve document set as a union\n",
    "val doc_set = squery._2.map(x => prun_htoken_id(x)).flatten.toSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// for each document in the set considered, create a map of token -> tf-idf\n",
    "doc_set.map(x => x -> get_tf_idf(squery._2,x)).toMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_idf(squery._2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_tf(squery._2,12117)\n",
    "\n",
    "get_tf_idf(squery._2,12117)\n",
    "\n",
    "get_tf_idf(squery._2,12117).values.sum\n",
    "\n",
    "get_tf(squery._2,12117).map(x => x._1 -> x._2 * get_idf(squery._2).getOrElse(x._1,0.toDouble))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language model score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing ground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Used Memory:  400\n",
    "// Free Memory:  215\n",
    "// Total Memory: 616\n",
    "// Max Memory:   3641"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val token_hm = MutHashMap[String, Int]()\n",
    "List(\"word1\", \"word3\").map(x => token_hm.getOrElseUpdate(x, token_hm.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classpath.addPath(tiny_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trait Result[T] extends Any {\n",
    "    def id : Int\n",
    "    def matches(that: T) : Int                 \n",
    "    def isMatch(that: T) = matches(that)==0\n",
    "    def matched(that: T) : T    \n",
    "}\n",
    "\n",
    "object InvertedIndex {\n",
    "    // generic list intersection (does not require sorted lists)\n",
    "    private def unsortedIntersect [A<% Result[A]](l1: List[A], l2: List[A]) = l1.intersect(l2)\n",
    "\n",
    "    // optimized list intersection for sorted posting lists \n",
    "    // uses \"matches\" and \"matched\" methods to work for all posting types\n",
    "    def sIntersect[A <% Result[A]] (l1: List[A], l2: List[A]) : List[A] = {\n",
    "        @annotation.tailrec\n",
    "        def iter (l1: List[A], l2: List[A], result: List[A]) : List[A] = {\n",
    "            if (l1.isEmpty || l2.isEmpty) \n",
    "                result.reverse\n",
    "            else (l1.head matches l2.head) match {\n",
    "                case n if n>0 => iter(l1, l2.tail,result)  // advance list l2\n",
    "                case n if n<0 => iter(l1.tail, l2,result)  // advance list l1\n",
    "                case _        => iter(l1.tail, l2.tail, (l1.head matched l2.head)::result)\t      \n",
    "            }\n",
    "        }    \n",
    "        iter(l1,l2,Nil)      \n",
    "    }\n",
    "}\n",
    "\n",
    "abstract class InvertedIndex[Res <% Result[Res]]  {\n",
    "    def results (term: String) : List[Res] \n",
    "    def results (terms: Seq[String]) : List[Res] = {\n",
    "        val resultLists      = terms.map(term => results(term))\n",
    "        val shortToLongLists = resultLists.sortWith( _.length < _.length) \n",
    "        shortToLongLists.reduceLeft( (l1,l2) => InvertedIndex.sIntersect(l1,l2) )\n",
    "    }\n",
    "}\n",
    "\n",
    "// import ch.ethz.dal.tinyir.indexing.InvertedIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scala.math._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Document(val id: Int, val tokens: List[Int])\n",
    "//     def id: Int = this.id\n",
    "//     def tokens: List[Int] = this.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "case class ProxResult(val id: Int, val lpos: Int, val rpos: Int) extends Result[ProxResult] {\n",
    "    def matches(that: ProxResult) : Int = {    \n",
    "        if (this.id != that.id) this.id - that.id\n",
    "        else if ((max(rpos,that.rpos) - min(lpos,that.lpos)) <= ProxWindow.size) 0 // match\n",
    "        else this.lpos-that.lpos  // advance in list with the minimal lpos\n",
    "    }\n",
    "    def matched(that: ProxResult) = \n",
    "        ProxResult(id, min(this.lpos,that.lpos), max(this.rpos,that.rpos))\n",
    "}\n",
    "\n",
    "object ProxWindow {\n",
    "    var size = 1\n",
    "    def setSize(w: Int) {assert(w>=1); size = w}\n",
    "}\n",
    "\n",
    "class PosIndex (docs: Stream[Document]) extends InvertedIndex[ProxResult] {\n",
    "\n",
    "    case class PosPosting(val id: Int, val pos: Int) extends Ordered[PosPosting] {\n",
    "        def this(t: PosTuple) = this(t.id, t.pos) \n",
    "//         def compare(that: PosPosting) = Ordering[Tuple2[Int, Int]].compare((this.id, this.pos), (that.id, that.pos) ) \n",
    "    }\n",
    "    type PostList = List[PosPosting]\n",
    "    val index : Map[String, PostList] = {\n",
    "        val groupedPostings = postings(docs).groupBy(_.term)\n",
    "        groupedPostings.mapValues(_.map(p => PosPosting(p.id,p.pos)).sorted)\n",
    "    }\n",
    "  \n",
    "    case class PosTuple(term: String, id: Int, pos: Int) \n",
    "    def postings (s: Stream[Document]): List[PosTuple] =\n",
    "        s.flatMap( d => d.tokens.zipWithIndex.map{ case (tk,pos) => PosTuple(tk,d.ID,pos) } ).toList\n",
    "\n",
    "    override def results (word: String) : List[ProxResult] = \n",
    "        index.getOrElse(word,null).map(p => ProxResult(p.id, p.pos, p.pos))\n",
    "    override def results (terms: Seq[String]) : List[ProxResult] = results(terms,1)\n",
    "    def results (terms: Seq[String], win: Int) : List[ProxResult] = {\n",
    "        val resultLists = terms.map(term => results(term))\n",
    "        val shortToLongLists = resultLists.sortWith( _.length < _.length)   \n",
    "        shortToLongLists.reduceLeft( (l1,l2) => InvertedIndex.sIntersect(l1,l2) )\n",
    "    } \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Scala 2.11",
   "language": "scala211",
   "name": "scala211"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
