{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set your user name as a case, pointing to your path to documents and tinyir.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdoc_dir\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"/Users/Max/Coding/ETH/Information_Retrieval_AS16/scala_practice/documents\"\u001b[0m\n",
       "\u001b[36mfiles_path\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"/Users/Max/Coding/ETH/Information_Retrieval_AS16/scala_practice/files/\"\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// set your case once\n",
    "val (doc_dir: String, files_path: String) = System.getProperties().get(\"user.name\").toString match {\n",
    "    case \"Yarden-\"  => (\"../documents\", \"../\")\n",
    "    case \"Max\"  => (\"/Users/Max/Coding/ETH/Information_Retrieval_AS16/scala_practice/documents\", \"/Users/Max/Coding/ETH/Information_Retrieval_AS16/scala_practice/files/\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classpath.addPath(files_path + \"tinyir-1.1.jar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36mscala.xml.XML\u001b[0m\n",
       "\u001b[32mimport \u001b[36mch.ethz.dal.tinyir._\u001b[0m\n",
       "\u001b[32mimport \u001b[36mcom.github.aztek.porterstemmer.PorterStemmer\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import scala.xml.XML\n",
    "import ch.ethz.dal.tinyir._\n",
    "import com.github.aztek.porterstemmer.PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36mscala.io.Source\u001b[0m\n",
       "\u001b[32mimport \u001b[36mjava.io._\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import scala.io.Source  // for importing txt files\n",
    "import java.io._  // for saving txt files\n",
    "// import scala.collection.mutable.HashMap  //HashMap used for counting elements in linear time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36mscala.collection.mutable.{Map => MutMap, HashMap => MutHashMap}\u001b[0m\n",
       "\u001b[32mimport \u001b[36mscala.collection.mutable.{Set => MutSet}\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// import scala.util.Random\n",
    "import scala.collection.mutable.{Map => MutMap, HashMap => MutHashMap}\n",
    "// enables \"mutable lists\"\n",
    "// import scala.collection.mutable.ListBuffer  \n",
    "import scala.collection.mutable.{Set => MutSet}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mtimeit\u001b[0m: \u001b[32mutil\u001b[0m.\u001b[32mStopWatch\u001b[0m = ch.ethz.dal.tinyir.util.StopWatch@4b520195"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val timeit = new util.StopWatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction \u001b[36mtoken_filter\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def token_filter(text_body: String) = {\n",
    "    processing.StopWords.filterOutSW(\n",
    "        processing.Tokenizer.tokenize(text_body.\n",
    "                                      replaceAll(\"\\\\P{L}+\", \" \"))\n",
    "    ).\n",
    "    map(x => PorterStemmer.stem(x)).filter(_.trim.nonEmpty).toList\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass \u001b[36mxml_doc\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class xml_doc (file_path: String) {\n",
    "    def get_doc(): xml.Elem = {\n",
    "        XML.loadFile(file_path: String)\n",
    "    }    \n",
    "    \n",
    "    def text() = {\n",
    "        (get_doc() \\\\ \"DOC\" \\\\ \"TEXT\").text\n",
    "    }\n",
    "    \n",
    "    def head() = {\n",
    "        (get_doc() \\\\ \"DOC\" \\\\ \"HEAD\").text\n",
    "    }\n",
    "\n",
    "    def id() = {\n",
    "        (get_doc() \\\\ \"DOC\" \\\\ \"DOCNO\").text.trim\n",
    "    }\n",
    "    \n",
    "    def tokens() = {\n",
    "        token_filter(head() ++ text())\n",
    "    }\n",
    "    \n",
    "    def hash_tokens() = {\n",
    "        tokens().map(x => x.hashCode())\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction \u001b[36mlist_docs\u001b[0m\n",
       "\u001b[36mnumPattern\u001b[0m: \u001b[32mscala\u001b[0m.\u001b[32mutil\u001b[0m.\u001b[32mmatching\u001b[0m.\u001b[32mRegex\u001b[0m = [0-9]+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def list_docs (path: String) = {  // : Array[java.io.File]\n",
    "        new java.io.File(path).listFiles.map(x => x.toString())\n",
    "    }\n",
    "val numPattern = \"[0-9]+\".r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mtoken_hash\u001b[0m: \u001b[32mcollection\u001b[0m.\u001b[32mmutable\u001b[0m.\u001b[32mHashMap\u001b[0m[\u001b[32mString\u001b[0m, \u001b[32mInt\u001b[0m] = \u001b[33mMap\u001b[0m()\n",
       "defined \u001b[32mfunction \u001b[36mcreate_hash_doc_subset\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val token_hash = MutHashMap[String, Int]() // token -> hash\n",
    "\n",
    "def create_hash_doc_subset(star_count: Int, end_count: Int,\n",
    "                           file_list: Array[String],\n",
    "                           token_hash_map: MutHashMap[String, Int] = token_hash) = {\n",
    "    val id_htoken = MutHashMap[Int, List[Int]]() // forward index, docID to tokens\n",
    "    val htoken_id = MutHashMap[Int, List[Int]]()  // inverse index, tokens to docID\n",
    "    val id_name = MutHashMap[Int, String]()  // inverse index, tokens to docID\n",
    "    val name_id = MutHashMap[String, Int]()  // inverse index, tokens to docID\n",
    "    var counter = star_count\n",
    "    while (counter < end_count){\n",
    "        var cur_doc = new xml_doc(file_list(counter))\n",
    "        // get token from XML, then hash, or create hashes \"on the fly\"\n",
    "        var cur_htoken = cur_doc.tokens.map(x => token_hash_map.getOrElseUpdate(x, token_hash_map.size))\n",
    "        id_htoken += counter -> cur_htoken\n",
    "        \n",
    "        // update the inverse mapping, from (hashed) tokens to docID\n",
    "        cur_htoken.distinct.foreach(\n",
    "            (token: Int) => htoken_id(token) = htoken_id.getOrElseUpdate(token, List[Int]()) ++ List(counter)\n",
    "        )\n",
    "        \n",
    "        id_name(counter) = cur_doc.id\n",
    "        name_id(cur_doc.id) = counter\n",
    "        \n",
    "        counter += 1\n",
    "        if (counter % 100 == 0) println(s\"iteration $counter\")\n",
    "    }\n",
    "    (id_htoken, htoken_id, token_hash_map, id_name, name_id)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction \u001b[36mwrite_int_to_intList\u001b[0m\n",
       "defined \u001b[32mfunction \u001b[36mwrite_int_string\u001b[0m\n",
       "defined \u001b[32mfunction \u001b[36mwrite_string_int\u001b[0m\n",
       "defined \u001b[32mfunction \u001b[36mwrite_int_to_int\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def write_int_to_intList(data: MutHashMap[Int, List[Int]], filename: String) = {\n",
    "\n",
    "    val bw = new BufferedWriter(new FileWriter(new File(filename)))\n",
    "    val iter = data.keys.iterator\n",
    "    while(iter.hasNext){\n",
    "        var elem = iter.next()\n",
    "        var values = data(elem).toList\n",
    "//         if(values.length>0){\n",
    "            bw.write(elem+\" \"+values.mkString(\" \"))\n",
    "            bw.newLine\n",
    "//         }    \n",
    "    }   \n",
    "    bw.close()\n",
    "}\n",
    "\n",
    "def write_int_string(data: MutHashMap[Int, String], filename: String) = {\n",
    "\n",
    "    val bw = new BufferedWriter(new FileWriter(new File(filename)))\n",
    "    val iter = data.keys.iterator\n",
    "    while(iter.hasNext){\n",
    "        var elem = iter.next()\n",
    "        var values = data(elem).toList\n",
    "        if(values.length>0){\n",
    "            bw.write(elem+\" \"+values.mkString(\"\"))\n",
    "            bw.newLine\n",
    "        }    \n",
    "    }   \n",
    "    bw.close()\n",
    "}\n",
    "\n",
    "def write_string_int(data: MutHashMap[String, Int], filename: String) = {\n",
    "\n",
    "    val bw = new BufferedWriter(new FileWriter(new File(filename)))\n",
    "    val iter = data.keys.iterator\n",
    "    while(iter.hasNext){\n",
    "        var elem = iter.next()\n",
    "        bw.write(elem+\" \"+data(elem).toString)\n",
    "        bw.newLine\n",
    "    }   \n",
    "    bw.close()\n",
    "}\n",
    "\n",
    "def write_int_to_int(data: MutHashMap[Int, Int], filename: String) = {\n",
    "\n",
    "    val bw = new BufferedWriter(new FileWriter(new File(filename)))\n",
    "    val iter = data.keys.iterator\n",
    "    while(iter.hasNext){\n",
    "        var elem = iter.next()\n",
    "        var value = data(elem)\n",
    "            bw.write(elem+\" \"+value)\n",
    "            bw.newLine\n",
    "    }   \n",
    "    bw.close()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction \u001b[36mload_mutmap_int_intList\u001b[0m\n",
       "defined \u001b[32mfunction \u001b[36mload_mutmap_int_string\u001b[0m\n",
       "defined \u001b[32mfunction \u001b[36mload_mutmap_string_int\u001b[0m\n",
       "defined \u001b[32mfunction \u001b[36mload_mutmap_int_int\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_mutmap_int_intList(path: String, mutmap: MutHashMap[Int, List[Int]]) = {\n",
    "    val lines = Source.fromFile(path).getLines.toList\n",
    "    for (line <- lines){\n",
    "        val line_split = line.split(\" \", -1).filter(_.trim.length > 0)\n",
    "        mutmap(line_split.head.toInt) = \n",
    "            line_split.tail.map(x => x.toInt).toList\n",
    "    }\n",
    "}\n",
    "\n",
    "def load_mutmap_int_string(path: String, mutmap: MutHashMap[Int, String]) = {\n",
    "    val lines = Source.fromFile(path).getLines.toList\n",
    "    for (line <- lines){\n",
    "        val line_split = line.split(\" \") // .filter(_.trim.length > 0)\n",
    "        mutmap(line_split.head.toInt) = \n",
    "            line_split.last\n",
    "    }\n",
    "}\n",
    "\n",
    "def load_mutmap_string_int(path: String, mutmap: MutHashMap[String, Int]) = {\n",
    "    val lines = Source.fromFile(path).getLines.toList\n",
    "    for (line <- lines){\n",
    "//         val line_split = line.split(\" \", -1)\n",
    "        val line_split = line.split(\" \") // .filter(_.trim.length > 0)\n",
    "        mutmap(line_split.head) = \n",
    "            line_split.last.toInt\n",
    "    }\n",
    "}\n",
    "\n",
    "def load_mutmap_int_int(path: String, mutmap: MutHashMap[Int, Int]) = {\n",
    "    val lines = Source.fromFile(path).getLines.toList\n",
    "    for (line <- lines){\n",
    "        val line_split = line.split(\" \", -1).filter(_.trim.length > 0)\n",
    "        mutmap(line_split.head.toInt) = \n",
    "            line_split.last.toInt\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mmb\u001b[0m: \u001b[32mInt\u001b[0m = \u001b[32m1048576\u001b[0m\n",
       "\u001b[36mruntime\u001b[0m: \u001b[32mRuntime\u001b[0m = java.lang.Runtime@31294559\n",
       "defined \u001b[32mfunction \u001b[36mprint_memory\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val mb = 1024*1024\n",
    "val runtime = Runtime.getRuntime\n",
    "def print_memory() = {\n",
    "    println(s\"Used Memory:  \" + (runtime.totalMemory - runtime.freeMemory) / mb)\n",
    "    println(s\"Free Memory:  \" + runtime.freeMemory / mb)\n",
    "    println(s\"Total Memory: \" + runtime.totalMemory / mb)\n",
    "    println(s\"Max Memory:   \" + runtime.maxMemory / mb)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mtrain_list\u001b[0m: \u001b[32mArray\u001b[0m[\u001b[32mString\u001b[0m] = \u001b[33mArray\u001b[0m(\n",
       "  \u001b[32m\"/Users/Max/Coding/ETH/Information_Retrieval_AS16/scala_practice/documents/AP880212-0006\"\u001b[0m,\n",
       "  \u001b[32m\"/Users/Max/Coding/ETH/Information_Retrieval_AS16/scala_practice/documents/AP880212-0007\"\u001b[0m,\n",
       "  \u001b[32m\"/Users/Max/Coding/ETH/Information_Retrieval_AS16/scala_practice/documents/AP880212-0009\"\u001b[0m,\n",
       "  \u001b[32m\"/Users/Max/Coding/ETH/Information_Retrieval_AS16/scala_practice/documents/AP880212-0017\"\u001b[0m,\n",
       "  \u001b[32m\"/Users/Max/Coding/ETH/Information_Retrieval_AS16/scala_practice/documents/AP880212-0018\"\u001b[0m,\n",
       "  \u001b[32m\"/Users/Max/Coding/ETH/Information_Retrieval_AS16/scala_practice/documents/AP880212-0022\"\u001b[0m,\n",
       "  \u001b[32m\"/Users/Max/Coding/ETH/Information_Retrieval_AS16/scala_practice/documents/AP880212-0031\"\u001b[0m,\n",
       "  \u001b[32m\"/Users/Max/Coding/ETH/Information_Retrieval_AS16/scala_practice/documents/AP880212-0039\"\u001b[0m,\n",
       "  \u001b[32m\"/Users/Max/Coding/ETH/Information_Retrieval_AS16/scala_practice/documents/AP880212-0042\"\u001b[0m,\n",
       "  \u001b[32m\"/Users/Max/Coding/ETH/Information_Retrieval_AS16/scala_practice/documents/AP8\u001b[33m...\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val train_list = list_docs(doc_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mPATH_id_htoken\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"/Users/Max/Coding/ETH/Information_Retrieval_AS16/scala_practice/files/id_htoken.txt\"\u001b[0m\n",
       "\u001b[36mPATH_htoken_id\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"/Users/Max/Coding/ETH/Information_Retrieval_AS16/scala_practice/files/htoken_id.txt\"\u001b[0m\n",
       "\u001b[36mPATH_id_name\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"/Users/Max/Coding/ETH/Information_Retrieval_AS16/scala_practice/files/id_name.txt\"\u001b[0m\n",
       "\u001b[36mPATH_name_id\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"/Users/Max/Coding/ETH/Information_Retrieval_AS16/scala_practice/files/name_id.txt\"\u001b[0m\n",
       "\u001b[36mPATH_token_hash\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"/Users/Max/Coding/ETH/Information_Retrieval_AS16/scala_practice/files/token_hash.txt\"\u001b[0m\n",
       "\u001b[36mPATH_prun_htoken_collectfreq\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"/Users/Max/Coding/ETH/Information_Retrieval_AS16/scala_practice/files/prun_htoken_collectfreq.txt\"\u001b[0m\n",
       "\u001b[36mPATH_prun_htoken_id\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"/Users/Max/Coding/ETH/Information_Retrieval_AS16/scala_practice/files/prun_htoken_id.txt\"\u001b[0m\n",
       "\u001b[36mPATH_prun_id_htoken\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"/Users/Max/Coding/ETH/Information_Retrieval_AS16/scala_practice/files/prun_id_htoken.txt\"\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val PATH_id_htoken = files_path + \"id_htoken.txt\"\n",
    "val PATH_htoken_id = files_path + \"htoken_id.txt\"\n",
    "val PATH_id_name = files_path + \"id_name.txt\"\n",
    "val PATH_name_id = files_path + \"name_id.txt\"\n",
    "val PATH_token_hash = files_path + \"token_hash.txt\"\n",
    "\n",
    "val PATH_prun_htoken_collectfreq = files_path + \"prun_htoken_collectfreq.txt\"\n",
    "val PATH_prun_htoken_id = files_path + \"prun_htoken_id.txt\"\n",
    "val PATH_prun_id_htoken = files_path + \"prun_id_htoken.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Results to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction \u001b[36mwrite_res\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// use model=\"l\" to save predictions for langauge model, i.e. t and l are required for the final submission\n",
    "// but in theory can use anything for this parameter in case you want to save mulitple predictions in the same folder\n",
    "// while trying out some different scoring and ranking approaches. \n",
    "def write_res(res: Map[String, List[String]],model: String=\"t\") = {\n",
    "    val file = new BufferedWriter(new FileWriter(new File(\"ranking-\"+model+\"-24.txt\")))\n",
    "    res.foreach{case (qId,doclist) => doclist.zipWithIndex // takes each qID, doclist pair to zip the list with an index\n",
    "                .foreach{case(name,rank) => file.write(qId+\" \"+(rank+1)+\" \"+name+\"\\n\")}} // self-explanatory\n",
    "    file.close()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31mMain.scala:25: not found: value answers",
      "write_res(answers)",
      "          ^\u001b[0m",
      "\u001b[31mMain.scala:28: not found: value answers",
      "write_res(answers,\"l\")",
      "          ^\u001b[0m",
      "\u001b[31mMain.scala:31: not found: value answers",
      "write_res(answers,\"abc\") // saves answers with arbitrary name",
      "          ^\u001b[0m"
     ]
    }
   ],
   "source": [
    "// Example Use (obviously need to have computed answers first)\n",
    "write_res(answers) // saves answers term model predictions\n",
    "write_res(answers,\"l\") // saves answers as language model predictions\n",
    "write_res(answers,\"abc\") // saves answers with arbitrary name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data files and creating maps\n",
    "# # not run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "// time it\n",
    "timeit.start\n",
    "\n",
    "val (id_htoken, htoken_id, token_hash, \n",
    "     id_name, name_id) = create_hash_doc_subset(0, 100000, train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// time it\n",
    "timeit.uptonow / 60.0\n",
    "// 87.56585954758334 , in minutes with 6GB\n",
    "// 67.88821773650001 , in minutes with 7GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_memory()\n",
    "\n",
    "// Used Memory:  3981\n",
    "// Free Memory:  1814\n",
    "// Total Memory: 5796\n",
    "// Max Memory:   5796"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_int_to_intList(id_htoken, PATH_id_htoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_int_to_intList(htoken_id, PATH_htoken_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_int_string(id_name, PATH_id_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_string_int(name_id, PATH_name_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "write_string_int(token_hash, PATH_token_hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// time it\n",
    "timeit.start\n",
    "\n",
    "val id_htoken: MutHashMap[Int, List[Int]] = MutHashMap[Int, List[Int]]()\n",
    "val htoken_id: MutHashMap[Int, List[Int]] = MutHashMap[Int, List[Int]]()\n",
    "val id_name: MutHashMap[Int, String] = MutHashMap[Int, String]()\n",
    "val token_hash: MutHashMap[String, Int] = MutHashMap[String, Int]()\n",
    "val name_id: MutHashMap[String, Int] = MutHashMap[String, Int]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "load_mutmap_int_intList(PATH_id_htoken, id_htoken)\n",
    "load_mutmap_int_intList(PATH_htoken_id, htoken_id)\n",
    "load_mutmap_int_string(PATH_id_name, id_name)\n",
    "load_mutmap_string_int(PATH_token_hash, token_hash)\n",
    "load_mutmap_string_int(PATH_name_id, name_id)\n",
    "\n",
    "// confirm load successful\n",
    "// test_load_mutmap_id_htoken == id_htoken\n",
    "// test_load_mutmap_htoken_id == htoken_id\n",
    "// test_load_mutmap_id_name == id_name\n",
    "// test_load_mutmap_token_hash == token_hash\n",
    "// test_load_mutmap_name_id == name_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// time it\n",
    "timeit.uptonow / 60.0\n",
    "// 1.4485827969833334 , in minutes\n",
    "// 1.3195113773833334 , in minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_memory()\n",
    "\n",
    "// Used Memory:  3468\n",
    "// Free Memory:  649\n",
    "// Total Memory: 4117\n",
    "// Max Memory:   5461"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prune vocabulary, collection and document frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "// htoken_id.mapValues(v => v.length).size\n",
    "// 1356183\n",
    "// htoken_id.mapValues(v => v.length).filter(_._2 > 5 - 1).size\n",
    "// 176866\n",
    "// reduction factor of ~7.67\n",
    "\n",
    "val prun_threshold = 5\n",
    "val pruned_token_set = htoken_id.mapValues(v => v.length).\n",
    "    filter(_._2 > prun_threshold - 1).keys.toSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "// time it\n",
    "timeit.start\n",
    "\n",
    "val prun_htoken_collectfreq: MutHashMap[Int, Int] = \n",
    "    MutHashMap(\n",
    "        id_htoken.flatMap{ case (k,v) => v.filter(pruned_token_set.contains(_)) }.\n",
    "        groupBy(identity).mapValues(_.size)\n",
    "        .toSeq:_*)\n",
    "\n",
    "prun_htoken_collectfreq.size\n",
    "\n",
    "timeit.uptonow / 60.0\n",
    "// 7.0919255263  , in minutes\n",
    "// 0.39257662956666667 , in minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "// time it\n",
    "timeit.start\n",
    "\n",
    "val prun_htoken_id: MutHashMap[Int, List[Int]] = \n",
    "    MutHashMap(\n",
    "        htoken_id.filterKeys(\n",
    "            pruned_token_set.contains(_)\n",
    "        ).toSeq:_*)\n",
    "\n",
    "prun_htoken_id.size\n",
    "\n",
    "timeit.uptonow / 60.0\n",
    "// 0.012546176999999999 , in minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// time it\n",
    "timeit.start\n",
    "\n",
    "val prun_id_htoken: MutHashMap[Int, List[Int]] = \n",
    "    MutHashMap(\n",
    "//         id_htoken.flatMap{ case (k,v) => (k, v.filter(pruned_token_set.contains(_))) }.\n",
    "        id_htoken.mapValues{ v => v.filter(pruned_token_set.contains(_)) }.\n",
    "        toSeq:_*)\n",
    "\n",
    "prun_id_htoken.size\n",
    "\n",
    "timeit.uptonow / 60.0\n",
    "// 0.20076514550000002 , in minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save pruned results to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_int_to_int(prun_htoken_collectfreq, PATH_prun_htoken_collectfreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_int_to_intList(prun_htoken_id, PATH_prun_htoken_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_int_to_intList(prun_id_htoken, PATH_prun_id_htoken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load maps (pruned)\n",
    "## # start from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mprun_htoken_collectfreq\u001b[0m: \u001b[32mcollection\u001b[0m.\u001b[32mmutable\u001b[0m.\u001b[32mHashMap\u001b[0m[\u001b[32mInt\u001b[0m, \u001b[32mInt\u001b[0m] = \u001b[33mMap\u001b[0m()\n",
       "\u001b[36mprun_id_htoken\u001b[0m: \u001b[32mcollection\u001b[0m.\u001b[32mmutable\u001b[0m.\u001b[32mHashMap\u001b[0m[\u001b[32mInt\u001b[0m, \u001b[32mList\u001b[0m[\u001b[32mInt\u001b[0m]] = \u001b[33mMap\u001b[0m()\n",
       "\u001b[36mprun_htoken_id\u001b[0m: \u001b[32mcollection\u001b[0m.\u001b[32mmutable\u001b[0m.\u001b[32mHashMap\u001b[0m[\u001b[32mInt\u001b[0m, \u001b[32mList\u001b[0m[\u001b[32mInt\u001b[0m]] = \u001b[33mMap\u001b[0m()\n",
       "\u001b[36mid_name\u001b[0m: \u001b[32mcollection\u001b[0m.\u001b[32mmutable\u001b[0m.\u001b[32mHashMap\u001b[0m[\u001b[32mInt\u001b[0m, \u001b[32mString\u001b[0m] = \u001b[33mMap\u001b[0m()\n",
       "\u001b[36mtoken_hash\u001b[0m: \u001b[32mcollection\u001b[0m.\u001b[32mmutable\u001b[0m.\u001b[32mHashMap\u001b[0m[\u001b[32mString\u001b[0m, \u001b[32mInt\u001b[0m] = \u001b[33mMap\u001b[0m()\n",
       "\u001b[36mname_id\u001b[0m: \u001b[32mcollection\u001b[0m.\u001b[32mmutable\u001b[0m.\u001b[32mHashMap\u001b[0m[\u001b[32mString\u001b[0m, \u001b[32mInt\u001b[0m] = \u001b[33mMap\u001b[0m()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// time it\n",
    "timeit.start\n",
    "\n",
    "val prun_htoken_collectfreq: MutHashMap[Int, Int] = MutHashMap[Int, Int]()\n",
    "val prun_id_htoken: MutHashMap[Int, List[Int]] = MutHashMap[Int, List[Int]]()\n",
    "val prun_htoken_id: MutHashMap[Int, List[Int]] = MutHashMap[Int, List[Int]]()\n",
    "val id_name: MutHashMap[Int, String] = MutHashMap[Int, String]()\n",
    "val token_hash: MutHashMap[String, Int] = MutHashMap[String, Int]()\n",
    "val name_id: MutHashMap[String, Int] = MutHashMap[String, Int]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load_mutmap_int_int(PATH_prun_htoken_collectfreq, prun_htoken_collectfreq)\n",
    "load_mutmap_int_intList(PATH_prun_id_htoken, prun_id_htoken)\n",
    "load_mutmap_int_intList(PATH_prun_htoken_id, prun_htoken_id)\n",
    "load_mutmap_int_string(PATH_id_name, id_name)\n",
    "load_mutmap_string_int(PATH_token_hash, token_hash)\n",
    "load_mutmap_string_int(PATH_name_id, name_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres18\u001b[0m: \u001b[32mDouble\u001b[0m = \u001b[32m0.9153833900833334\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// time it\n",
    "timeit.uptonow / 60.0\n",
    "// 2.1530588158666664 , in minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Memory:  2549\n",
      "Free Memory:  1090\n",
      "Total Memory: 3641\n",
      "Max Memory:   3641\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_memory()\n",
    "\n",
    "// Used Memory:  2569\n",
    "// Free Memory:  1782\n",
    "// Total Memory: 4352\n",
    "// Max Memory:   5461"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Queries & Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36mch.ethz.dal.tinyir.lectures._\u001b[0m\n",
       "\u001b[36mtruth\u001b[0m: \u001b[32mlectures\u001b[0m.\u001b[32mTipsterGroundTruth\u001b[0m = ch.ethz.dal.tinyir.lectures.TipsterGroundTruth@221d8d6f\n",
       "\u001b[36mres20_2\u001b[0m: \u001b[32mArray\u001b[0m[\u001b[32mString\u001b[0m] = \u001b[33mArray\u001b[0m(\n",
       "  \u001b[32m\"AP8803010271\"\u001b[0m,\n",
       "  \u001b[32m\"AP8803020275\"\u001b[0m,\n",
       "  \u001b[32m\"AP8803110301\"\u001b[0m,\n",
       "  \u001b[32m\"AP8803160292\"\u001b[0m,\n",
       "  \u001b[32m\"AP8803180287\"\u001b[0m,\n",
       "  \u001b[32m\"AP8803250293\"\u001b[0m,\n",
       "  \u001b[32m\"AP8804060267\"\u001b[0m,\n",
       "  \u001b[32m\"AP8804070258\"\u001b[0m,\n",
       "  \u001b[32m\"AP8804120268\"\u001b[0m,\n",
       "  \u001b[32m\"AP8804280301\"\u001b[0m,\n",
       "  \u001b[32m\"AP8806270045\"\u001b[0m,\n",
       "  \u001b[32m\"AP8806270093\"\u001b[0m,\n",
       "  \u001b[32m\"AP8806280097\"\u001b[0m,\n",
       "  \u001b[32m\"AP8806280170\"\u001b[0m,\n",
       "  \u001b[32m\"AP8806280310\"\u001b[0m,\n",
       "  \u001b[32m\"AP8807060311\"\u001b[0m,\n",
       "  \u001b[32m\"AP8807310085\"\u001b[0m,\n",
       "  \u001b[32m\"AP8809220206\"\u001b[0m,\n",
       "  \u001b[32m\"AP8809260235\"\u001b[0m,\n",
       "\u001b[33m...\u001b[0m\n",
       "\u001b[36mres20_3\u001b[0m: \u001b[32mInt\u001b[0m = \u001b[32m52\u001b[0m\n",
       "\u001b[36mres20_4\u001b[0m: \u001b[32mInt\u001b[0m = \u001b[32m894\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// requires: having added tinyir to classpath, having added the qrels, i.e. \"relevance-judgements.csv\" in root \n",
    "// builds truth, an object, whose only method .judgements(\"query-ID\") returns the set of all document-IDs deemed \n",
    "// relevant to that query, note that these document-IDs are provided as List[String]\n",
    "// observe that query-ID is a string of an integer between 51 and 90 -> 40 queries in total\n",
    "import ch.ethz.dal.tinyir.lectures._\n",
    "val truth = new TipsterGroundTruth(files_path + \"/relevance-judgements.csv\")\n",
    "\n",
    "// how to use it, example:\n",
    "truth.judgements(\"51\")\n",
    "// observe that the size of relevant documents varies between queries, with the minimum being 52 and the maximum 894\n",
    "truth.judgements.values.map(x => x.size).min\n",
    "truth.judgements.values.map(x => x.size).max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36mscala.io.Source\u001b[0m\n",
       "\u001b[36mnumPattern\u001b[0m: \u001b[32mscala\u001b[0m.\u001b[32mutil\u001b[0m.\u001b[32mmatching\u001b[0m.\u001b[32mRegex\u001b[0m = [0-9]+\n",
       "\u001b[36mtitle\u001b[0m: \u001b[32mList\u001b[0m[\u001b[32mList\u001b[0m[\u001b[32mString\u001b[0m]] = \u001b[33mList\u001b[0m(\n",
       "  \u001b[33mList\u001b[0m(\u001b[32m\"airbu\"\u001b[0m, \u001b[32m\"subsidi\"\u001b[0m),\n",
       "  \u001b[33mList\u001b[0m(\u001b[32m\"south\"\u001b[0m, \u001b[32m\"african\"\u001b[0m, \u001b[32m\"sanction\"\u001b[0m),\n",
       "  \u001b[33mList\u001b[0m(\u001b[32m\"leverag\"\u001b[0m, \u001b[32m\"buyout\"\u001b[0m),\n",
       "  \u001b[33mList\u001b[0m(\u001b[32m\"satellit\"\u001b[0m, \u001b[32m\"launch\"\u001b[0m, \u001b[32m\"contract\"\u001b[0m),\n",
       "  \u001b[33mList\u001b[0m(\u001b[32m\"insid\"\u001b[0m, \u001b[32m\"trade\"\u001b[0m),\n",
       "  \u001b[33mList\u001b[0m(\u001b[32m\"prime\"\u001b[0m, \u001b[32m\"lend\"\u001b[0m, \u001b[32m\"rate\"\u001b[0m, \u001b[32m\"move\"\u001b[0m, \u001b[32m\"predict\"\u001b[0m),\n",
       "  \u001b[33mList\u001b[0m(\u001b[32m\"mci\"\u001b[0m),\n",
       "  \u001b[33mList\u001b[0m(\u001b[32m\"rail\"\u001b[0m, \u001b[32m\"strike\"\u001b[0m),\n",
       "  \u001b[33mList\u001b[0m(\u001b[32m\"weather\"\u001b[0m, \u001b[32m\"relat\"\u001b[0m, \u001b[32m\"fatal\"\u001b[0m),\n",
       "  \u001b[33mList\u001b[0m(\u001b[32m\"merit\"\u001b[0m, \u001b[32m\"pai\"\u001b[0m, \u001b[32m\"senior\"\u001b[0m),\n",
       "  \u001b[33mList\u001b[0m(\u001b[32m\"isra\"\u001b[0m, \u001b[32m\"role\"\u001b[0m, \u001b[32m\"iran\"\u001b[0m, \u001b[32m\"contra\"\u001b[0m, \u001b[32m\"affair\"\u001b[0m),\n",
       "  \u001b[33mList\u001b[0m(\u001b[32m\"militari\"\u001b[0m, \u001b[32m\"coup\"\u001b[0m, \u001b[32m\"etat\"\u001b[0m),\n",
       "  \u001b[33mList\u001b[0m(\u001b[32m\"machin\"\u001b[0m, \u001b[32m\"translat\"\u001b[0m),\n",
       "  \u001b[33mList\u001b[0m(\u001b[32m\"hostag\"\u001b[0m, \u001b[32m\"take\"\u001b[0m),\n",
       "  \u001b[33mList\u001b[0m(\u001b[32m\"inform\"\u001b[0m, \u001b[32m\"retriev\"\u001b[0m, \u001b[32m\"system\"\u001b[0m),\n",
       "  \u001b[33mList\u001b[0m(\u001b[32m\"natur\"\u001b[0m, \u001b[32m\"languag\"\u001b[0m, \u001b[32m\"process\"\u001b[0m),\n",
       "  \u001b[33mList\u001b[0m(\u001b[32m\"polit\"\u001b[0m, \u001b[32m\"motiv\"\u001b[0m, \u001b[32m\"civil\"\u001b[0m, \u001b[32m\"disturb\"\u001b[0m),\n",
       "  \u001b[33mList\u001b[0m(\u001b[32m\"health\"\u001b[0m, \u001b[32m\"hazard\"\u001b[0m, \u001b[32m\"fine\"\u001b[0m, \u001b[32m\"diamet\"\u001b[0m, \u001b[32m\"fiber\"\u001b[0m),\n",
       "  \u001b[33mList\u001b[0m(\u001b[32m\"attempt\"\u001b[0m, \u001b[32m\"reviv\"\u001b[0m, \u001b[32m\"salt\"\u001b[0m, \u001b[32m\"treati\"\u001b[0m),\n",
       "\u001b[33m...\u001b[0m\n",
       "\u001b[36mnum\u001b[0m: \u001b[32mList\u001b[0m[\u001b[32mString\u001b[0m] = \u001b[33mList\u001b[0m(\n",
       "  \u001b[32m\"51\"\u001b[0m,\n",
       "  \u001b[32m\"52\"\u001b[0m,\n",
       "  \u001b[32m\"53\"\u001b[0m,\n",
       "  \u001b[32m\"54\"\u001b[0m,\n",
       "  \u001b[32m\"55\"\u001b[0m,\n",
       "  \u001b[32m\"56\"\u001b[0m,\n",
       "  \u001b[32m\"57\"\u001b[0m,\n",
       "  \u001b[32m\"58\"\u001b[0m,\n",
       "  \u001b[32m\"59\"\u001b[0m,\n",
       "  \u001b[32m\"60\"\u001b[0m,\n",
       "  \u001b[32m\"61\"\u001b[0m,\n",
       "  \u001b[32m\"62\"\u001b[0m,\n",
       "  \u001b[32m\"63\"\u001b[0m,\n",
       "  \u001b[32m\"64\"\u001b[0m,\n",
       "  \u001b[32m\"65\"\u001b[0m,\n",
       "  \u001b[32m\"66\"\u001b[0m,\n",
       "  \u001b[32m\"67\"\u001b[0m,\n",
       "  \u001b[32m\"68\"\u001b[0m,\n",
       "  \u001b[32m\"69\"\u001b[0m,\n",
       "\u001b[33m...\u001b[0m\n",
       "\u001b[36mquery\u001b[0m: \u001b[32mList\u001b[0m[(\u001b[32mString\u001b[0m, \u001b[32mList\u001b[0m[\u001b[32mString\u001b[0m])] = \u001b[33mList\u001b[0m(\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"51\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"airbu\"\u001b[0m, \u001b[32m\"subsidi\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"52\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"south\"\u001b[0m, \u001b[32m\"african\"\u001b[0m, \u001b[32m\"sanction\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"53\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"leverag\"\u001b[0m, \u001b[32m\"buyout\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"54\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"satellit\"\u001b[0m, \u001b[32m\"launch\"\u001b[0m, \u001b[32m\"contract\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"55\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"insid\"\u001b[0m, \u001b[32m\"trade\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"56\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"prime\"\u001b[0m, \u001b[32m\"lend\"\u001b[0m, \u001b[32m\"rate\"\u001b[0m, \u001b[32m\"move\"\u001b[0m, \u001b[32m\"predict\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"57\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"mci\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"58\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"rail\"\u001b[0m, \u001b[32m\"strike\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"59\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"weather\"\u001b[0m, \u001b[32m\"relat\"\u001b[0m, \u001b[32m\"fatal\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"60\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"merit\"\u001b[0m, \u001b[32m\"pai\"\u001b[0m, \u001b[32m\"senior\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"61\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"isra\"\u001b[0m, \u001b[32m\"role\"\u001b[0m, \u001b[32m\"iran\"\u001b[0m, \u001b[32m\"contra\"\u001b[0m, \u001b[32m\"affair\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"62\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"militari\"\u001b[0m, \u001b[32m\"coup\"\u001b[0m, \u001b[32m\"etat\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"63\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"machin\"\u001b[0m, \u001b[32m\"translat\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"64\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"hostag\"\u001b[0m, \u001b[32m\"take\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"65\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"inform\"\u001b[0m, \u001b[32m\"retriev\"\u001b[0m, \u001b[32m\"system\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"66\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"natur\"\u001b[0m, \u001b[32m\"languag\"\u001b[0m, \u001b[32m\"process\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"67\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"polit\"\u001b[0m, \u001b[32m\"motiv\"\u001b[0m, \u001b[32m\"civil\"\u001b[0m, \u001b[32m\"disturb\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"68\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"health\"\u001b[0m, \u001b[32m\"hazard\"\u001b[0m, \u001b[32m\"fine\"\u001b[0m, \u001b[32m\"diamet\"\u001b[0m, \u001b[32m\"fiber\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"69\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"attempt\"\u001b[0m, \u001b[32m\"reviv\"\u001b[0m, \u001b[32m\"salt\"\u001b[0m, \u001b[32m\"treati\"\u001b[0m)),\n",
       "\u001b[33m...\u001b[0m\n",
       "\u001b[36mres21_5\u001b[0m: \u001b[32mList\u001b[0m[(\u001b[32mString\u001b[0m, \u001b[32mList\u001b[0m[\u001b[32mString\u001b[0m])] = \u001b[33mList\u001b[0m(\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"51\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"airbu\"\u001b[0m, \u001b[32m\"subsidi\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"52\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"south\"\u001b[0m, \u001b[32m\"african\"\u001b[0m, \u001b[32m\"sanction\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"53\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"leverag\"\u001b[0m, \u001b[32m\"buyout\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"54\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"satellit\"\u001b[0m, \u001b[32m\"launch\"\u001b[0m, \u001b[32m\"contract\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"55\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"insid\"\u001b[0m, \u001b[32m\"trade\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"56\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"prime\"\u001b[0m, \u001b[32m\"lend\"\u001b[0m, \u001b[32m\"rate\"\u001b[0m, \u001b[32m\"move\"\u001b[0m, \u001b[32m\"predict\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"57\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"mci\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"58\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"rail\"\u001b[0m, \u001b[32m\"strike\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"59\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"weather\"\u001b[0m, \u001b[32m\"relat\"\u001b[0m, \u001b[32m\"fatal\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"60\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"merit\"\u001b[0m, \u001b[32m\"pai\"\u001b[0m, \u001b[32m\"senior\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"61\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"isra\"\u001b[0m, \u001b[32m\"role\"\u001b[0m, \u001b[32m\"iran\"\u001b[0m, \u001b[32m\"contra\"\u001b[0m, \u001b[32m\"affair\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"62\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"militari\"\u001b[0m, \u001b[32m\"coup\"\u001b[0m, \u001b[32m\"etat\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"63\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"machin\"\u001b[0m, \u001b[32m\"translat\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"64\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"hostag\"\u001b[0m, \u001b[32m\"take\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"65\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"inform\"\u001b[0m, \u001b[32m\"retriev\"\u001b[0m, \u001b[32m\"system\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"66\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"natur\"\u001b[0m, \u001b[32m\"languag\"\u001b[0m, \u001b[32m\"process\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"67\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"polit\"\u001b[0m, \u001b[32m\"motiv\"\u001b[0m, \u001b[32m\"civil\"\u001b[0m, \u001b[32m\"disturb\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"68\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"health\"\u001b[0m, \u001b[32m\"hazard\"\u001b[0m, \u001b[32m\"fine\"\u001b[0m, \u001b[32m\"diamet\"\u001b[0m, \u001b[32m\"fiber\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"69\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"attempt\"\u001b[0m, \u001b[32m\"reviv\"\u001b[0m, \u001b[32m\"salt\"\u001b[0m, \u001b[32m\"treati\"\u001b[0m)),\n",
       "\u001b[33m...\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// requires: having added the file \"questions-descriptions.txt\" to source\n",
    "// This cell will build a list (can be Stream if required) of query tokens. \n",
    "// Note that the 16 is hard-coded to ignore the first 15 characters of these <title> line, which all read \n",
    "// \"<title> Topic: \"\n",
    "import scala.io.Source\n",
    "val numPattern = \"[0-9]+\".r\n",
    "\n",
    "val title = Source.fromFile(files_path +\"questions-descriptions.txt\").getLines().filter(_.startsWith(\"<title>\"))\n",
    "                .map(_.substring(16).trim).map(x => token_filter(x)).toList\n",
    "\n",
    "val num = Source.fromFile(files_path +\"questions-descriptions.txt\").getLines().filter(_.startsWith(\"<num>\"))   \n",
    "                .map(x => numPattern.findFirstIn(x.toString).get.substring(1)).toList\n",
    "\n",
    "val query = num zip title\n",
    "query.sortBy(_._1) // the sorted order remains inherent to the object query (nice!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Term-Frequency Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction \u001b[36mhash_query\u001b[0m\n",
       "\u001b[36mcorpus_size\u001b[0m: \u001b[32mInt\u001b[0m = \u001b[32m100000\u001b[0m\n",
       "defined \u001b[32mfunction \u001b[36mget_idf\u001b[0m\n",
       "defined \u001b[32mfunction \u001b[36mget_tf\u001b[0m\n",
       "defined \u001b[32mfunction \u001b[36mget_tf_idf\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// DEFINE AUXILLARY FUNCTIONS\n",
    "\n",
    "// get inverse-document frequency (idf)\n",
    "// is defined as the logarithmically scaled inverse fraction of the documents that contain the word, \n",
    "// obtained by dividing the total number of documents by the number of documents containing the term, and then \n",
    "// taking the logarithm of that quotient.\n",
    "\n",
    "def hash_query(query: (String, List[String])) = {\n",
    "    (query._1, query._2.map(x => token_hash.getOrElse(x,-1)).filter(prun_htoken_id.keys.toSet.contains(_)).toSet)\n",
    "}\n",
    "\n",
    "val corpus_size = prun_id_htoken.size\n",
    "def get_idf(query: Set[Int]) = {\n",
    "    query.map(x => x -> Math.log(corpus_size / prun_htoken_id(x).size)).toMap\n",
    "}\n",
    "\n",
    "// get term frequency in a specific document (doc)\n",
    "def get_tf(query: Set[Int],doc: Int) = {\n",
    "    prun_id_htoken(doc).filter(query.contains(_)).groupBy(identity).mapValues(_.size)\n",
    "}\n",
    "\n",
    "// get tf-idf is defined as tf-idf = tf * idf\n",
    "def get_tf_idf(query: Set[Int],doc:Int) = {\n",
    "    get_tf(query,doc).map(x => x._1 -> x._2 * get_idf(query).getOrElse(x._1,0.toDouble))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction \u001b[36mhandle\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Handle a Query --> take in a query, produce a ranking\n",
    "def handle(query: (String, List[String])) = {\n",
    "    val hashed_query = hash_query(query)\n",
    "    val doc_set = hashed_query._2.flatMap(x => prun_htoken_id(x)).toSet\n",
    "    val ranking = doc_set.map(x => x -> get_tf_idf(hashed_query._2,x).values.sum).toSeq.sortBy(-_._2)\n",
    "                    .take(100).map(x => x._1).toList    \n",
    "    (query._1,ranking)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres124\u001b[0m: \u001b[32mInt\u001b[0m = \u001b[32m11210\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// test it out.\n",
    "hash_query(query(1))._2.flatMap(x => prun_htoken_id(x)).toSet.size\n",
    "//handle(query(35))\n",
    "timeit.start\n",
    "\n",
    "timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36manswers\u001b[0m: \u001b[32mMap\u001b[0m[\u001b[32mString\u001b[0m, \u001b[32mList\u001b[0m[\u001b[32mString\u001b[0m]] = \u001b[33mMap\u001b[0m(\n",
       "  \u001b[32m\"67\"\u001b[0m -> \u001b[33mList\u001b[0m(\n",
       "    \u001b[32m\"FR88907-0036\"\u001b[0m,\n",
       "    \u001b[32m\"FR88208-0014\"\u001b[0m,\n",
       "    \u001b[32m\"FR89123-0023\"\u001b[0m,\n",
       "    \u001b[32m\"FR89818-0017\"\u001b[0m,\n",
       "    \u001b[32m\"FR881107-0026\"\u001b[0m,\n",
       "    \u001b[32m\"FR88514-0001\"\u001b[0m,\n",
       "    \u001b[32m\"FR88602-0011\"\u001b[0m,\n",
       "    \u001b[32m\"FR88831-0149\"\u001b[0m,\n",
       "    \u001b[32m\"FR88516-0128\"\u001b[0m,\n",
       "    \u001b[32m\"FR88728-0019\"\u001b[0m,\n",
       "    \u001b[32m\"FR88511-0168\"\u001b[0m,\n",
       "    \u001b[32m\"FR891013-0117\"\u001b[0m,\n",
       "    \u001b[32m\"FR88830-0023\"\u001b[0m,\n",
       "    \u001b[32m\"FR88829-0023\"\u001b[0m,\n",
       "    \u001b[32m\"FR88211-0266\"\u001b[0m,\n",
       "    \u001b[32m\"FR881206-0016\"\u001b[0m,\n",
       "    \u001b[32m\"FR891025-0107\"\u001b[0m,\n",
       "    \u001b[32m\"FR881006-0001\"\u001b[0m,\n",
       "\u001b[33m...\u001b[0m\n",
       "\u001b[36mres25_2\u001b[0m: \u001b[32mDouble\u001b[0m = \u001b[32m7.059047356016666\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Does it work on mass-answering queries?\n",
    "// It takes nearly 8 minutes though, so about 12 seconds per query on average. \n",
    "// Potential speed improvements: Write a function that reduces document collection in the first place.\n",
    "timeit.start\n",
    "val answers = query.map(x => handle(x)).toMap.mapValues(_.map(x => id_name(x)))\n",
    "timeit.uptonow / 60.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mobject \u001b[36mInspector\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// The object Inspector contains all functions required to calculate the evaluation metrics (Precision, Recall, \n",
    "// F1-Score and MAP (mean average precision))\n",
    "\n",
    "object Inspector\n",
    "{\n",
    "// calculates average precision for a given answer (returned result of query)\n",
    "def badass1(retriev2: List[String], relev: Array[String], bounded: Boolean=false): Double ={\n",
    "    val retriev = retriev2.map(_.replace(\"-\", \"\"))\n",
    "    // remember to remove the \"-\" hyphens from the prediction for comparison purposes\n",
    "    (retriev.map(relev.contains(_)) // produces a boolean list with true where element belongs to relevant\n",
    "        .scanLeft(0){case (sum, next) => if(next) sum + 1 else sum}.tail // creates cumulative count of the booleans\n",
    "        .zipWithIndex.map(x => x._1.toDouble / (x._2 + 1)) // calculates average precision for each element\n",
    "        .zip(retriev.map(relev.contains(_))) // combines average precision with the boolean list from the start\n",
    "        .filter(_._2) // to filter out the ones that are not relevant \n",
    "        .map(_._1).sum // calculates the numerator (sums up the precision for all elements that are relevant)\n",
    "        )/ (if (bounded) retriev.size else relev.size) // divides by numerator (depending on bounded or not)\n",
    "    }\n",
    "\n",
    "// calculates mean average precision over a set of queries. \n",
    "def badass2(retriev_all: Map[String, List[String]], relev_all: Map[String, Array[String]], \n",
    "            bounded: Boolean=false): Double = {\n",
    "    (retriev_all.map(x => Inspector.badass1(x._2,relev_all(x._1),bounded)) // calculate average precision for each query\n",
    "    .sum)/(retriev_all.size) // calculates mean average precision (average precision over all queries)    \n",
    "}\n",
    "// Classic Precision and Recall for a given query, not striclty necessary. \n",
    "def evaluate(retriev: List[String], relev: Array[String])={\n",
    "    val TP = retriev.filter(relev.contains(_)).size.toDouble\n",
    "    val precision = TP / retriev.size\n",
    "    val recall = TP / relev.size\n",
    "    (precision,recall)\n",
    "}\n",
    "def recall1(retriev2: List[String],relev: Array[String]): Double = {\n",
    "        val retriev = retriev2.map(_.replace(\"-\", \"\"))\n",
    "        val TP = relev.filter(retriev.contains(_)).size.toDouble\n",
    "        TP / relev.size\n",
    "}\n",
    "\n",
    "def recall2(retriev_all: Map[String, List[String]],relev_all: Map[String, Array[String]]) = {\n",
    "        retriev_all.map(x => x._1 -> Inspector.recall1(x._2,relev_all(x._1)))\n",
    "}\n",
    "\n",
    "def tps1(retriev2: List[String],relev: Array[String]): Double = {\n",
    "        val retriev = retriev2.map(_.replace(\"-\", \"\"))\n",
    "        val TP = relev.filter(retriev.contains(_)).size.toDouble\n",
    "        TP \n",
    "}\n",
    "\n",
    "\n",
    "def tps2(retriev_all: Map[String, List[String]],relev_all: Map[String, Array[String]]) = {\n",
    "        retriev_all.map(x => x._1 -> Inspector.tps1(x._2,relev_all(x._1)))\n",
    "}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mquery_ID\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"51\"\u001b[0m\n",
       "\u001b[36mres27_1\u001b[0m: \u001b[32mDouble\u001b[0m = \u001b[32m0.11750883431017148\u001b[0m\n",
       "\u001b[36mres27_2\u001b[0m: \u001b[32mDouble\u001b[0m = \u001b[32m0.07435778472378372\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Example Usage\n",
    "val query_ID = \"51\"\n",
    "Inspector.badass1(answers(query_ID),truth.judgements(query_ID),bounded=true)\n",
    "Inspector.badass2(answers,truth.judgements,bounded=true) // answers are my predictions, truth.judgements from tinyIR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mquery_ID\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"67\"\u001b[0m\n",
       "\u001b[36mres28_1\u001b[0m: \u001b[32mInt\u001b[0m = \u001b[32m534\u001b[0m\n",
       "\u001b[36mres28_2\u001b[0m: \u001b[32mcollection\u001b[0m.\u001b[32mimmutable\u001b[0m.\u001b[32mIterable\u001b[0m[(\u001b[32mDouble\u001b[0m, \u001b[32mString\u001b[0m)] = \u001b[33mList\u001b[0m(\n",
       "  \u001b[33m\u001b[0m(\u001b[32m0.0\u001b[0m, \u001b[32m\"67\"\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m0.004212364150565186\u001b[0m, \u001b[32m\"66\"\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m0.0\u001b[0m, \u001b[32m\"89\"\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m0.11750883431017148\u001b[0m, \u001b[32m\"51\"\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m0.0\u001b[0m, \u001b[32m\"84\"\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m0.0\u001b[0m, \u001b[32m\"73\"\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m0.6886297042598777\u001b[0m, \u001b[32m\"78\"\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m0.03887095023193616\u001b[0m, \u001b[32m\"62\"\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m1.4925373134328358E-4\u001b[0m, \u001b[32m\"88\"\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m0.2937763328953522\u001b[0m, \u001b[32m\"77\"\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m0.0\u001b[0m, \u001b[32m\"90\"\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m1.6666666666666666E-4\u001b[0m, \u001b[32m\"56\"\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m0.06267791926937441\u001b[0m, \u001b[32m\"55\"\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m5.88235294117647E-4\u001b[0m, \u001b[32m\"68\"\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m0.168030891265378\u001b[0m, \u001b[32m\"61\"\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m0.02035487713456863\u001b[0m, \u001b[32m\"83\"\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m0.0\u001b[0m, \u001b[32m\"79\"\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m0.0\u001b[0m, \u001b[32m\"72\"\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m0.0\u001b[0m, \u001b[32m\"59\"\u001b[0m),\n",
       "\u001b[33m...\u001b[0m\n",
       "\u001b[36mres28_3\u001b[0m: \u001b[32mOption\u001b[0m[\u001b[32mList\u001b[0m[\u001b[32mString\u001b[0m]] = \u001b[33mSome\u001b[0m(\u001b[33mList\u001b[0m(\u001b[32m\"polit\"\u001b[0m, \u001b[32m\"motiv\"\u001b[0m, \u001b[32m\"civil\"\u001b[0m, \u001b[32m\"disturb\"\u001b[0m))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// investigate which queries cause problems.\n",
    "val query_ID = \"67\"\n",
    "truth.judgements(query_ID).size // How many relevant documents exist for that query?\n",
    "\n",
    "// Give a list of (AP, query_ID) for some easy investigation\n",
    "answers.map(x => Inspector.badass1(x._2,truth.judgements(x._1),bounded=true)).zip(answers.keys)//.filter(_._1<0.1)//.size\n",
    "\n",
    "// Look at a particular original query:\n",
    "query.toMap.get(query_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction \u001b[36minvestigate1\u001b[0m\n",
       "defined \u001b[32mfunction \u001b[36minvestigate2\u001b[0m\n",
       "defined \u001b[32mfunction \u001b[36minvestigate3\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def investigate1(query: (String, List[String])) = {\n",
    "    val noranking = hash_query(query)._2.map(x => prun_htoken_id(x)).flatten.toSet.toList\n",
    "    (query._1,noranking)\n",
    "}\n",
    "def investigate2(query: (String, List[String])) = {\n",
    "    val noranking = hash_query(query)._2.map(x => prun_htoken_id(x)).toList.flatten.groupBy(identity).filter(_._2.size  >= query._2.size)\n",
    "    .keys.toList\n",
    "    (query._1,noranking)\n",
    "}\n",
    "def investigate3(query: (String, List[String])) = {\n",
    "    val noranking = hash_query(query)._2.map(x => prun_htoken_id(x)).toList.flatten.groupBy(identity).filter(_._2.size  >= (query._2.size - 1))\n",
    "    .keys.toList\n",
    "    (query._1,noranking)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36manswers1\u001b[0m: \u001b[32mMap\u001b[0m[\u001b[32mString\u001b[0m, \u001b[32mList\u001b[0m[\u001b[32mString\u001b[0m]] = \u001b[33mMap\u001b[0m(\n",
       "  \u001b[32m\"67\"\u001b[0m -> \u001b[33mList\u001b[0m(\n",
       "    \u001b[32m\"FR88907-0036\"\u001b[0m,\n",
       "    \u001b[32m\"FR89809-0102\"\u001b[0m,\n",
       "    \u001b[32m\"ZF109-507-231\"\u001b[0m,\n",
       "    \u001b[32m\"WSJ880201-0065\"\u001b[0m,\n",
       "    \u001b[32m\"AP880301-0135\"\u001b[0m,\n",
       "    \u001b[32m\"AP890327-0005\"\u001b[0m,\n",
       "    \u001b[32m\"AP890917-0007\"\u001b[0m,\n",
       "    \u001b[32m\"AP891109-0137\"\u001b[0m,\n",
       "    \u001b[32m\"WSJ891026-0121\"\u001b[0m,\n",
       "    \u001b[32m\"AP890529-0063\"\u001b[0m,\n",
       "    \u001b[32m\"DOE2-67-1249\"\u001b[0m,\n",
       "    \u001b[32m\"WSJ870114-0056\"\u001b[0m,\n",
       "    \u001b[32m\"WSJ881006-0139\"\u001b[0m,\n",
       "    \u001b[32m\"AP890112-0155\"\u001b[0m,\n",
       "    \u001b[32m\"AP891120-0004\"\u001b[0m,\n",
       "    \u001b[32m\"WSJ920213-0022\"\u001b[0m,\n",
       "    \u001b[32m\"AP880726-0043\"\u001b[0m,\n",
       "    \u001b[32m\"WSJ910724-0099\"\u001b[0m,\n",
       "\u001b[33m...\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "//val answers1 = query.map(x => investigate1(x)).toMap.mapValues(_.map(x => id_name(x)))\n",
    "// val answers2 = query.map(x => investigate2(x)).toMap.mapValues(_.map(x => id_name(x)))\n",
    "// val answers3 = query.map(x => investigate3(x)).toMap.mapValues(_.map(x => id_name(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mrec1\u001b[0m: \u001b[32mMap\u001b[0m[\u001b[32mString\u001b[0m, \u001b[32mDouble\u001b[0m] = \u001b[33mMap\u001b[0m(\n",
       "  \u001b[32m\"67\"\u001b[0m -> \u001b[32m0.5430711610486891\u001b[0m,\n",
       "  \u001b[32m\"66\"\u001b[0m -> \u001b[32m0.9847715736040609\u001b[0m,\n",
       "  \u001b[32m\"89\"\u001b[0m -> \u001b[32m0.8850574712643678\u001b[0m,\n",
       "  \u001b[32m\"51\"\u001b[0m -> \u001b[32m0.9710144927536232\u001b[0m,\n",
       "  \u001b[32m\"84\"\u001b[0m -> \u001b[32m0.9367088607594937\u001b[0m,\n",
       "  \u001b[32m\"73\"\u001b[0m -> \u001b[32m0.6065573770491803\u001b[0m,\n",
       "  \u001b[32m\"78\"\u001b[0m -> \u001b[32m0.9074074074074074\u001b[0m,\n",
       "  \u001b[32m\"62\"\u001b[0m -> \u001b[32m0.9966329966329966\u001b[0m,\n",
       "  \u001b[32m\"88\"\u001b[0m -> \u001b[32m1.0\u001b[0m,\n",
       "  \u001b[32m\"77\"\u001b[0m -> \u001b[32m0.6014492753623188\u001b[0m,\n",
       "  \u001b[32m\"90\"\u001b[0m -> \u001b[32m0.9398496240601504\u001b[0m,\n",
       "  \u001b[32m\"56\"\u001b[0m -> \u001b[32m0.9954441913439636\u001b[0m,\n",
       "  \u001b[32m\"55\"\u001b[0m -> \u001b[32m0.971604938271605\u001b[0m,\n",
       "  \u001b[32m\"68\"\u001b[0m -> \u001b[32m0.764102564102564\u001b[0m,\n",
       "  \u001b[32m\"61\"\u001b[0m -> \u001b[32m1.0\u001b[0m,\n",
       "  \u001b[32m\"83\"\u001b[0m -> \u001b[32m0.7674050632911392\u001b[0m,\n",
       "  \u001b[32m\"79\"\u001b[0m -> \u001b[32m0.9612068965517241\u001b[0m,\n",
       "  \u001b[32m\"72\"\u001b[0m -> \u001b[32m0.42016806722689076\u001b[0m,\n",
       "  \u001b[32m\"59\"\u001b[0m -> \u001b[32m0.6234887737478411\u001b[0m,\n",
       "\u001b[33m...\u001b[0m\n",
       "\u001b[36mrec2\u001b[0m: \u001b[32mMap\u001b[0m[\u001b[32mString\u001b[0m, \u001b[32mDouble\u001b[0m] = \u001b[33mMap\u001b[0m(\n",
       "  \u001b[32m\"67\"\u001b[0m -> \u001b[32m0.0018726591760299626\u001b[0m,\n",
       "  \u001b[32m\"66\"\u001b[0m -> \u001b[32m0.38071065989847713\u001b[0m,\n",
       "  \u001b[32m\"89\"\u001b[0m -> \u001b[32m0.005747126436781609\u001b[0m,\n",
       "  \u001b[32m\"51\"\u001b[0m -> \u001b[32m0.5942028985507246\u001b[0m,\n",
       "  \u001b[32m\"84\"\u001b[0m -> \u001b[32m0.0\u001b[0m,\n",
       "  \u001b[32m\"73\"\u001b[0m -> \u001b[32m0.0\u001b[0m,\n",
       "  \u001b[32m\"78\"\u001b[0m -> \u001b[32m0.9074074074074074\u001b[0m,\n",
       "  \u001b[32m\"62\"\u001b[0m -> \u001b[32m0.020202020202020204\u001b[0m,\n",
       "  \u001b[32m\"88\"\u001b[0m -> \u001b[32m0.0\u001b[0m,\n",
       "  \u001b[32m\"77\"\u001b[0m -> \u001b[32m0.6014492753623188\u001b[0m,\n",
       "  \u001b[32m\"90\"\u001b[0m -> \u001b[32m0.0\u001b[0m,\n",
       "  \u001b[32m\"56\"\u001b[0m -> \u001b[32m0.12870159453302962\u001b[0m,\n",
       "  \u001b[32m\"55\"\u001b[0m -> \u001b[32m0.9012345679012346\u001b[0m,\n",
       "  \u001b[32m\"68\"\u001b[0m -> \u001b[32m0.005128205128205128\u001b[0m,\n",
       "  \u001b[32m\"61\"\u001b[0m -> \u001b[32m0.34951456310679613\u001b[0m,\n",
       "  \u001b[32m\"83\"\u001b[0m -> \u001b[32m0.0\u001b[0m,\n",
       "  \u001b[32m\"79\"\u001b[0m -> \u001b[32m0.0\u001b[0m,\n",
       "  \u001b[32m\"72\"\u001b[0m -> \u001b[32m0.07563025210084033\u001b[0m,\n",
       "  \u001b[32m\"59\"\u001b[0m -> \u001b[32m0.008635578583765112\u001b[0m,\n",
       "\u001b[33m...\u001b[0m\n",
       "\u001b[36mrec3\u001b[0m: \u001b[32mMap\u001b[0m[\u001b[32mString\u001b[0m, \u001b[32mDouble\u001b[0m] = \u001b[33mMap\u001b[0m(\n",
       "  \u001b[32m\"67\"\u001b[0m -> \u001b[32m0.00749063670411985\u001b[0m,\n",
       "  \u001b[32m\"66\"\u001b[0m -> \u001b[32m0.7461928934010152\u001b[0m,\n",
       "  \u001b[32m\"89\"\u001b[0m -> \u001b[32m0.08045977011494253\u001b[0m,\n",
       "  \u001b[32m\"51\"\u001b[0m -> \u001b[32m0.9710144927536232\u001b[0m,\n",
       "  \u001b[32m\"84\"\u001b[0m -> \u001b[32m0.002531645569620253\u001b[0m,\n",
       "  \u001b[32m\"73\"\u001b[0m -> \u001b[32m0.01092896174863388\u001b[0m,\n",
       "  \u001b[32m\"78\"\u001b[0m -> \u001b[32m0.9074074074074074\u001b[0m,\n",
       "  \u001b[32m\"62\"\u001b[0m -> \u001b[32m0.8552188552188552\u001b[0m,\n",
       "  \u001b[32m\"88\"\u001b[0m -> \u001b[32m0.05454545454545454\u001b[0m,\n",
       "  \u001b[32m\"77\"\u001b[0m -> \u001b[32m0.6014492753623188\u001b[0m,\n",
       "  \u001b[32m\"90\"\u001b[0m -> \u001b[32m0.15789473684210525\u001b[0m,\n",
       "  \u001b[32m\"56\"\u001b[0m -> \u001b[32m0.38610478359908884\u001b[0m,\n",
       "  \u001b[32m\"55\"\u001b[0m -> \u001b[32m0.971604938271605\u001b[0m,\n",
       "  \u001b[32m\"68\"\u001b[0m -> \u001b[32m0.015384615384615385\u001b[0m,\n",
       "  \u001b[32m\"61\"\u001b[0m -> \u001b[32m0.7330097087378641\u001b[0m,\n",
       "  \u001b[32m\"83\"\u001b[0m -> \u001b[32m0.24367088607594936\u001b[0m,\n",
       "  \u001b[32m\"79\"\u001b[0m -> \u001b[32m0.17672413793103448\u001b[0m,\n",
       "  \u001b[32m\"72\"\u001b[0m -> \u001b[32m0.42016806722689076\u001b[0m,\n",
       "  \u001b[32m\"59\"\u001b[0m -> \u001b[32m0.15716753022452504\u001b[0m,\n",
       "\u001b[33m...\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val rec1 = Inspector.recall2(answers1,truth.judgements) // this is taking the union\n",
    "val rec2 = Inspector.recall2(answers2,truth.judgements) // this is taking the intersection\n",
    "val rec3 = Inspector.recall2(answers3,truth.judgements) // this allows to miss out on at most one token from the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mtp1\u001b[0m: \u001b[32mMap\u001b[0m[\u001b[32mString\u001b[0m, \u001b[32mDouble\u001b[0m] = \u001b[33mMap\u001b[0m(\n",
       "  \u001b[32m\"67\"\u001b[0m -> \u001b[32m290.0\u001b[0m,\n",
       "  \u001b[32m\"66\"\u001b[0m -> \u001b[32m194.0\u001b[0m,\n",
       "  \u001b[32m\"89\"\u001b[0m -> \u001b[32m154.0\u001b[0m,\n",
       "  \u001b[32m\"51\"\u001b[0m -> \u001b[32m134.0\u001b[0m,\n",
       "  \u001b[32m\"84\"\u001b[0m -> \u001b[32m370.0\u001b[0m,\n",
       "  \u001b[32m\"73\"\u001b[0m -> \u001b[32m111.0\u001b[0m,\n",
       "  \u001b[32m\"78\"\u001b[0m -> \u001b[32m147.0\u001b[0m,\n",
       "  \u001b[32m\"62\"\u001b[0m -> \u001b[32m296.0\u001b[0m,\n",
       "  \u001b[32m\"88\"\u001b[0m -> \u001b[32m165.0\u001b[0m,\n",
       "  \u001b[32m\"77\"\u001b[0m -> \u001b[32m83.0\u001b[0m,\n",
       "  \u001b[32m\"90\"\u001b[0m -> \u001b[32m250.0\u001b[0m,\n",
       "  \u001b[32m\"56\"\u001b[0m -> \u001b[32m874.0\u001b[0m,\n",
       "  \u001b[32m\"55\"\u001b[0m -> \u001b[32m787.0\u001b[0m,\n",
       "  \u001b[32m\"68\"\u001b[0m -> \u001b[32m149.0\u001b[0m,\n",
       "  \u001b[32m\"61\"\u001b[0m -> \u001b[32m206.0\u001b[0m,\n",
       "  \u001b[32m\"83\"\u001b[0m -> \u001b[32m485.0\u001b[0m,\n",
       "  \u001b[32m\"79\"\u001b[0m -> \u001b[32m223.0\u001b[0m,\n",
       "  \u001b[32m\"72\"\u001b[0m -> \u001b[32m50.0\u001b[0m,\n",
       "  \u001b[32m\"59\"\u001b[0m -> \u001b[32m361.0\u001b[0m,\n",
       "\u001b[33m...\u001b[0m\n",
       "\u001b[36mtp2\u001b[0m: \u001b[32mMap\u001b[0m[\u001b[32mString\u001b[0m, \u001b[32mDouble\u001b[0m] = \u001b[33mMap\u001b[0m(\n",
       "  \u001b[32m\"67\"\u001b[0m -> \u001b[32m1.0\u001b[0m,\n",
       "  \u001b[32m\"66\"\u001b[0m -> \u001b[32m75.0\u001b[0m,\n",
       "  \u001b[32m\"89\"\u001b[0m -> \u001b[32m1.0\u001b[0m,\n",
       "  \u001b[32m\"51\"\u001b[0m -> \u001b[32m82.0\u001b[0m,\n",
       "  \u001b[32m\"84\"\u001b[0m -> \u001b[32m0.0\u001b[0m,\n",
       "  \u001b[32m\"73\"\u001b[0m -> \u001b[32m0.0\u001b[0m,\n",
       "  \u001b[32m\"78\"\u001b[0m -> \u001b[32m147.0\u001b[0m,\n",
       "  \u001b[32m\"62\"\u001b[0m -> \u001b[32m6.0\u001b[0m,\n",
       "  \u001b[32m\"88\"\u001b[0m -> \u001b[32m0.0\u001b[0m,\n",
       "  \u001b[32m\"77\"\u001b[0m -> \u001b[32m83.0\u001b[0m,\n",
       "  \u001b[32m\"90\"\u001b[0m -> \u001b[32m0.0\u001b[0m,\n",
       "  \u001b[32m\"56\"\u001b[0m -> \u001b[32m113.0\u001b[0m,\n",
       "  \u001b[32m\"55\"\u001b[0m -> \u001b[32m730.0\u001b[0m,\n",
       "  \u001b[32m\"68\"\u001b[0m -> \u001b[32m1.0\u001b[0m,\n",
       "  \u001b[32m\"61\"\u001b[0m -> \u001b[32m72.0\u001b[0m,\n",
       "  \u001b[32m\"83\"\u001b[0m -> \u001b[32m0.0\u001b[0m,\n",
       "  \u001b[32m\"79\"\u001b[0m -> \u001b[32m0.0\u001b[0m,\n",
       "  \u001b[32m\"72\"\u001b[0m -> \u001b[32m9.0\u001b[0m,\n",
       "  \u001b[32m\"59\"\u001b[0m -> \u001b[32m5.0\u001b[0m,\n",
       "\u001b[33m...\u001b[0m\n",
       "\u001b[36mtp3\u001b[0m: \u001b[32mMap\u001b[0m[\u001b[32mString\u001b[0m, \u001b[32mDouble\u001b[0m] = \u001b[33mMap\u001b[0m(\n",
       "  \u001b[32m\"67\"\u001b[0m -> \u001b[32m4.0\u001b[0m,\n",
       "  \u001b[32m\"66\"\u001b[0m -> \u001b[32m147.0\u001b[0m,\n",
       "  \u001b[32m\"89\"\u001b[0m -> \u001b[32m14.0\u001b[0m,\n",
       "  \u001b[32m\"51\"\u001b[0m -> \u001b[32m134.0\u001b[0m,\n",
       "  \u001b[32m\"84\"\u001b[0m -> \u001b[32m1.0\u001b[0m,\n",
       "  \u001b[32m\"73\"\u001b[0m -> \u001b[32m2.0\u001b[0m,\n",
       "  \u001b[32m\"78\"\u001b[0m -> \u001b[32m147.0\u001b[0m,\n",
       "  \u001b[32m\"62\"\u001b[0m -> \u001b[32m254.0\u001b[0m,\n",
       "  \u001b[32m\"88\"\u001b[0m -> \u001b[32m9.0\u001b[0m,\n",
       "  \u001b[32m\"77\"\u001b[0m -> \u001b[32m83.0\u001b[0m,\n",
       "  \u001b[32m\"90\"\u001b[0m -> \u001b[32m42.0\u001b[0m,\n",
       "  \u001b[32m\"56\"\u001b[0m -> \u001b[32m339.0\u001b[0m,\n",
       "  \u001b[32m\"55\"\u001b[0m -> \u001b[32m787.0\u001b[0m,\n",
       "  \u001b[32m\"68\"\u001b[0m -> \u001b[32m3.0\u001b[0m,\n",
       "  \u001b[32m\"61\"\u001b[0m -> \u001b[32m151.0\u001b[0m,\n",
       "  \u001b[32m\"83\"\u001b[0m -> \u001b[32m154.0\u001b[0m,\n",
       "  \u001b[32m\"79\"\u001b[0m -> \u001b[32m41.0\u001b[0m,\n",
       "  \u001b[32m\"72\"\u001b[0m -> \u001b[32m50.0\u001b[0m,\n",
       "  \u001b[32m\"59\"\u001b[0m -> \u001b[32m91.0\u001b[0m,\n",
       "\u001b[33m...\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val tp1 = Inspector.tps2(answers1,truth.judgements) // this is taking the union\n",
    "val tp2 = Inspector.tps2(answers2,truth.judgements) // this is taking the intersection\n",
    "val tp3 = Inspector.tps2(answers3,truth.judgements) // this allows to miss out on at most one token from the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67,534,0.5430711610486891,0.00749063670411985,0.0018726591760299626)\n",
      "(66,197,0.9847715736040609,0.7461928934010152,0.38071065989847713)\n",
      "(89,174,0.8850574712643678,0.08045977011494253,0.005747126436781609)\n",
      "(51,138,0.9710144927536232,0.9710144927536232,0.5942028985507246)\n",
      "(84,395,0.9367088607594937,0.002531645569620253,0.0)\n",
      "(73,183,0.6065573770491803,0.01092896174863388,0.0)\n",
      "(78,162,0.9074074074074074,0.9074074074074074,0.9074074074074074)\n",
      "(62,297,0.9966329966329966,0.8552188552188552,0.020202020202020204)\n",
      "(88,165,1.0,0.05454545454545454,0.0)\n",
      "(77,138,0.6014492753623188,0.6014492753623188,0.6014492753623188)\n",
      "(90,266,0.9398496240601504,0.15789473684210525,0.0)\n",
      "(56,878,0.9954441913439636,0.38610478359908884,0.12870159453302962)\n",
      "(55,810,0.971604938271605,0.971604938271605,0.9012345679012346)\n",
      "(68,195,0.764102564102564,0.015384615384615385,0.005128205128205128)\n",
      "(61,206,1.0,0.7330097087378641,0.34951456310679613)\n",
      "(83,632,0.7674050632911392,0.24367088607594936,0.0)\n",
      "(79,232,0.9612068965517241,0.17672413793103448,0.0)\n",
      "(72,119,0.42016806722689076,0.42016806722689076,0.07563025210084033)\n",
      "(59,579,0.6234887737478411,0.15716753022452504,0.008635578583765112)\n",
      "(87,188,0.7553191489361702,0.05851063829787234,0.0)\n",
      "(76,294,0.9489795918367347,0.4523809523809524,0.19047619047619047)\n",
      "(54,171,0.9824561403508771,0.9181286549707602,0.2982456140350877)\n",
      "(65,386,0.9637305699481865,0.8238341968911918,0.44041450777202074)\n",
      "(71,380,0.8710526315789474,0.8710526315789474,0.30526315789473685)\n",
      "(57,461,0.93058568329718,0.93058568329718,0.93058568329718)\n",
      "(80,374,0.9705882352941176,0.7647058823529411,0.17647058823529413)\n",
      "(82,599,0.8697829716193656,0.8697829716193656,0.6126878130217028)\n",
      "(60,60,0.95,0.6333333333333333,0.13333333333333333)\n",
      "(69,52,0.9807692307692307,0.21153846153846154,0.019230769230769232)\n",
      "(58,159,0.9874213836477987,0.9874213836477987,0.8616352201257862)\n",
      "(64,375,0.9626666666666667,0.9626666666666667,0.192)\n",
      "(53,571,0.9439579684763573,0.9439579684763573,0.6900175131348512)\n",
      "(75,365,0.8328767123287671,0.8328767123287671,0.8328767123287671)\n",
      "(70,55,0.9818181818181818,0.9818181818181818,0.2)\n",
      "(86,213,0.9953051643192489,0.9953051643192489,0.6103286384976526)\n",
      "(81,62,0.7580645161290323,0.0,0.0)\n",
      "(63,208,0.7451923076923077,0.7451923076923077,0.3125)\n",
      "(74,499,0.5551102204408818,0.5551102204408818,0.12024048096192384)\n",
      "(52,535,0.9925233644859813,0.9271028037383178,0.6990654205607477)\n",
      "(85,894,0.9116331096196868,0.9116331096196868,0.5369127516778524)\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for (k <- rec1.keys){\n",
    "    println(k,truth.judgements(k).size,rec1(k),rec3(k),rec2(k))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mth\u001b[0m: \u001b[32mDouble\u001b[0m = \u001b[32m0.5\u001b[0m\n",
       "\u001b[36mres99_1\u001b[0m: \u001b[32mInt\u001b[0m = \u001b[32m39\u001b[0m\n",
       "\u001b[36mres99_2\u001b[0m: \u001b[32mInt\u001b[0m = \u001b[32m12\u001b[0m\n",
       "\u001b[36mres99_3\u001b[0m: \u001b[32mInt\u001b[0m = \u001b[32m24\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val th = 0.5\n",
    "rec1.filter(_._2 > th).size\n",
    "rec3.filter(_._2 > th).size\n",
    "rec2.filter(_._2 > th).size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67,534,290,4,1)\n",
      "(66,197,194,147,75)\n",
      "(89,174,154,14,1)\n",
      "(51,138,134,134,82)\n",
      "(84,395,370,1,0)\n",
      "(73,183,111,2,0)\n",
      "(78,162,147,147,147)\n",
      "(62,297,296,254,6)\n",
      "(88,165,165,9,0)\n",
      "(77,138,83,83,83)\n",
      "(90,266,250,42,0)\n",
      "(56,878,874,339,113)\n",
      "(55,810,787,787,730)\n",
      "(68,195,149,3,1)\n",
      "(61,206,206,151,72)\n",
      "(83,632,485,154,0)\n",
      "(79,232,223,41,0)\n",
      "(72,119,50,50,9)\n",
      "(59,579,361,91,5)\n",
      "(87,188,142,11,0)\n",
      "(76,294,279,133,56)\n",
      "(54,171,168,157,51)\n",
      "(65,386,372,318,170)\n",
      "(71,380,331,331,116)\n",
      "(57,461,429,429,429)\n",
      "(80,374,363,286,66)\n",
      "(82,599,521,521,367)\n",
      "(60,60,57,38,8)\n",
      "(69,52,51,11,1)\n",
      "(58,159,157,157,137)\n",
      "(64,375,361,361,72)\n",
      "(53,571,539,539,394)\n",
      "(75,365,304,304,304)\n",
      "(70,55,54,54,11)\n",
      "(86,213,212,212,130)\n",
      "(81,62,47,0,0)\n",
      "(63,208,155,155,65)\n",
      "(74,499,277,277,60)\n",
      "(52,535,531,496,374)\n",
      "(85,894,815,815,480)\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for (k <- tp1.keys){\n",
    "    println(k,truth.judgements(k).size,tp1(k).toInt,tp3(k).toInt,tp2(k).toInt)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mth\u001b[0m: \u001b[32mInt\u001b[0m = \u001b[32m100\u001b[0m\n",
       "\u001b[36mres110_1\u001b[0m: \u001b[32mInt\u001b[0m = \u001b[32m34\u001b[0m\n",
       "\u001b[36mres110_2\u001b[0m: \u001b[32mInt\u001b[0m = \u001b[32m24\u001b[0m\n",
       "\u001b[36mres110_3\u001b[0m: \u001b[32mInt\u001b[0m = \u001b[32m13\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val th = 100\n",
    "tp1.filter(_._2 > th).size\n",
    "tp3.filter(_._2 > th).size\n",
    "tp2.filter(_._2 > th).size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "2\n",
      "3\n",
      "2\n",
      "5\n",
      "1\n",
      "2\n",
      "3\n",
      "3\n",
      "5\n",
      "3\n",
      "2\n",
      "2\n",
      "3\n",
      "3\n",
      "4\n",
      "5\n",
      "4\n",
      "2\n",
      "2\n",
      "2\n",
      "4\n",
      "2\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "4\n",
      "3\n",
      "5\n",
      "2\n",
      "3\n",
      "6\n",
      "2\n",
      "2\n",
      "4\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query.map(_._2.size).foreach{\n",
    "    println(_)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language model score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// // collection tf\n",
    "val collection_size = prun_htoken_collectfreq.foldLeft(0.0)(_+_._2)\n",
    "val collection_size_log = prun_htoken_collectfreq.foldLeft(0.0)(\n",
    "    (res,value) => res + Math.log(1.0 + value._2.toDouble))\n",
    "\n",
    "val pruned_token_set = prun_htoken_collectfreq.keys.toSet\n",
    "\n",
    "// discards log scores\n",
    "def unfold_name_time(score :(List[(String, Double)], Double)) = {\n",
    "    (score._1.unzip._1, score._2)\n",
    "}\n",
    "\n",
    "// returns id's of docs in which the most query tokens appear in\n",
    "def reduce_candidate_doc(query: (String, List[Int]), \n",
    "                         prun_htoken_id: MutHashMap[Int, List[Int]] = prun_htoken_id, \n",
    "                         take_k_results: Int = 100) = {\n",
    "    \n",
    "    // doc_id -> # of occurances\n",
    "    val doc_occurance = query._2.flatMap(token => prun_htoken_id(token)).\n",
    "        groupBy(identity).mapValues(_.size)\n",
    "    \n",
    "    // sorted iterator of # of occurances\n",
    "    val intersect_value = doc_occurance.values.toSet.toList.sorted.reverse.toIterator\n",
    "    var iter = intersect_value.next\n",
    "    \n",
    "    var cur_doc_occurance = doc_occurance.filter(_._2 > iter - 1)\n",
    "    \n",
    "    while((cur_doc_occurance.size < take_k_results) & (intersect_value.hasNext)) {\n",
    "        iter = intersect_value.next\n",
    "        cur_doc_occurance = doc_occurance.filter(_._2 > iter - 1)\n",
    "    }\n",
    "    cur_doc_occurance.keys.toList\n",
    "}\n",
    "\n",
    "val query_hash = query.map{ \n",
    "    case (id, str) => (id, str.\n",
    "                       flatMap(x => token_hash.get(x)).filter(pruned_token_set.contains(_))\n",
    "                      )}\n",
    "\n",
    "val lambda = 0.01 // smoothing parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "val query_hash = query.map{ \n",
    "    case (id, str) => (id, str.\n",
    "                       flatMap(x => token_hash.get(x)).filter(pruned_token_set.contains(_))\n",
    "                      )}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lang_query(query: (String, List[Int]),\n",
    "               method: String = \"index\", \n",
    "               log_opt: String = \"tf\",\n",
    "               prun_htoken_collectfreq: MutHashMap[Int, Int] = prun_htoken_collectfreq,\n",
    "               collection_size: Double = collection_size, \n",
    "               collection_size_log: Double = collection_size_log, \n",
    "               lambda: Double = lambda, \n",
    "               prun_id_htoken: MutHashMap[Int, List[Int]] = prun_id_htoken, \n",
    "               prun_htoken_id: MutHashMap[Int, List[Int]] = prun_htoken_id) = {\n",
    "    \n",
    "    // list of doc id's containing tokens in query\n",
    "    def candidate_doc(): List[Int] = method match {\n",
    "        case \"index\" => reduce_candidate_doc(query = query, take_k_results = take_k_results)\n",
    "        case \"no_index\" => (0 to 100000-1).toList\n",
    "//         case \"test\" => (1 to 2).toList\n",
    "        case _ => throw new Exception(\"Please choose either 'index' or 'no_index'\")\n",
    "//    def candidate_doc(): List[Int] = {\n",
    "// //        query._2.flatMap(token => prun_htoken_id(token)).distinct\n",
    "//        List(1,2,6,14,36,37,50,65,68)\n",
    "    }\n",
    "        \n",
    "    // map of tokens to frequency in a given doc\n",
    "    def doc_tf_map(doc_id: Int) = log_opt match {\n",
    "        case \"tf\" => prun_id_htoken(doc_id).\n",
    "            groupBy(identity).mapValues(x => x.size.toDouble)\n",
    "        case \"log\" => prun_id_htoken(doc_id).\n",
    "            groupBy(identity).mapValues(x => Math.log(1.0+x.size))\n",
    "        case _ => throw new Exception(\"Please choose either 'log' or 'tf'\")\n",
    "    }\n",
    "    \n",
    "    // number of tokens in doc\n",
    "    def doc_size(doc_id: Int) = {\n",
    "        doc_tf_map(doc_id).values.sum\n",
    "    }\n",
    "\n",
    "    // list of (relative) frequency of query tokens in a given doc\n",
    "    def doc_query_tf(doc_id: Int) = {\n",
    "        query._2.map(token => \n",
    "                     doc_tf_map(doc_id).getOrElse(token, 0.0) / doc_size(doc_id))\n",
    "    }\n",
    "    \n",
    "    // list of (relative) frequency of query tokens in the collection\n",
    "    def query_cf() = log_opt match {\n",
    "        case \"tf\" => query._2.map(token => \n",
    "                                  prun_htoken_collectfreq(token).toDouble / collection_size)\n",
    "        case \"log\" => query._2.map(token => \n",
    "                                  Math.log(1.0 + prun_htoken_collectfreq(token)) / collection_size_log)\n",
    "    }\n",
    "        \n",
    "    // this only needs to be calculated once per query\n",
    "    // (wasteful to call function multiple times)\n",
    "    val cur_query_cf = query_cf()\n",
    "    \n",
    "    //\n",
    "    def smooth_prob(doc_id: Int) = {\n",
    "        doc_query_tf(doc_id).zip(cur_query_cf).\n",
    "            map{case (x, y) => (1 - lambda) * x + lambda * y}\n",
    "    }\n",
    "    \n",
    "    // sum log(x) elements of list\n",
    "    def doc_lang_score(doc_id: Int) = {\n",
    "        smooth_prob(doc_id).foldLeft(0.0)(_ + Math.log(_))\n",
    "    }\n",
    "    \n",
    "    candidate_doc().map(doc => \n",
    "                        (query._1, doc, doc_lang_score(doc))\n",
    "                       ).sortWith(_._3 > _._3)\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "// with raw log scores\n",
    "val lang_model_rank_time = query_hash.map(query => \n",
    "                       (query._1, unfold_name_time(\n",
    "                           lang_query(query, method = \"index\", log_opt = \"tf\"))\n",
    "                       )\n",
    "                                                 ).toMap\n",
    "\n",
    "val lang_model_time = lang_model_rank_time.values.map(x => x._2)\n",
    "val lang_model_time_average = average(lang_model_time)\n",
    "\n",
    "// before reducing candidates\n",
    "// Iterable[Double] = List(\n",
    "//   0.3791437873666667,\n",
    "//   0.5371110624833333,\n",
    "//   9.883984977816668,\n",
    "//   1.4138989739999999,\n",
    "//   2.41249985935\n",
    "// )\n",
    "\n",
    "val lang_model_rank = lang_model_rank_time.mapValues(x => x._1)\n",
    "\n",
    "// Example Usage\n",
    "Inspector.badass2(lang_model_rank,truth.judgements,bounded=true) // answers are my predictions, truth.judgements from tinyIR\n",
    "\n",
    "// Inspector.evaluate(lang_model_rank(\"51\").map(_.replace(\"-\", \"\")),truth.judgements(\"51\"))\n",
    "\n",
    "// lang_model_rank.keys.toList.map(q_id => q_id)\n",
    "var precision = List[Double]()\n",
    "var recall = List[Double]()\n",
    "\n",
    "// (precision,recall)\n",
    "for (key <- lang_model_rank.keys) {\n",
    "//     println(key)\n",
    "    var p_r = Inspector.evaluate(lang_model_rank(key).map(_.replace(\"-\", \"\")),truth.judgements(key))\n",
    "    precision ++= List(p_r._1)\n",
    "    recall ++= List(p_r._2)\n",
    "}\n",
    "\n",
    "println(f\"mean precision is ${average(precision)}%1.3f\")\n",
    "// average(mean_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def lang_model_results_MAP(query_hash: List[(String, List[Int])] = query_hash, \n",
    "                           method: String, \n",
    "                           log_opt: String, \n",
    "                           lambda: Double = lambda, \n",
    "                           truth: TipsterGroundTruth = truth) = {\n",
    "    \n",
    "    val lang_model_rank_time = query_hash.map(query => \n",
    "                           (query._1, unfold_name_time(\n",
    "                               lang_query(query, \n",
    "                                          method = method, \n",
    "                                          log_opt = log_opt, \n",
    "                                          lambda = lambda))\n",
    "                           )\n",
    "                                             ).toMap\n",
    "    \n",
    "    val lang_model_time = average(lang_model_rank_time.values.map(x => x._2))\n",
    "    val lang_model_rank = lang_model_rank_time.mapValues(x => x._1)\n",
    "    println(f\"Average time per query is ${lang_model_time}%1.3f seconds\")\n",
    "    \n",
    "    val MAP_score = Inspector.badass2(lang_model_rank,truth.judgements,bounded=true)\n",
    "    println(f\"MAP score is ${MAP_score}%1.3f\")\n",
    "    \n",
    "    var precision = List[Double]()\n",
    "    \n",
    "    // (precision,recall)\n",
    "    for (key <- lang_model_rank.keys) {\n",
    "        var p_r = Inspector.evaluate(lang_model_rank(key).map(_.replace(\"-\", \"\")),truth.judgements(key))\n",
    "        precision ++= List(p_r._1)\n",
    "    //     recall ++= List(p_r._2)\n",
    "    }\n",
    "    println(f\"mean precision is ${average(precision)}%1.3f\")\n",
    "    \n",
    "    MAP_score\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lang_model_results_MAP(method = \"index\", \n",
    "                       log_opt = \"log\", \n",
    "                       lambda = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing ground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Used Memory:  400\n",
    "// Free Memory:  215\n",
    "// Total Memory: 616\n",
    "// Max Memory:   3641"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val token_hm = MutHashMap[String, Int]()\n",
    "List(\"word1\", \"word3\").map(x => token_hm.getOrElseUpdate(x, token_hm.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classpath.addPath(tiny_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trait Result[T] extends Any {\n",
    "    def id : Int\n",
    "    def matches(that: T) : Int                 \n",
    "    def isMatch(that: T) = matches(that)==0\n",
    "    def matched(that: T) : T    \n",
    "}\n",
    "\n",
    "object InvertedIndex {\n",
    "    // generic list intersection (does not require sorted lists)\n",
    "    private def unsortedIntersect [A<% Result[A]](l1: List[A], l2: List[A]) = l1.intersect(l2)\n",
    "\n",
    "    // optimized list intersection for sorted posting lists \n",
    "    // uses \"matches\" and \"matched\" methods to work for all posting types\n",
    "    def sIntersect[A <% Result[A]] (l1: List[A], l2: List[A]) : List[A] = {\n",
    "        @annotation.tailrec\n",
    "        def iter (l1: List[A], l2: List[A], result: List[A]) : List[A] = {\n",
    "            if (l1.isEmpty || l2.isEmpty) \n",
    "                result.reverse\n",
    "            else (l1.head matches l2.head) match {\n",
    "                case n if n>0 => iter(l1, l2.tail,result)  // advance list l2\n",
    "                case n if n<0 => iter(l1.tail, l2,result)  // advance list l1\n",
    "                case _        => iter(l1.tail, l2.tail, (l1.head matched l2.head)::result)\t      \n",
    "            }\n",
    "        }    \n",
    "        iter(l1,l2,Nil)      \n",
    "    }\n",
    "}\n",
    "\n",
    "abstract class InvertedIndex[Res <% Result[Res]]  {\n",
    "    def results (term: String) : List[Res] \n",
    "    def results (terms: Seq[String]) : List[Res] = {\n",
    "        val resultLists      = terms.map(term => results(term))\n",
    "        val shortToLongLists = resultLists.sortWith( _.length < _.length) \n",
    "        shortToLongLists.reduceLeft( (l1,l2) => InvertedIndex.sIntersect(l1,l2) )\n",
    "    }\n",
    "}\n",
    "\n",
    "// import ch.ethz.dal.tinyir.indexing.InvertedIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scala.math._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Document(val id: Int, val tokens: List[Int])\n",
    "//     def id: Int = this.id\n",
    "//     def tokens: List[Int] = this.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "case class ProxResult(val id: Int, val lpos: Int, val rpos: Int) extends Result[ProxResult] {\n",
    "    def matches(that: ProxResult) : Int = {    \n",
    "        if (this.id != that.id) this.id - that.id\n",
    "        else if ((max(rpos,that.rpos) - min(lpos,that.lpos)) <= ProxWindow.size) 0 // match\n",
    "        else this.lpos-that.lpos  // advance in list with the minimal lpos\n",
    "    }\n",
    "    def matched(that: ProxResult) = \n",
    "        ProxResult(id, min(this.lpos,that.lpos), max(this.rpos,that.rpos))\n",
    "}\n",
    "\n",
    "object ProxWindow {\n",
    "    var size = 1\n",
    "    def setSize(w: Int) {assert(w>=1); size = w}\n",
    "}\n",
    "\n",
    "class PosIndex (docs: Stream[Document]) extends InvertedIndex[ProxResult] {\n",
    "\n",
    "    case class PosPosting(val id: Int, val pos: Int) extends Ordered[PosPosting] {\n",
    "        def this(t: PosTuple) = this(t.id, t.pos) \n",
    "//         def compare(that: PosPosting) = Ordering[Tuple2[Int, Int]].compare((this.id, this.pos), (that.id, that.pos) ) \n",
    "    }\n",
    "    type PostList = List[PosPosting]\n",
    "    val index : Map[String, PostList] = {\n",
    "        val groupedPostings = postings(docs).groupBy(_.term)\n",
    "        groupedPostings.mapValues(_.map(p => PosPosting(p.id,p.pos)).sorted)\n",
    "    }\n",
    "  \n",
    "    case class PosTuple(term: String, id: Int, pos: Int) \n",
    "    def postings (s: Stream[Document]): List[PosTuple] =\n",
    "        s.flatMap( d => d.tokens.zipWithIndex.map{ case (tk,pos) => PosTuple(tk,d.ID,pos) } ).toList\n",
    "\n",
    "    override def results (word: String) : List[ProxResult] = \n",
    "        index.getOrElse(word,null).map(p => ProxResult(p.id, p.pos, p.pos))\n",
    "    override def results (terms: Seq[String]) : List[ProxResult] = results(terms,1)\n",
    "    def results (terms: Seq[String], win: Int) : List[ProxResult] = {\n",
    "        val resultLists = terms.map(term => results(term))\n",
    "        val shortToLongLists = resultLists.sortWith( _.length < _.length)   \n",
    "        shortToLongLists.reduceLeft( (l1,l2) => InvertedIndex.sIntersect(l1,l2) )\n",
    "    } \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Scala 2.11",
   "language": "scala211",
   "name": "scala211"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
