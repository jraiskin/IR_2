{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set your user name as a case, pointing to your path to documents and tinyir.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// set your case once\n",
    "val (doc_dir: String, files_path: String) = System.getProperties().get(\"user.name\").toString match {\n",
    "    case \"Yarden-\"  => (\"../documents\", \"../\")\n",
    "    case \"Max\"  => (\"/Users/Max/Coding/ETH/Information_Retrieval_AS16/scala_practice/documents\", \"/Users/Max/Coding/ETH/Information_Retrieval_AS16/scala_practice/files/\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classpath.addPath(files_path + \"tinyir-1.1.jar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import scala.xml.XML\n",
    "import ch.ethz.dal.tinyir._\n",
    "import com.github.aztek.porterstemmer.PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scala.io.Source  // for importing txt files\n",
    "import java.io._  // for saving txt files\n",
    "// import scala.collection.mutable.HashMap  //HashMap used for counting elements in linear time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// import scala.util.Random\n",
    "import scala.collection.mutable.{Map => MutMap, HashMap => MutHashMap}\n",
    "// enables \"mutable lists\"\n",
    "// import scala.collection.mutable.ListBuffer  \n",
    "import scala.collection.mutable.{Set => MutSet}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val timeit = new util.StopWatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def average[T]( ts: Iterable[T] )( implicit num: Numeric[T] ) = {\n",
    "  num.toDouble( ts.sum ) / ts.size\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def token_filter(text_body: String) = {\n",
    "    processing.StopWords.filterOutSW(\n",
    "        processing.Tokenizer.tokenize(text_body.\n",
    "                                      replaceAll(\"\\\\P{L}+\", \" \"))\n",
    "    ).\n",
    "    map(x => PorterStemmer.stem(x)).filter(_.trim.nonEmpty).toList\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class xml_doc (file_path: String) {\n",
    "    def get_doc(): xml.Elem = {\n",
    "        XML.loadFile(file_path: String)\n",
    "    }    \n",
    "    \n",
    "    def text() = {\n",
    "        (get_doc() \\\\ \"DOC\" \\\\ \"TEXT\").text\n",
    "    }\n",
    "    \n",
    "    def head() = {\n",
    "        (get_doc() \\\\ \"DOC\" \\\\ \"HEAD\").text\n",
    "    }\n",
    "\n",
    "    def id() = {\n",
    "        (get_doc() \\\\ \"DOC\" \\\\ \"DOCNO\").text.trim\n",
    "    }\n",
    "    \n",
    "    def tokens() = {\n",
    "        token_filter(head() ++ text())\n",
    "    }\n",
    "    \n",
    "    def hash_tokens() = {\n",
    "        tokens().map(x => x.hashCode())\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def list_docs (path: String) = {  // : Array[java.io.File]\n",
    "        new java.io.File(path).listFiles.map(x => x.toString())\n",
    "    }\n",
    "val numPattern = \"[0-9]+\".r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val token_hash = MutHashMap[String, Int]() // token -> hash\n",
    "\n",
    "def create_hash_doc_subset(star_count: Int, end_count: Int,\n",
    "                           file_list: Array[String],\n",
    "                           token_hash_map: MutHashMap[String, Int] = token_hash) = {\n",
    "    val id_htoken = MutHashMap[Int, List[Int]]() // forward index, docID to tokens\n",
    "    val htoken_id = MutHashMap[Int, List[Int]]()  // inverse index, tokens to docID\n",
    "    val id_name = MutHashMap[Int, String]()  // inverse index, tokens to docID\n",
    "    val name_id = MutHashMap[String, Int]()  // inverse index, tokens to docID\n",
    "    var counter = star_count\n",
    "    while (counter < end_count){\n",
    "        var cur_doc = new xml_doc(file_list(counter))\n",
    "        // get token from XML, then hash, or create hashes \"on the fly\"\n",
    "        var cur_htoken = cur_doc.tokens.map(x => token_hash_map.getOrElseUpdate(x, token_hash_map.size))\n",
    "        id_htoken += counter -> cur_htoken\n",
    "        \n",
    "        // update the inverse mapping, from (hashed) tokens to docID\n",
    "        cur_htoken.distinct.foreach(\n",
    "            (token: Int) => htoken_id(token) = htoken_id.getOrElseUpdate(token, List[Int]()) ++ List(counter)\n",
    "        )\n",
    "        \n",
    "        id_name(counter) = cur_doc.id\n",
    "        name_id(cur_doc.id) = counter\n",
    "        \n",
    "        counter += 1\n",
    "        if (counter % 100 == 0) println(s\"iteration $counter\")\n",
    "    }\n",
    "    (id_htoken, htoken_id, token_hash_map, id_name, name_id)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def write_int_to_intList(data: MutHashMap[Int, List[Int]], filename: String) = {\n",
    "\n",
    "    val bw = new BufferedWriter(new FileWriter(new File(filename)))\n",
    "    val iter = data.keys.iterator\n",
    "    while(iter.hasNext){\n",
    "        var elem = iter.next()\n",
    "        var values = data(elem).toList\n",
    "//         if(values.length>0){\n",
    "            bw.write(elem+\" \"+values.mkString(\" \"))\n",
    "            bw.newLine\n",
    "//         }    \n",
    "    }   \n",
    "    bw.close()\n",
    "}\n",
    "\n",
    "def write_int_string(data: MutHashMap[Int, String], filename: String) = {\n",
    "\n",
    "    val bw = new BufferedWriter(new FileWriter(new File(filename)))\n",
    "    val iter = data.keys.iterator\n",
    "    while(iter.hasNext){\n",
    "        var elem = iter.next()\n",
    "        var values = data(elem).toList\n",
    "        if(values.length>0){\n",
    "            bw.write(elem+\" \"+values.mkString(\"\"))\n",
    "            bw.newLine\n",
    "        }    \n",
    "    }   \n",
    "    bw.close()\n",
    "}\n",
    "\n",
    "def write_string_int(data: MutHashMap[String, Int], filename: String) = {\n",
    "\n",
    "    val bw = new BufferedWriter(new FileWriter(new File(filename)))\n",
    "    val iter = data.keys.iterator\n",
    "    while(iter.hasNext){\n",
    "        var elem = iter.next()\n",
    "        bw.write(elem+\" \"+data(elem).toString)\n",
    "        bw.newLine\n",
    "    }   \n",
    "    bw.close()\n",
    "}\n",
    "\n",
    "def write_int_to_int(data: MutHashMap[Int, Int], filename: String) = {\n",
    "\n",
    "    val bw = new BufferedWriter(new FileWriter(new File(filename)))\n",
    "    val iter = data.keys.iterator\n",
    "    while(iter.hasNext){\n",
    "        var elem = iter.next()\n",
    "        var value = data(elem)\n",
    "            bw.write(elem+\" \"+value)\n",
    "            bw.newLine\n",
    "    }   \n",
    "    bw.close()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_mutmap_int_intList(path: String, mutmap: MutHashMap[Int, List[Int]]) = {\n",
    "    val lines = Source.fromFile(path).getLines.toList\n",
    "    for (line <- lines){\n",
    "        val line_split = line.split(\" \", -1).filter(_.trim.length > 0)\n",
    "        mutmap(line_split.head.toInt) = \n",
    "            line_split.tail.map(x => x.toInt).toList\n",
    "    }\n",
    "}\n",
    "\n",
    "def load_mutmap_int_string(path: String, mutmap: MutHashMap[Int, String]) = {\n",
    "    val lines = Source.fromFile(path).getLines.toList\n",
    "    for (line <- lines){\n",
    "        val line_split = line.split(\" \") // .filter(_.trim.length > 0)\n",
    "        mutmap(line_split.head.toInt) = \n",
    "            line_split.last\n",
    "    }\n",
    "}\n",
    "\n",
    "def load_mutmap_string_int(path: String, mutmap: MutHashMap[String, Int]) = {\n",
    "    val lines = Source.fromFile(path).getLines.toList\n",
    "    for (line <- lines){\n",
    "//         val line_split = line.split(\" \", -1)\n",
    "        val line_split = line.split(\" \") // .filter(_.trim.length > 0)\n",
    "        mutmap(line_split.head) = \n",
    "            line_split.last.toInt\n",
    "    }\n",
    "}\n",
    "\n",
    "def load_mutmap_int_int(path: String, mutmap: MutHashMap[Int, Int]) = {\n",
    "    val lines = Source.fromFile(path).getLines.toList\n",
    "    for (line <- lines){\n",
    "        val line_split = line.split(\" \", -1).filter(_.trim.length > 0)\n",
    "        mutmap(line_split.head.toInt) = \n",
    "            line_split.last.toInt\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val mb = 1024*1024\n",
    "val runtime = Runtime.getRuntime\n",
    "def print_memory() = {\n",
    "    println(s\"Used Memory:  \" + (runtime.totalMemory - runtime.freeMemory) / mb)\n",
    "    println(s\"Free Memory:  \" + runtime.freeMemory / mb)\n",
    "    println(s\"Total Memory: \" + runtime.totalMemory / mb)\n",
    "    println(s\"Max Memory:   \" + runtime.maxMemory / mb)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val train_list = list_docs(doc_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val PATH_id_htoken = files_path + \"id_htoken.txt\"\n",
    "val PATH_htoken_id = files_path + \"htoken_id.txt\"\n",
    "val PATH_id_name = files_path + \"id_name.txt\"\n",
    "val PATH_name_id = files_path + \"name_id.txt\"\n",
    "val PATH_token_hash = files_path + \"token_hash.txt\"\n",
    "\n",
    "val PATH_prun_htoken_collectfreq = files_path + \"prun_htoken_collectfreq.txt\"\n",
    "val PATH_prun_htoken_id = files_path + \"prun_htoken_id.txt\"\n",
    "val PATH_prun_id_htoken = files_path + \"prun_id_htoken.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data files and creating maps\n",
    "# # not run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "// time it\n",
    "timeit.start\n",
    "\n",
    "val (id_htoken, htoken_id, token_hash, \n",
    "     id_name, name_id) = create_hash_doc_subset(0, 100000, train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// time it\n",
    "timeit.uptonow / 60.0\n",
    "// 87.56585954758334 , in minutes with 6GB\n",
    "// 67.88821773650001 , in minutes with 7GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_memory()\n",
    "\n",
    "// Used Memory:  3981\n",
    "// Free Memory:  1814\n",
    "// Total Memory: 5796\n",
    "// Max Memory:   5796"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_int_to_intList(id_htoken, PATH_id_htoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_int_to_intList(htoken_id, PATH_htoken_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_int_string(id_name, PATH_id_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_string_int(name_id, PATH_name_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "write_string_int(token_hash, PATH_token_hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// time it\n",
    "timeit.start\n",
    "\n",
    "val id_htoken: MutHashMap[Int, List[Int]] = MutHashMap[Int, List[Int]]()\n",
    "val htoken_id: MutHashMap[Int, List[Int]] = MutHashMap[Int, List[Int]]()\n",
    "val id_name: MutHashMap[Int, String] = MutHashMap[Int, String]()\n",
    "val token_hash: MutHashMap[String, Int] = MutHashMap[String, Int]()\n",
    "val name_id: MutHashMap[String, Int] = MutHashMap[String, Int]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "load_mutmap_int_intList(PATH_id_htoken, id_htoken)\n",
    "load_mutmap_int_intList(PATH_htoken_id, htoken_id)\n",
    "load_mutmap_int_string(PATH_id_name, id_name)\n",
    "load_mutmap_string_int(PATH_token_hash, token_hash)\n",
    "load_mutmap_string_int(PATH_name_id, name_id)\n",
    "\n",
    "// confirm load successful\n",
    "// test_load_mutmap_id_htoken == id_htoken\n",
    "// test_load_mutmap_htoken_id == htoken_id\n",
    "// test_load_mutmap_id_name == id_name\n",
    "// test_load_mutmap_token_hash == token_hash\n",
    "// test_load_mutmap_name_id == name_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// time it\n",
    "timeit.uptonow / 60.0\n",
    "// 1.4485827969833334 , in minutes\n",
    "// 1.3195113773833334 , in minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_memory()\n",
    "\n",
    "// Used Memory:  3468\n",
    "// Free Memory:  649\n",
    "// Total Memory: 4117\n",
    "// Max Memory:   5461"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prune vocabulary, collection and document frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "// htoken_id.mapValues(v => v.length).size\n",
    "// 1356183\n",
    "// htoken_id.mapValues(v => v.length).filter(_._2 > 5 - 1).size\n",
    "// 176866\n",
    "// reduction factor of ~7.67\n",
    "\n",
    "val prun_threshold = 5\n",
    "val pruned_token_set = htoken_id.mapValues(v => v.length).\n",
    "    filter(_._2 > prun_threshold - 1).keys.toSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "// time it\n",
    "timeit.start\n",
    "\n",
    "val prun_htoken_collectfreq: MutHashMap[Int, Int] = \n",
    "    MutHashMap(\n",
    "        id_htoken.flatMap{ case (k,v) => v.filter(pruned_token_set.contains(_)) }.\n",
    "        groupBy(identity).mapValues(_.size)\n",
    "        .toSeq:_*)\n",
    "\n",
    "prun_htoken_collectfreq.size\n",
    "\n",
    "timeit.uptonow / 60.0\n",
    "// 7.0919255263  , in minutes\n",
    "// 0.39257662956666667 , in minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "// time it\n",
    "timeit.start\n",
    "\n",
    "val prun_htoken_id: MutHashMap[Int, List[Int]] = \n",
    "    MutHashMap(\n",
    "        htoken_id.filterKeys(\n",
    "            pruned_token_set.contains(_)\n",
    "        ).toSeq:_*)\n",
    "\n",
    "prun_htoken_id.size\n",
    "\n",
    "timeit.uptonow / 60.0\n",
    "// 0.012546176999999999 , in minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// time it\n",
    "timeit.start\n",
    "\n",
    "val prun_id_htoken: MutHashMap[Int, List[Int]] = \n",
    "    MutHashMap(\n",
    "//         id_htoken.flatMap{ case (k,v) => (k, v.filter(pruned_token_set.contains(_))) }.\n",
    "        id_htoken.mapValues{ v => v.filter(pruned_token_set.contains(_)) }.\n",
    "        toSeq:_*)\n",
    "\n",
    "prun_id_htoken.size\n",
    "\n",
    "timeit.uptonow / 60.0\n",
    "// 0.20076514550000002 , in minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save pruned results to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_int_to_int(prun_htoken_collectfreq, PATH_prun_htoken_collectfreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_int_to_intList(prun_htoken_id, PATH_prun_htoken_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_int_to_intList(prun_id_htoken, PATH_prun_id_htoken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load maps (pruned)\n",
    "## # start from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// time it\n",
    "timeit.start\n",
    "\n",
    "val prun_htoken_collectfreq: MutHashMap[Int, Int] = MutHashMap[Int, Int]()\n",
    "val prun_id_htoken: MutHashMap[Int, List[Int]] = MutHashMap[Int, List[Int]]()\n",
    "val prun_htoken_id: MutHashMap[Int, List[Int]] = MutHashMap[Int, List[Int]]()\n",
    "val id_name: MutHashMap[Int, String] = MutHashMap[Int, String]()\n",
    "val token_hash: MutHashMap[String, Int] = MutHashMap[String, Int]()\n",
    "val name_id: MutHashMap[String, Int] = MutHashMap[String, Int]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "load_mutmap_int_int(PATH_prun_htoken_collectfreq, prun_htoken_collectfreq)\n",
    "load_mutmap_int_intList(PATH_prun_id_htoken, prun_id_htoken)\n",
    "load_mutmap_int_intList(PATH_prun_htoken_id, prun_htoken_id)\n",
    "load_mutmap_int_string(PATH_id_name, id_name)\n",
    "load_mutmap_string_int(PATH_token_hash, token_hash)\n",
    "load_mutmap_string_int(PATH_name_id, name_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// time it\n",
    "timeit.uptonow / 60.0\n",
    "// 2.1530588158666664 , in minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_memory()\n",
    "\n",
    "// Used Memory:  2569\n",
    "// Free Memory:  1782\n",
    "// Total Memory: 4352\n",
    "// Max Memory:   5461"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Queries & Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "// requires: having added tinyir to classpath, having added the qrels, i.e. \"relevance-judgements.csv\" in root \n",
    "// builds truth, an object, whose only method .judgements(\"query-ID\") returns the set of all document-IDs deemed \n",
    "// relevant to that query, note that these document-IDs are provided as List[String]\n",
    "// observe that query-ID is a string of an integer between 51 and 90 -> 40 queries in total\n",
    "import ch.ethz.dal.tinyir.lectures._\n",
    "val truth = new TipsterGroundTruth(files_path + \"/relevance-judgements.csv\")\n",
    "\n",
    "// how to use it, example:\n",
    "truth.judgements(\"51\")\n",
    "// observe that the size of relevant documents varies between queries, with the minimum being 52 and the maximum 894\n",
    "truth.judgements.values.map(x => x.size).min\n",
    "truth.judgements.values.map(x => x.size).max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "// requires: having added the file \"questions-descriptions.txt\" to source\n",
    "// This cell will build a list (can be Stream if required) of query tokens. \n",
    "// Note that the 16 is hard-coded to ignore the first 15 characters of these <title> line, which all read \n",
    "// \"<title> Topic: \"\n",
    "import scala.io.Source\n",
    "val numPattern = \"[0-9]+\".r\n",
    "\n",
    "val title = Source.fromFile(files_path +\"questions-descriptions.txt\").getLines().filter(_.startsWith(\"<title>\"))\n",
    "                .map(_.substring(16).trim).map(x => token_filter(x)).toList\n",
    "\n",
    "val num = Source.fromFile(files_path +\"questions-descriptions.txt\").getLines().filter(_.startsWith(\"<num>\"))   \n",
    "                .map(x => numPattern.findFirstIn(x.toString).get.substring(1)).toList\n",
    "\n",
    "val query = num zip title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Term-Frequency Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// DEFINE AUXILLARY FUNCTIONS\n",
    "\n",
    "// get inverse-document frequency (idf)\n",
    "// is defined as the logarithmically scaled inverse fraction of the documents that contain the word, \n",
    "// obtained by dividing the total number of documents by the number of documents containing the term, and then \n",
    "// taking the logarithm of that quotient.\n",
    "\n",
    "def hash_query(query: (String, List[String])) = {\n",
    "    (query._1, query._2.map(x => token_hash.getOrElse(x,-1)).filter(prun_htoken_id.keys.toSet.contains(_)).toSet)\n",
    "}\n",
    "\n",
    "val corpus_size = prun_id_htoken.size\n",
    "def get_idf(query: Set[Int]) = {\n",
    "    query.map(x => x -> Math.log(corpus_size / prun_htoken_id(x).size)).toMap\n",
    "}\n",
    "\n",
    "// get term frequency in a specific document (doc)\n",
    "def get_tf(query: Set[Int],doc: Int) = {\n",
    "    prun_id_htoken(doc).filter(query.contains(_)).groupBy(identity).mapValues(_.size)\n",
    "}\n",
    "\n",
    "// get tf-idf is defined as tf-idf = tf * idf\n",
    "def get_tf_idf(query: Set[Int],doc:Int) = {\n",
    "    get_tf(query,doc).map(x => x._1 -> x._2 * get_idf(query).getOrElse(x._1,0.toDouble))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Handle a Query --> take in a query, produce a ranking\n",
    "def handle(query: (String, List[String])) = {\n",
    "    val hashed_query = hash_query(query)\n",
    "    val doc_set = hashed_query._2.map(x => prun_htoken_id(x)).flatten.toSet\n",
    "    val ranking = doc_set.map(x => x -> get_tf_idf(hashed_query._2,x).values.sum).toSeq.sortBy(-_._2)\n",
    "                    .take(100).map(x => x._1).toList    \n",
    "    (query._1,ranking)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "// test it out.\n",
    "handle(query(35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "// Does it work on mass-answering queries?\n",
    "// It takes nearly 8 minutes though, so about 12 seconds per query on average. \n",
    "// Potential speed improvements: Write a function that reduces document collection in the first place.\n",
    "timeit.start\n",
    "val answers = query.map(x => handle(x)).toMap.mapValues(_.map(x => id_name(x)))\n",
    "timeit.uptonow / 60.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// The object Inspector contains all functions required to calculate the evaluation metrics (Precision, Recall, \n",
    "// F1-Score and MAP (mean average precision))\n",
    "\n",
    "object Inspector\n",
    "{\n",
    "// calculates average precision for a given answer (returned result of query)\n",
    "def badass1(retriev2: List[String], relev: Array[String], bounded: Boolean=false): Double ={\n",
    "    val retriev = retriev2.map(_.replace(\"-\", \"\"))\n",
    "    // remember to remove the \"-\" hyphens from the prediction for comparison purposes\n",
    "    (retriev.map(relev.contains(_)) // produces a boolean list with true where element belongs to relevant\n",
    "        .scanLeft(0){case (sum, next) => if(next) sum + 1 else sum}.tail // creates cumulative count of the booleans\n",
    "        .zipWithIndex.map(x => x._1.toDouble / (x._2 + 1)) // calculates average precision for each element\n",
    "        .zip(retriev.map(relev.contains(_))) // combines average precision with the boolean list from the start\n",
    "        .filter(_._2) // to filter out the ones that are not relevant \n",
    "        .map(_._1).sum // calculates the numerator (sums up the precision for all elements that are relevant)\n",
    "        )/ (if (bounded) retriev.size else relev.size) // divides by numerator (depending on bounded or not)\n",
    "    }\n",
    "\n",
    "// calculates mean average precision over a set of queries. \n",
    "def badass2(retriev_all: Map[String, List[String]], relev_all: Map[String, Array[String]], \n",
    "            bounded: Boolean=false): Double = {\n",
    "    (retriev_all.map(x => Inspector.badass1(x._2,relev_all(x._1),bounded)) // calculate average precision for each query\n",
    "    .sum)/(retriev_all.size) // calculates mean average precision (average precision over all queries)    \n",
    "}\n",
    "// Classic Precision and Recall for a given query, not striclty necessary. \n",
    "def evaluate(retriev: List[String], relev: Array[String])={\n",
    "    val TP = retriev.filter(relev.contains(_)).size.toDouble\n",
    "    val precision = TP / retriev.size\n",
    "    val recall = TP / relev.size\n",
    "    (precision,recall)\n",
    "}\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Example Usage\n",
    "val query_ID = \"51\"\n",
    "Inspector.badass1(answers(query_ID),truth.judgements(query_ID),bounded=true)\n",
    "Inspector.badass2(answers,truth.judgements,bounded=true) // answers are my predictions, truth.judgements from tinyIR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// investigate which queries cause problems.\n",
    "val query_ID = \"67\"\n",
    "truth.judgements(query_ID).size // How many relevant documents exist for that query?\n",
    "\n",
    "// Give a list of (AP, query_ID) for some easy investigation\n",
    "answers.map(x => Inspector.badass1(x._2,truth.judgements(x._1),bounded=true)).zip(answers.keys)//.filter(_._1<0.1)//.size\n",
    "\n",
    "// Look at a particular original query:\n",
    "query.toMap.get(query_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language model score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "answers.keys.toList.zipWith(answers.keys.toList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "answers.map(x => Inspector.badass1(x._2,truth.judgements(x._1),bounded=true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "// // collection tf\n",
    "val collection_size = prun_htoken_collectfreq.foldLeft(0)(_+_._2)\n",
    "\n",
    "val pruned_token_set = prun_htoken_collectfreq.keys.toSet\n",
    "\n",
    "// discards log scores\n",
    "def unfold_name_time(score :(List[(String, Double)], Double)) = {\n",
    "    (score._1.unzip._1, score._2)\n",
    "}\n",
    "\n",
    "// returns id's of docs in which the most query tokens appear in\n",
    "def reduce_candidate_doc(query: (String, List[Int]), \n",
    "                         prun_htoken_id: MutHashMap[Int, List[Int]] = prun_htoken_id, \n",
    "                         take_k_results: Int = 100) = {\n",
    "    \n",
    "    // doc_id -> # of occurances\n",
    "    val doc_occurance = query._2.flatMap(token => prun_htoken_id(token)).\n",
    "        groupBy(identity).mapValues(_.size)\n",
    "    \n",
    "    // sorted iterator of # of occurances\n",
    "    val intersect_value = doc_occurance.values.toSet.toList.sorted.reverse.toIterator\n",
    "    var iter = intersect_value.next\n",
    "    \n",
    "    var cur_doc_occurance = doc_occurance.filter(_._2 > iter - 1)\n",
    "    \n",
    "    while((cur_doc_occurance.size < take_k_results) & (intersect_value.hasNext)) {\n",
    "        iter = intersect_value.next\n",
    "        cur_doc_occurance = doc_occurance.filter(_._2 > iter - 1)\n",
    "    }\n",
    "    cur_doc_occurance.keys.toList\n",
    "}\n",
    "\n",
    "val query_hash = query.map{ \n",
    "    case (id, str) => (id, str.\n",
    "                       flatMap(x => token_hash.get(x)).filter(pruned_token_set.contains(_))\n",
    "                      )}\n",
    "\n",
    "val lambda = 0.01 // smoothing parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lang_query(query: (String, List[Int]),\n",
    "               method: String = \"index\",\n",
    "               prun_htoken_collectfreq: MutHashMap[Int, Int] = prun_htoken_collectfreq,\n",
    "               collection_size: Int = collection_size,\n",
    "               lambda: Double = lambda, \n",
    "               prun_id_htoken: MutHashMap[Int, List[Int]] = prun_id_htoken, \n",
    "               prun_htoken_id: MutHashMap[Int, List[Int]] = prun_htoken_id, \n",
    "               take_k_results: Int = 100) = {\n",
    "    \n",
    "    // time it\n",
    "    timeit.start\n",
    "    \n",
    "    // list of doc id's containing tokens in query\n",
    "    def candidate_doc(): List[Int] = method match {\n",
    "        case \"index\" => reduce_candidate_doc(query = query, take_k_results = take_k_results)\n",
    "        case \"no_index\" => (0 to 100000-1).toList\n",
    "        case \"test\" => (1 to 2).toList\n",
    "        case _ => throw new Exception(\"Please choose either 'index' or 'no_index'\")\n",
    "    }\n",
    "    \n",
    "    // number of tokens in doc\n",
    "    def doc_size(doc_id: Int) = {\n",
    "        prun_id_htoken(doc_id).size\n",
    "    }\n",
    "    \n",
    "    // map of tokens to frequency in a given doc\n",
    "    def doc_tf_map(doc_id: Int) = {\n",
    "        prun_id_htoken(doc_id).\n",
    "            groupBy(identity).mapValues(_.size)\n",
    "    }\n",
    "    \n",
    "    // list of (relative) frequency of query tokens in a given doc\n",
    "    def doc_query_tf(doc_id: Int) = {\n",
    "        query._2.map(token => \n",
    "                     doc_tf_map(doc_id).getOrElse(token, 0).toDouble / doc_size(doc_id))\n",
    "    }\n",
    "    \n",
    "    // list of (relative) frequency of query tokens in the collection\n",
    "    def query_cf() = {\n",
    "        query._2.map(token => \n",
    "                     prun_htoken_collectfreq(token).toDouble / collection_size)\n",
    "    }\n",
    "    \n",
    "    // this only needs to be calculated once per query\n",
    "    // (wasteful to call function multiple times)\n",
    "    val cur_query_cf = query_cf()\n",
    "    \n",
    "    //\n",
    "    def smooth_prob(doc_id: Int) = {\n",
    "        doc_query_tf(doc_id).zip(cur_query_cf).\n",
    "            map{case (x, y) => (1 - lambda) * x + lambda * y}\n",
    "    }\n",
    "    \n",
    "    // sum log(x) elements of list\n",
    "    def doc_lang_score(doc_id: Int) = {\n",
    "        smooth_prob(doc_id).foldLeft(0.0)(_ + Math.log(_))\n",
    "    }\n",
    "    \n",
    "    val log_scores = candidate_doc().map(doc => \n",
    "                                         (id_name(doc), doc_lang_score(doc))\n",
    "                                         ).sortWith(_._2 > _._2)\n",
    "    \n",
    "    (log_scores.take(take_k_results), timeit.uptonow / 60.0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "// with raw log scores\n",
    "val lang_model_rank = query_hash.take(10).map(query => \n",
    "                       (query._1, unfold_name_time(\n",
    "                           lang_query(query, method = \"index\"))\n",
    "                       )\n",
    "                                                 ).toMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val lang_model_runtime = lang_model_rank.values.map(x => x._2)\n",
    "val lang_model_runtime_average = average(lang_model_runtime)\n",
    "\n",
    "// before reducing candidates\n",
    "// Iterable[Double] = List(\n",
    "//   0.3791437873666667,\n",
    "//   0.5371110624833333,\n",
    "//   9.883984977816668,\n",
    "//   1.4138989739999999,\n",
    "//   2.41249985935\n",
    "// )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing ground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Scala 2.11",
   "language": "scala211",
   "name": "scala211"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
