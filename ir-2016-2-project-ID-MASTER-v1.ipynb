{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set your user name as a case, pointing to your path to documents and tinyir.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdoc_dir\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"/Users/Max/Coding/ETH/Information_Retrieval_AS16/scala_practice/documents\"\u001b[0m\n",
       "\u001b[36mfiles_path\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"/Users/Max/Coding/ETH/Information_Retrieval_AS16/scala_practice/files/\"\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// set your case once\n",
    "// doc_dir: location of xml documents\n",
    "// files_path: all other files, including generated maps are stored / loaded from here\n",
    "val (doc_dir: String, files_path: String) = System.getProperties().get(\"user.name\").toString match {\n",
    "    case \"Yarden-\"  => (\"../documents\", \"../\")\n",
    "    case \"Max\"  => (\"/Users/Max/Coding/ETH/Information_Retrieval_AS16/scala_practice/documents\", \"/Users/Max/Coding/ETH/Information_Retrieval_AS16/scala_practice/files/\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classpath.addPath(files_path + \"tinyir-1.1.jar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36mscala.xml.XML\u001b[0m\n",
       "\u001b[32mimport \u001b[36mch.ethz.dal.tinyir._\u001b[0m\n",
       "\u001b[32mimport \u001b[36mch.ethz.dal.tinyir.lectures._\u001b[0m\n",
       "\u001b[32mimport \u001b[36mcom.github.aztek.porterstemmer.PorterStemmer\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import scala.xml.XML\n",
    "import ch.ethz.dal.tinyir._\n",
    "import ch.ethz.dal.tinyir.lectures._\n",
    "import com.github.aztek.porterstemmer.PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36mscala.io.Source\u001b[0m\n",
       "\u001b[32mimport \u001b[36mjava.io._\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import scala.io.Source  // for importing txt files\n",
    "import java.io._  // for saving txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36mscala.collection.mutable.{Map => MutMap, HashMap => MutHashMap}\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import scala.collection.mutable.{Map => MutMap, HashMap => MutHashMap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mtimeit\u001b[0m: \u001b[32mutil\u001b[0m.\u001b[32mStopWatch\u001b[0m = ch.ethz.dal.tinyir.util.StopWatch@6aaefab4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val timeit = new util.StopWatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction \u001b[36maverage\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def average[T]( ts: Iterable[T] )( implicit num: Numeric[T] ) = {\n",
    "  num.toDouble( ts.sum ) / ts.size\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction \u001b[36mtoken_filter\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def token_filter(text_body: String) = {\n",
    "    processing.StopWords.filterOutSW(\n",
    "        processing.Tokenizer.tokenize(text_body.\n",
    "                                      replaceAll(\"\\\\P{L}+\", \" \"))\n",
    "    ).\n",
    "    map(x => PorterStemmer.stem(x)).filter(_.trim.nonEmpty).toList\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass \u001b[36mxml_doc\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class xml_doc (file_path: String) {\n",
    "    def get_doc(): xml.Elem = {\n",
    "        XML.loadFile(file_path: String)\n",
    "    }    \n",
    "    \n",
    "    def text() = {\n",
    "        (get_doc() \\\\ \"DOC\" \\\\ \"TEXT\").text\n",
    "    }\n",
    "    \n",
    "    def head() = {\n",
    "        (get_doc() \\\\ \"DOC\" \\\\ \"HEAD\").text\n",
    "    }\n",
    "\n",
    "    def id() = {\n",
    "        (get_doc() \\\\ \"DOC\" \\\\ \"DOCNO\").text.trim\n",
    "    }\n",
    "    \n",
    "    def tokens() = {\n",
    "        token_filter(head() ++ text())\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction \u001b[36mlist_docs\u001b[0m\n",
       "\u001b[36mnumPattern\u001b[0m: \u001b[32mscala\u001b[0m.\u001b[32mutil\u001b[0m.\u001b[32mmatching\u001b[0m.\u001b[32mRegex\u001b[0m = [0-9]+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def list_docs (path: String) = {\n",
    "        new java.io.File(path).listFiles.map(x => x.toString())\n",
    "    }\n",
    "val numPattern = \"[0-9]+\".r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mtoken_hash\u001b[0m: \u001b[32mcollection\u001b[0m.\u001b[32mmutable\u001b[0m.\u001b[32mHashMap\u001b[0m[\u001b[32mString\u001b[0m, \u001b[32mInt\u001b[0m] = \u001b[33mMap\u001b[0m()\n",
       "defined \u001b[32mfunction \u001b[36mcreate_hash_doc_subset\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// token -> hash (Int)\n",
    "val token_hash = MutHashMap[String, Int]() // token -> hash\n",
    "\n",
    "def create_hash_doc_subset(star_count: Int, end_count: Int,\n",
    "                           file_list: Array[String],\n",
    "                           token_hash_map: MutHashMap[String, Int] = token_hash) = {\n",
    "    val id_htoken = MutHashMap[Int, List[Int]]() // forward index, docID to tokens\n",
    "    val htoken_id = MutHashMap[Int, List[Int]]()  // inverse index, tokens to docID\n",
    "    val id_name = MutHashMap[Int, String]()  // inverse index, tokens to docID\n",
    "    val name_id = MutHashMap[String, Int]()  // inverse index, tokens to docID\n",
    "    var counter = star_count\n",
    "    while (counter < end_count){\n",
    "        var cur_doc = new xml_doc(file_list(counter))\n",
    "        // get token from XML, then hash, or create hashes \"on the fly\"\n",
    "        var cur_htoken = cur_doc.tokens.map(x => token_hash_map.getOrElseUpdate(x, token_hash_map.size))\n",
    "        id_htoken += counter -> cur_htoken\n",
    "        \n",
    "        // update the inverse mapping, from (hashed) tokens to docID\n",
    "        cur_htoken.distinct.foreach(\n",
    "            (token: Int) => htoken_id(token) = htoken_id.getOrElseUpdate(token, List[Int]()) ++ List(counter)\n",
    "        )\n",
    "        \n",
    "        id_name(counter) = cur_doc.id\n",
    "        name_id(cur_doc.id) = counter\n",
    "        \n",
    "        counter += 1\n",
    "        if (counter % 100 == 0) println(s\"iteration $counter\")\n",
    "    }\n",
    "    (id_htoken, htoken_id, token_hash_map, id_name, name_id)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction \u001b[36mwrite_int_to_intList\u001b[0m\n",
       "defined \u001b[32mfunction \u001b[36mwrite_int_string\u001b[0m\n",
       "defined \u001b[32mfunction \u001b[36mwrite_string_int\u001b[0m\n",
       "defined \u001b[32mfunction \u001b[36mwrite_int_to_int\u001b[0m\n",
       "defined \u001b[32mfunction \u001b[36mwrite_lang_model_search\u001b[0m\n",
       "defined \u001b[32mfunction \u001b[36mwrite_res\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// writing to file\n",
    "def write_int_to_intList(data: MutHashMap[Int, List[Int]], filename: String) = {\n",
    "\n",
    "    val bw = new BufferedWriter(new FileWriter(new File(filename)))\n",
    "    val iter = data.keys.iterator\n",
    "    while(iter.hasNext){\n",
    "        var elem = iter.next()\n",
    "        var values = data(elem).toList\n",
    "//         if(values.length>0){\n",
    "            bw.write(elem+\" \"+values.mkString(\" \"))\n",
    "            bw.newLine\n",
    "//         }    \n",
    "    }   \n",
    "    bw.close()\n",
    "}\n",
    "\n",
    "def write_int_string(data: MutHashMap[Int, String], filename: String) = {\n",
    "\n",
    "    val bw = new BufferedWriter(new FileWriter(new File(filename)))\n",
    "    val iter = data.keys.iterator\n",
    "    while(iter.hasNext){\n",
    "        var elem = iter.next()\n",
    "        var values = data(elem).toList\n",
    "        if(values.length>0){\n",
    "            bw.write(elem+\" \"+values.mkString(\"\"))\n",
    "            bw.newLine\n",
    "        }    \n",
    "    }   \n",
    "    bw.close()\n",
    "}\n",
    "\n",
    "def write_string_int(data: MutHashMap[String, Int], filename: String) = {\n",
    "\n",
    "    val bw = new BufferedWriter(new FileWriter(new File(filename)))\n",
    "    val iter = data.keys.iterator\n",
    "    while(iter.hasNext){\n",
    "        var elem = iter.next()\n",
    "        bw.write(elem+\" \"+data(elem).toString)\n",
    "        bw.newLine\n",
    "    }   \n",
    "    bw.close()\n",
    "}\n",
    "\n",
    "def write_int_to_int(data: MutHashMap[Int, Int], filename: String) = {\n",
    "\n",
    "    val bw = new BufferedWriter(new FileWriter(new File(filename)))\n",
    "    val iter = data.keys.iterator\n",
    "    while(iter.hasNext){\n",
    "        var elem = iter.next()\n",
    "        var value = data(elem)\n",
    "            bw.write(elem+\" \"+value)\n",
    "            bw.newLine\n",
    "    }   \n",
    "    bw.close()\n",
    "}\n",
    "\n",
    "def write_lang_model_search(data: List[(String, Double, Int, Double)], filename: String) = {\n",
    "\n",
    "    val bw = new BufferedWriter(new FileWriter(new File(filename)))\n",
    "    \n",
    "    data.foreach{\n",
    "        case (log_opt, lambda, candidate_size, score) => bw.write(Seq(log_opt, lambda, candidate_size, score).mkString(\" \"))\n",
    "        bw.newLine\n",
    "    }\n",
    "    bw.close()\n",
    "}\n",
    "\n",
    "// write results (ranking) to file\n",
    "// model should be {\"t\", \"l\"}\n",
    "def write_res(res: Map[String, List[String]],model: String=\"t\") = {\n",
    "    val file = new BufferedWriter(new FileWriter(new File(\"ranking-\"+model+\"-24.txt\")))\n",
    "    res.foreach{case (qId,doclist) => doclist.zipWithIndex // takes each qID, doclist pair to zip the list with an index\n",
    "                .foreach{case(name,rank) => file.write(qId+\" \"+(rank+1)+\" \"+name+\"\\n\")}} // self-explanatory\n",
    "    file.close()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction \u001b[36mload_mutmap_int_intList\u001b[0m\n",
       "defined \u001b[32mfunction \u001b[36mload_mutmap_int_string\u001b[0m\n",
       "defined \u001b[32mfunction \u001b[36mload_mutmap_string_int\u001b[0m\n",
       "defined \u001b[32mfunction \u001b[36mload_mutmap_int_int\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_mutmap_int_intList(path: String, mutmap: MutHashMap[Int, List[Int]]) = {\n",
    "    val lines = Source.fromFile(path).getLines.toList\n",
    "    for (line <- lines){\n",
    "        val line_split = line.split(\" \", -1).filter(_.trim.length > 0)\n",
    "        mutmap(line_split.head.toInt) = \n",
    "            line_split.tail.map(x => x.toInt).toList\n",
    "    }\n",
    "}\n",
    "\n",
    "def load_mutmap_int_string(path: String, mutmap: MutHashMap[Int, String]) = {\n",
    "    val lines = Source.fromFile(path).getLines.toList\n",
    "    for (line <- lines){\n",
    "        val line_split = line.split(\" \") // .filter(_.trim.length > 0)\n",
    "        mutmap(line_split.head.toInt) = \n",
    "            line_split.last\n",
    "    }\n",
    "}\n",
    "\n",
    "def load_mutmap_string_int(path: String, mutmap: MutHashMap[String, Int]) = {\n",
    "    val lines = Source.fromFile(path).getLines.toList\n",
    "    for (line <- lines){\n",
    "//         val line_split = line.split(\" \", -1)\n",
    "        val line_split = line.split(\" \") // .filter(_.trim.length > 0)\n",
    "        mutmap(line_split.head) = \n",
    "            line_split.last.toInt\n",
    "    }\n",
    "}\n",
    "\n",
    "def load_mutmap_int_int(path: String, mutmap: MutHashMap[Int, Int]) = {\n",
    "    val lines = Source.fromFile(path).getLines.toList\n",
    "    for (line <- lines){\n",
    "        val line_split = line.split(\" \", -1).filter(_.trim.length > 0)\n",
    "        mutmap(line_split.head.toInt) = \n",
    "            line_split.last.toInt\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mmb\u001b[0m: \u001b[32mInt\u001b[0m = \u001b[32m1048576\u001b[0m\n",
       "\u001b[36mruntime\u001b[0m: \u001b[32mRuntime\u001b[0m = java.lang.Runtime@63d4150d\n",
       "defined \u001b[32mfunction \u001b[36mprint_memory\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val mb = 1024*1024\n",
    "val runtime = Runtime.getRuntime\n",
    "def print_memory() = {\n",
    "    println(s\"Used Memory:  \" + (runtime.totalMemory - runtime.freeMemory) / mb)\n",
    "    println(s\"Free Memory:  \" + runtime.freeMemory / mb)\n",
    "    println(s\"Total Memory: \" + runtime.totalMemory / mb)\n",
    "    println(s\"Max Memory:   \" + runtime.maxMemory / mb)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mtrain_list\u001b[0m: \u001b[32mArray\u001b[0m[\u001b[32mString\u001b[0m] = \u001b[33mArray\u001b[0m(\n",
       "  \u001b[32m\"/Users/Max/Coding/ETH/Information_Retrieval_AS16/scala_practice/documents/AP880212-0006\"\u001b[0m,\n",
       "  \u001b[32m\"/Users/Max/Coding/ETH/Information_Retrieval_AS16/scala_practice/documents/AP880212-0007\"\u001b[0m,\n",
       "  \u001b[32m\"/Users/Max/Coding/ETH/Information_Retrieval_AS16/scala_practice/documents/AP880212-0009\"\u001b[0m,\n",
       "  \u001b[32m\"/Users/Max/Coding/ETH/Information_Retrieval_AS16/scala_practice/documents/AP880212-0017\"\u001b[0m,\n",
       "  \u001b[32m\"/Users/Max/Coding/ETH/Information_Retrieval_AS16/scala_practice/documents/AP880212-0018\"\u001b[0m,\n",
       "  \u001b[32m\"/Users/Max/Coding/ETH/Information_Retrieval_AS16/scala_practice/documents/AP880212-0022\"\u001b[0m,\n",
       "  \u001b[32m\"/Users/Max/Coding/ETH/Information_Retrieval_AS16/scala_practice/documents/AP880212-0031\"\u001b[0m,\n",
       "  \u001b[32m\"/Users/Max/Coding/ETH/Information_Retrieval_AS16/scala_practice/documents/AP880212-0039\"\u001b[0m,\n",
       "  \u001b[32m\"/Users/Max/Coding/ETH/Information_Retrieval_AS16/scala_practice/documents/AP880212-0042\"\u001b[0m,\n",
       "  \u001b[32m\"/Users/Max/Coding/ETH/Information_Retrieval_AS16/scala_practice/documents/AP8\u001b[33m...\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val train_list = list_docs(doc_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mPATH_id_htoken\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"/Users/Max/Coding/ETH/Information_Retrieval_AS16/scala_practice/files/id_htoken.txt\"\u001b[0m\n",
       "\u001b[36mPATH_htoken_id\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"/Users/Max/Coding/ETH/Information_Retrieval_AS16/scala_practice/files/htoken_id.txt\"\u001b[0m\n",
       "\u001b[36mPATH_id_name\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"/Users/Max/Coding/ETH/Information_Retrieval_AS16/scala_practice/files/id_name.txt\"\u001b[0m\n",
       "\u001b[36mPATH_name_id\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"/Users/Max/Coding/ETH/Information_Retrieval_AS16/scala_practice/files/name_id.txt\"\u001b[0m\n",
       "\u001b[36mPATH_token_hash\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"/Users/Max/Coding/ETH/Information_Retrieval_AS16/scala_practice/files/token_hash.txt\"\u001b[0m\n",
       "\u001b[36mPATH_prun_htoken_collectfreq\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"/Users/Max/Coding/ETH/Information_Retrieval_AS16/scala_practice/files/prun_htoken_collectfreq.txt\"\u001b[0m\n",
       "\u001b[36mPATH_prun_htoken_id\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"/Users/Max/Coding/ETH/Information_Retrieval_AS16/scala_practice/files/prun_htoken_id.txt\"\u001b[0m\n",
       "\u001b[36mPATH_prun_id_htoken\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"/Users/Max/Coding/ETH/Information_Retrieval_AS16/scala_practice/files/prun_id_htoken.txt\"\u001b[0m\n",
       "\u001b[36mPATH_lang_model_search\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"/Users/Max/Coding/ETH/Information_Retrieval_AS16/scala_practice/files/lang_model_search.txt\"\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val PATH_id_htoken = files_path + \"id_htoken.txt\"\n",
    "val PATH_htoken_id = files_path + \"htoken_id.txt\"\n",
    "val PATH_id_name = files_path + \"id_name.txt\"\n",
    "val PATH_name_id = files_path + \"name_id.txt\"\n",
    "val PATH_token_hash = files_path + \"token_hash.txt\"\n",
    "\n",
    "val PATH_prun_htoken_collectfreq = files_path + \"prun_htoken_collectfreq.txt\"\n",
    "val PATH_prun_htoken_id = files_path + \"prun_htoken_id.txt\"\n",
    "val PATH_prun_id_htoken = files_path + \"prun_id_htoken.txt\"\n",
    "\n",
    "val PATH_lang_model_search = files_path + \"lang_model_search.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data files and creating maps\n",
    "# # not run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "// time it\n",
    "timeit.start\n",
    "\n",
    "val (id_htoken, htoken_id, token_hash, \n",
    "     id_name, name_id) = create_hash_doc_subset(0, 100000, train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// time it\n",
    "timeit.uptonow / 60.0\n",
    "// 87.56585954758334 , in minutes with 6GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_memory()\n",
    "\n",
    "// Used Memory:  3981\n",
    "// Free Memory:  1814\n",
    "// Total Memory: 5796\n",
    "// Max Memory:   5796"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_int_to_intList(id_htoken, PATH_id_htoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_int_to_intList(htoken_id, PATH_htoken_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_int_string(id_name, PATH_id_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_string_int(name_id, PATH_name_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "write_string_int(token_hash, PATH_token_hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// time it\n",
    "timeit.start\n",
    "\n",
    "val id_htoken: MutHashMap[Int, List[Int]] = MutHashMap[Int, List[Int]]()\n",
    "val htoken_id: MutHashMap[Int, List[Int]] = MutHashMap[Int, List[Int]]()\n",
    "val id_name: MutHashMap[Int, String] = MutHashMap[Int, String]()\n",
    "val token_hash: MutHashMap[String, Int] = MutHashMap[String, Int]()\n",
    "val name_id: MutHashMap[String, Int] = MutHashMap[String, Int]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "load_mutmap_int_intList(PATH_id_htoken, id_htoken)\n",
    "load_mutmap_int_intList(PATH_htoken_id, htoken_id)\n",
    "load_mutmap_int_string(PATH_id_name, id_name)\n",
    "load_mutmap_string_int(PATH_token_hash, token_hash)\n",
    "load_mutmap_string_int(PATH_name_id, name_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// time it\n",
    "timeit.uptonow / 60.0\n",
    "// 1.4485827969833334 , in minutes\n",
    "// 1.3195113773833334 , in minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_memory()\n",
    "\n",
    "// Used Memory:  3468\n",
    "// Free Memory:  649\n",
    "// Total Memory: 4117\n",
    "// Max Memory:   5461"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prune vocabulary, collection and document frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "// htoken_id.mapValues(v => v.length).size\n",
    "// 1356183\n",
    "// htoken_id.mapValues(v => v.length).filter(_._2 > 5 - 1).size\n",
    "// 176866\n",
    "// reduction factor of ~7.67\n",
    "\n",
    "val prun_threshold = 5\n",
    "// prun tokens that have document freq >= 5\n",
    "val pruned_token_set = htoken_id.mapValues(v => v.length).\n",
    "    filter(_._2 > prun_threshold - 1).keys.toSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "// time it\n",
    "timeit.start\n",
    "\n",
    "val prun_htoken_collectfreq: MutHashMap[Int, Int] = \n",
    "    MutHashMap(\n",
    "        id_htoken.flatMap{ case (k,v) => v.filter(pruned_token_set.contains(_)) }.\n",
    "        groupBy(identity).mapValues(_.size)\n",
    "        .toSeq:_*)\n",
    "\n",
    "prun_htoken_collectfreq.size\n",
    "\n",
    "timeit.uptonow / 60.0\n",
    "// 7.0919255263  , in minutes\n",
    "// 0.39257662956666667 , in minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "// time it\n",
    "timeit.start\n",
    "\n",
    "val prun_htoken_id: MutHashMap[Int, List[Int]] = \n",
    "    MutHashMap(\n",
    "        htoken_id.filterKeys(\n",
    "            pruned_token_set.contains(_)\n",
    "        ).toSeq:_*)\n",
    "\n",
    "prun_htoken_id.size\n",
    "\n",
    "timeit.uptonow / 60.0\n",
    "// 0.012546176999999999 , in minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// time it\n",
    "timeit.start\n",
    "\n",
    "val prun_id_htoken: MutHashMap[Int, List[Int]] = \n",
    "    MutHashMap(\n",
    "//         id_htoken.flatMap{ case (k,v) => (k, v.filter(pruned_token_set.contains(_))) }.\n",
    "        id_htoken.mapValues{ v => v.filter(pruned_token_set.contains(_)) }.\n",
    "        toSeq:_*)\n",
    "\n",
    "prun_id_htoken.size\n",
    "\n",
    "timeit.uptonow / 60.0\n",
    "// 0.20076514550000002 , in minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save pruned results to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_int_to_int(prun_htoken_collectfreq, PATH_prun_htoken_collectfreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_int_to_intList(prun_htoken_id, PATH_prun_htoken_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_int_to_intList(prun_id_htoken, PATH_prun_id_htoken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load maps (pruned)\n",
    "## # start from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mprun_htoken_collectfreq\u001b[0m: \u001b[32mcollection\u001b[0m.\u001b[32mmutable\u001b[0m.\u001b[32mHashMap\u001b[0m[\u001b[32mInt\u001b[0m, \u001b[32mInt\u001b[0m] = \u001b[33mMap\u001b[0m()\n",
       "\u001b[36mprun_id_htoken\u001b[0m: \u001b[32mcollection\u001b[0m.\u001b[32mmutable\u001b[0m.\u001b[32mHashMap\u001b[0m[\u001b[32mInt\u001b[0m, \u001b[32mList\u001b[0m[\u001b[32mInt\u001b[0m]] = \u001b[33mMap\u001b[0m()\n",
       "\u001b[36mprun_htoken_id\u001b[0m: \u001b[32mcollection\u001b[0m.\u001b[32mmutable\u001b[0m.\u001b[32mHashMap\u001b[0m[\u001b[32mInt\u001b[0m, \u001b[32mList\u001b[0m[\u001b[32mInt\u001b[0m]] = \u001b[33mMap\u001b[0m()\n",
       "\u001b[36mid_name\u001b[0m: \u001b[32mcollection\u001b[0m.\u001b[32mmutable\u001b[0m.\u001b[32mHashMap\u001b[0m[\u001b[32mInt\u001b[0m, \u001b[32mString\u001b[0m] = \u001b[33mMap\u001b[0m()\n",
       "\u001b[36mtoken_hash\u001b[0m: \u001b[32mcollection\u001b[0m.\u001b[32mmutable\u001b[0m.\u001b[32mHashMap\u001b[0m[\u001b[32mString\u001b[0m, \u001b[32mInt\u001b[0m] = \u001b[33mMap\u001b[0m()\n",
       "\u001b[36mname_id\u001b[0m: \u001b[32mcollection\u001b[0m.\u001b[32mmutable\u001b[0m.\u001b[32mHashMap\u001b[0m[\u001b[32mString\u001b[0m, \u001b[32mInt\u001b[0m] = \u001b[33mMap\u001b[0m()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// time it\n",
    "timeit.start\n",
    "\n",
    "val prun_htoken_collectfreq: MutHashMap[Int, Int] = MutHashMap[Int, Int]()\n",
    "val prun_id_htoken: MutHashMap[Int, List[Int]] = MutHashMap[Int, List[Int]]()\n",
    "val prun_htoken_id: MutHashMap[Int, List[Int]] = MutHashMap[Int, List[Int]]()\n",
    "val id_name: MutHashMap[Int, String] = MutHashMap[Int, String]()\n",
    "val token_hash: MutHashMap[String, Int] = MutHashMap[String, Int]()\n",
    "val name_id: MutHashMap[String, Int] = MutHashMap[String, Int]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load_mutmap_int_int(PATH_prun_htoken_collectfreq, prun_htoken_collectfreq)\n",
    "load_mutmap_int_intList(PATH_prun_id_htoken, prun_id_htoken)\n",
    "load_mutmap_int_intList(PATH_prun_htoken_id, prun_htoken_id)\n",
    "load_mutmap_int_string(PATH_id_name, id_name)\n",
    "load_mutmap_string_int(PATH_token_hash, token_hash)\n",
    "load_mutmap_string_int(PATH_name_id, name_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres18\u001b[0m: \u001b[32mDouble\u001b[0m = \u001b[32m1.0760687918333334\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// time it\n",
    "timeit.uptonow / 60.0\n",
    "// 2.1530588158666664 , in minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Memory:  2939\n",
      "Free Memory:  701\n",
      "Total Memory: 3641\n",
      "Max Memory:   3641\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_memory()\n",
    "\n",
    "// Used Memory:  2569\n",
    "// Free Memory:  1782\n",
    "// Total Memory: 4352\n",
    "// Max Memory:   5461"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Queries & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mtruth\u001b[0m: \u001b[32mTipsterGroundTruth\u001b[0m = ch.ethz.dal.tinyir.lectures.TipsterGroundTruth@7850977d\n",
       "\u001b[36mres20_1\u001b[0m: \u001b[32mArray\u001b[0m[\u001b[32mString\u001b[0m] = \u001b[33mArray\u001b[0m(\n",
       "  \u001b[32m\"AP8803010271\"\u001b[0m,\n",
       "  \u001b[32m\"AP8803020275\"\u001b[0m,\n",
       "  \u001b[32m\"AP8803110301\"\u001b[0m,\n",
       "  \u001b[32m\"AP8803160292\"\u001b[0m,\n",
       "  \u001b[32m\"AP8803180287\"\u001b[0m,\n",
       "  \u001b[32m\"AP8803250293\"\u001b[0m,\n",
       "  \u001b[32m\"AP8804060267\"\u001b[0m,\n",
       "  \u001b[32m\"AP8804070258\"\u001b[0m,\n",
       "  \u001b[32m\"AP8804120268\"\u001b[0m,\n",
       "  \u001b[32m\"AP8804280301\"\u001b[0m,\n",
       "  \u001b[32m\"AP8806270045\"\u001b[0m,\n",
       "  \u001b[32m\"AP8806270093\"\u001b[0m,\n",
       "  \u001b[32m\"AP8806280097\"\u001b[0m,\n",
       "  \u001b[32m\"AP8806280170\"\u001b[0m,\n",
       "  \u001b[32m\"AP8806280310\"\u001b[0m,\n",
       "  \u001b[32m\"AP8807060311\"\u001b[0m,\n",
       "  \u001b[32m\"AP8807310085\"\u001b[0m,\n",
       "  \u001b[32m\"AP8809220206\"\u001b[0m,\n",
       "  \u001b[32m\"AP8809260235\"\u001b[0m,\n",
       "\u001b[33m...\u001b[0m\n",
       "\u001b[36mres20_2\u001b[0m: \u001b[32mInt\u001b[0m = \u001b[32m52\u001b[0m\n",
       "\u001b[36mres20_3\u001b[0m: \u001b[32mInt\u001b[0m = \u001b[32m894\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// requires: having added tinyir to classpath, having added the qrels, i.e. \"relevance-judgements.csv\" in root \n",
    "// builds truth, an object, whose only method .judgements(\"query-ID\") returns the set of all document-IDs deemed \n",
    "// relevant to that query, note that these document-IDs are provided as List[String]\n",
    "// observe that query-ID is a string of an integer between 51 and 90 -> 40 queries in total\n",
    "val truth = new TipsterGroundTruth(files_path + \"/relevance-judgements.csv\")\n",
    "\n",
    "// how to use it, example:\n",
    "truth.judgements(\"51\")\n",
    "// observe that the size of relevant documents varies between queries, with the minimum being 52 and the maximum 894\n",
    "truth.judgements.values.map(x => x.size).min\n",
    "truth.judgements.values.map(x => x.size).max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mtitle\u001b[0m: \u001b[32mList\u001b[0m[\u001b[32mList\u001b[0m[\u001b[32mString\u001b[0m]] = \u001b[33mList\u001b[0m(\n",
       "  \u001b[33mList\u001b[0m(\u001b[32m\"airbu\"\u001b[0m, \u001b[32m\"subsidi\"\u001b[0m),\n",
       "  \u001b[33mList\u001b[0m(\u001b[32m\"south\"\u001b[0m, \u001b[32m\"african\"\u001b[0m, \u001b[32m\"sanction\"\u001b[0m),\n",
       "  \u001b[33mList\u001b[0m(\u001b[32m\"leverag\"\u001b[0m, \u001b[32m\"buyout\"\u001b[0m),\n",
       "  \u001b[33mList\u001b[0m(\u001b[32m\"satellit\"\u001b[0m, \u001b[32m\"launch\"\u001b[0m, \u001b[32m\"contract\"\u001b[0m),\n",
       "  \u001b[33mList\u001b[0m(\u001b[32m\"insid\"\u001b[0m, \u001b[32m\"trade\"\u001b[0m),\n",
       "  \u001b[33mList\u001b[0m(\u001b[32m\"prime\"\u001b[0m, \u001b[32m\"lend\"\u001b[0m, \u001b[32m\"rate\"\u001b[0m, \u001b[32m\"move\"\u001b[0m, \u001b[32m\"predict\"\u001b[0m),\n",
       "  \u001b[33mList\u001b[0m(\u001b[32m\"mci\"\u001b[0m),\n",
       "  \u001b[33mList\u001b[0m(\u001b[32m\"rail\"\u001b[0m, \u001b[32m\"strike\"\u001b[0m),\n",
       "  \u001b[33mList\u001b[0m(\u001b[32m\"weather\"\u001b[0m, \u001b[32m\"relat\"\u001b[0m, \u001b[32m\"fatal\"\u001b[0m),\n",
       "  \u001b[33mList\u001b[0m(\u001b[32m\"merit\"\u001b[0m, \u001b[32m\"pai\"\u001b[0m, \u001b[32m\"senior\"\u001b[0m),\n",
       "  \u001b[33mList\u001b[0m(\u001b[32m\"isra\"\u001b[0m, \u001b[32m\"role\"\u001b[0m, \u001b[32m\"iran\"\u001b[0m, \u001b[32m\"contra\"\u001b[0m, \u001b[32m\"affair\"\u001b[0m),\n",
       "  \u001b[33mList\u001b[0m(\u001b[32m\"militari\"\u001b[0m, \u001b[32m\"coup\"\u001b[0m, \u001b[32m\"etat\"\u001b[0m),\n",
       "  \u001b[33mList\u001b[0m(\u001b[32m\"machin\"\u001b[0m, \u001b[32m\"translat\"\u001b[0m),\n",
       "  \u001b[33mList\u001b[0m(\u001b[32m\"hostag\"\u001b[0m, \u001b[32m\"take\"\u001b[0m),\n",
       "  \u001b[33mList\u001b[0m(\u001b[32m\"inform\"\u001b[0m, \u001b[32m\"retriev\"\u001b[0m, \u001b[32m\"system\"\u001b[0m),\n",
       "  \u001b[33mList\u001b[0m(\u001b[32m\"natur\"\u001b[0m, \u001b[32m\"languag\"\u001b[0m, \u001b[32m\"process\"\u001b[0m),\n",
       "  \u001b[33mList\u001b[0m(\u001b[32m\"polit\"\u001b[0m, \u001b[32m\"motiv\"\u001b[0m, \u001b[32m\"civil\"\u001b[0m, \u001b[32m\"disturb\"\u001b[0m),\n",
       "  \u001b[33mList\u001b[0m(\u001b[32m\"health\"\u001b[0m, \u001b[32m\"hazard\"\u001b[0m, \u001b[32m\"fine\"\u001b[0m, \u001b[32m\"diamet\"\u001b[0m, \u001b[32m\"fiber\"\u001b[0m),\n",
       "  \u001b[33mList\u001b[0m(\u001b[32m\"attempt\"\u001b[0m, \u001b[32m\"reviv\"\u001b[0m, \u001b[32m\"salt\"\u001b[0m, \u001b[32m\"treati\"\u001b[0m),\n",
       "\u001b[33m...\u001b[0m\n",
       "\u001b[36mnum\u001b[0m: \u001b[32mList\u001b[0m[\u001b[32mString\u001b[0m] = \u001b[33mList\u001b[0m(\n",
       "  \u001b[32m\"51\"\u001b[0m,\n",
       "  \u001b[32m\"52\"\u001b[0m,\n",
       "  \u001b[32m\"53\"\u001b[0m,\n",
       "  \u001b[32m\"54\"\u001b[0m,\n",
       "  \u001b[32m\"55\"\u001b[0m,\n",
       "  \u001b[32m\"56\"\u001b[0m,\n",
       "  \u001b[32m\"57\"\u001b[0m,\n",
       "  \u001b[32m\"58\"\u001b[0m,\n",
       "  \u001b[32m\"59\"\u001b[0m,\n",
       "  \u001b[32m\"60\"\u001b[0m,\n",
       "  \u001b[32m\"61\"\u001b[0m,\n",
       "  \u001b[32m\"62\"\u001b[0m,\n",
       "  \u001b[32m\"63\"\u001b[0m,\n",
       "  \u001b[32m\"64\"\u001b[0m,\n",
       "  \u001b[32m\"65\"\u001b[0m,\n",
       "  \u001b[32m\"66\"\u001b[0m,\n",
       "  \u001b[32m\"67\"\u001b[0m,\n",
       "  \u001b[32m\"68\"\u001b[0m,\n",
       "  \u001b[32m\"69\"\u001b[0m,\n",
       "\u001b[33m...\u001b[0m\n",
       "\u001b[36mquery\u001b[0m: \u001b[32mList\u001b[0m[(\u001b[32mString\u001b[0m, \u001b[32mList\u001b[0m[\u001b[32mString\u001b[0m])] = \u001b[33mList\u001b[0m(\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"51\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"airbu\"\u001b[0m, \u001b[32m\"subsidi\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"52\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"south\"\u001b[0m, \u001b[32m\"african\"\u001b[0m, \u001b[32m\"sanction\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"53\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"leverag\"\u001b[0m, \u001b[32m\"buyout\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"54\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"satellit\"\u001b[0m, \u001b[32m\"launch\"\u001b[0m, \u001b[32m\"contract\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"55\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"insid\"\u001b[0m, \u001b[32m\"trade\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"56\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"prime\"\u001b[0m, \u001b[32m\"lend\"\u001b[0m, \u001b[32m\"rate\"\u001b[0m, \u001b[32m\"move\"\u001b[0m, \u001b[32m\"predict\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"57\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"mci\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"58\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"rail\"\u001b[0m, \u001b[32m\"strike\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"59\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"weather\"\u001b[0m, \u001b[32m\"relat\"\u001b[0m, \u001b[32m\"fatal\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"60\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"merit\"\u001b[0m, \u001b[32m\"pai\"\u001b[0m, \u001b[32m\"senior\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"61\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"isra\"\u001b[0m, \u001b[32m\"role\"\u001b[0m, \u001b[32m\"iran\"\u001b[0m, \u001b[32m\"contra\"\u001b[0m, \u001b[32m\"affair\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"62\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"militari\"\u001b[0m, \u001b[32m\"coup\"\u001b[0m, \u001b[32m\"etat\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"63\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"machin\"\u001b[0m, \u001b[32m\"translat\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"64\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"hostag\"\u001b[0m, \u001b[32m\"take\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"65\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"inform\"\u001b[0m, \u001b[32m\"retriev\"\u001b[0m, \u001b[32m\"system\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"66\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"natur\"\u001b[0m, \u001b[32m\"languag\"\u001b[0m, \u001b[32m\"process\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"67\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"polit\"\u001b[0m, \u001b[32m\"motiv\"\u001b[0m, \u001b[32m\"civil\"\u001b[0m, \u001b[32m\"disturb\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"68\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"health\"\u001b[0m, \u001b[32m\"hazard\"\u001b[0m, \u001b[32m\"fine\"\u001b[0m, \u001b[32m\"diamet\"\u001b[0m, \u001b[32m\"fiber\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"69\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"attempt\"\u001b[0m, \u001b[32m\"reviv\"\u001b[0m, \u001b[32m\"salt\"\u001b[0m, \u001b[32m\"treati\"\u001b[0m)),\n",
       "\u001b[33m...\u001b[0m\n",
       "\u001b[36mres21_3\u001b[0m: \u001b[32mList\u001b[0m[(\u001b[32mString\u001b[0m, \u001b[32mList\u001b[0m[\u001b[32mString\u001b[0m])] = \u001b[33mList\u001b[0m(\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"51\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"airbu\"\u001b[0m, \u001b[32m\"subsidi\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"52\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"south\"\u001b[0m, \u001b[32m\"african\"\u001b[0m, \u001b[32m\"sanction\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"53\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"leverag\"\u001b[0m, \u001b[32m\"buyout\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"54\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"satellit\"\u001b[0m, \u001b[32m\"launch\"\u001b[0m, \u001b[32m\"contract\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"55\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"insid\"\u001b[0m, \u001b[32m\"trade\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"56\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"prime\"\u001b[0m, \u001b[32m\"lend\"\u001b[0m, \u001b[32m\"rate\"\u001b[0m, \u001b[32m\"move\"\u001b[0m, \u001b[32m\"predict\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"57\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"mci\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"58\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"rail\"\u001b[0m, \u001b[32m\"strike\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"59\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"weather\"\u001b[0m, \u001b[32m\"relat\"\u001b[0m, \u001b[32m\"fatal\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"60\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"merit\"\u001b[0m, \u001b[32m\"pai\"\u001b[0m, \u001b[32m\"senior\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"61\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"isra\"\u001b[0m, \u001b[32m\"role\"\u001b[0m, \u001b[32m\"iran\"\u001b[0m, \u001b[32m\"contra\"\u001b[0m, \u001b[32m\"affair\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"62\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"militari\"\u001b[0m, \u001b[32m\"coup\"\u001b[0m, \u001b[32m\"etat\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"63\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"machin\"\u001b[0m, \u001b[32m\"translat\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"64\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"hostag\"\u001b[0m, \u001b[32m\"take\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"65\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"inform\"\u001b[0m, \u001b[32m\"retriev\"\u001b[0m, \u001b[32m\"system\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"66\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"natur\"\u001b[0m, \u001b[32m\"languag\"\u001b[0m, \u001b[32m\"process\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"67\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"polit\"\u001b[0m, \u001b[32m\"motiv\"\u001b[0m, \u001b[32m\"civil\"\u001b[0m, \u001b[32m\"disturb\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"68\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"health\"\u001b[0m, \u001b[32m\"hazard\"\u001b[0m, \u001b[32m\"fine\"\u001b[0m, \u001b[32m\"diamet\"\u001b[0m, \u001b[32m\"fiber\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"69\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"attempt\"\u001b[0m, \u001b[32m\"reviv\"\u001b[0m, \u001b[32m\"salt\"\u001b[0m, \u001b[32m\"treati\"\u001b[0m)),\n",
       "\u001b[33m...\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// requires: having added the file \"questions-descriptions.txt\" to source\n",
    "// This cell will build a list (can be Stream if required) of query tokens. \n",
    "// Note that the 16 is hard-coded to ignore the first 15 characters of these <title> line, which all read \n",
    "// \"<title> Topic: \"\n",
    "\n",
    "val title = Source.fromFile(files_path +\"questions-descriptions.txt\").getLines().filter(_.startsWith(\"<title>\"))\n",
    "                .map(_.substring(16).trim).map(x => token_filter(x)).toList\n",
    "\n",
    "val num = Source.fromFile(files_path +\"questions-descriptions.txt\").getLines().filter(_.startsWith(\"<num>\"))   \n",
    "                .map(x => numPattern.findFirstIn(x.toString).get.substring(1)).toList\n",
    "\n",
    "val query = num zip title\n",
    "query.sortBy(_._1) // the sorted order remains inherent to the object query (nice!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mtest_title\u001b[0m: \u001b[32mList\u001b[0m[\u001b[32mList\u001b[0m[\u001b[32mString\u001b[0m]] = \u001b[33mList\u001b[0m(\n",
       "  \u001b[33mList\u001b[0m(\u001b[32m\"armi\"\u001b[0m, \u001b[32m\"acquisit\"\u001b[0m, \u001b[32m\"advanc\"\u001b[0m, \u001b[32m\"weapon\"\u001b[0m, \u001b[32m\"system\"\u001b[0m),\n",
       "  \u001b[33mList\u001b[0m(\u001b[32m\"nternat\"\u001b[0m, \u001b[32m\"militari\"\u001b[0m, \u001b[32m\"equip\"\u001b[0m, \u001b[32m\"sale\"\u001b[0m),\n",
       "  \u001b[33mList\u001b[0m(\u001b[32m\"hat\"\u001b[0m, \u001b[32m\"back\"\u001b[0m, \u001b[32m\"nation\"\u001b[0m, \u001b[32m\"rifl\"\u001b[0m, \u001b[32m\"associ\"\u001b[0m),\n",
       "  \u001b[33mList\u001b[0m(\u001b[32m\"omput\"\u001b[0m, \u001b[32m\"aid\"\u001b[0m, \u001b[32m\"crime\"\u001b[0m),\n",
       "  \u001b[33mList\u001b[0m(\u001b[32m\"omput\"\u001b[0m, \u001b[32m\"aid\"\u001b[0m, \u001b[32m\"crime\"\u001b[0m, \u001b[32m\"detect\"\u001b[0m),\n",
       "  \u001b[33mList\u001b[0m(\u001b[32m\"omput\"\u001b[0m, \u001b[32m\"aid\"\u001b[0m, \u001b[32m\"medic\"\u001b[0m, \u001b[32m\"diagnosi\"\u001b[0m),\n",
       "  \u001b[33mList\u001b[0m(\u001b[32m\"iber\"\u001b[0m, \u001b[32m\"optic\"\u001b[0m, \u001b[32m\"applic\"\u001b[0m),\n",
       "  \u001b[33mList\u001b[0m(\u001b[32m\"iber\"\u001b[0m, \u001b[32m\"optic\"\u001b[0m, \u001b[32m\"equip\"\u001b[0m, \u001b[32m\"manufactur\"\u001b[0m),\n",
       "  \u001b[33mList\u001b[0m(\u001b[32m\"ran\"\u001b[0m, \u001b[32m\"contra\"\u001b[0m, \u001b[32m\"affair\"\u001b[0m),\n",
       "  \u001b[33mList\u001b[0m(\u001b[32m\"ontrol\"\u001b[0m, \u001b[32m\"transfer\"\u001b[0m, \u001b[32m\"high\"\u001b[0m, \u001b[32m\"technologi\"\u001b[0m)\n",
       ")\n",
       "\u001b[36mtest_num\u001b[0m: \u001b[32mList\u001b[0m[\u001b[32mString\u001b[0m] = \u001b[33mList\u001b[0m(\u001b[32m\"91\"\u001b[0m, \u001b[32m\"92\"\u001b[0m, \u001b[32m\"93\"\u001b[0m, \u001b[32m\"94\"\u001b[0m, \u001b[32m\"95\"\u001b[0m, \u001b[32m\"96\"\u001b[0m, \u001b[32m\"97\"\u001b[0m, \u001b[32m\"98\"\u001b[0m, \u001b[32m\"99\"\u001b[0m, \u001b[32m\"00\"\u001b[0m)\n",
       "\u001b[36mtest_query\u001b[0m: \u001b[32mList\u001b[0m[(\u001b[32mString\u001b[0m, \u001b[32mList\u001b[0m[\u001b[32mString\u001b[0m])] = \u001b[33mList\u001b[0m(\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"91\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"armi\"\u001b[0m, \u001b[32m\"acquisit\"\u001b[0m, \u001b[32m\"advanc\"\u001b[0m, \u001b[32m\"weapon\"\u001b[0m, \u001b[32m\"system\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"92\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"nternat\"\u001b[0m, \u001b[32m\"militari\"\u001b[0m, \u001b[32m\"equip\"\u001b[0m, \u001b[32m\"sale\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"93\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"hat\"\u001b[0m, \u001b[32m\"back\"\u001b[0m, \u001b[32m\"nation\"\u001b[0m, \u001b[32m\"rifl\"\u001b[0m, \u001b[32m\"associ\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"94\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"omput\"\u001b[0m, \u001b[32m\"aid\"\u001b[0m, \u001b[32m\"crime\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"95\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"omput\"\u001b[0m, \u001b[32m\"aid\"\u001b[0m, \u001b[32m\"crime\"\u001b[0m, \u001b[32m\"detect\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"96\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"omput\"\u001b[0m, \u001b[32m\"aid\"\u001b[0m, \u001b[32m\"medic\"\u001b[0m, \u001b[32m\"diagnosi\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"97\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"iber\"\u001b[0m, \u001b[32m\"optic\"\u001b[0m, \u001b[32m\"applic\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"98\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"iber\"\u001b[0m, \u001b[32m\"optic\"\u001b[0m, \u001b[32m\"equip\"\u001b[0m, \u001b[32m\"manufactur\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"99\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"ran\"\u001b[0m, \u001b[32m\"contra\"\u001b[0m, \u001b[32m\"affair\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"00\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"ontrol\"\u001b[0m, \u001b[32m\"transfer\"\u001b[0m, \u001b[32m\"high\"\u001b[0m, \u001b[32m\"technologi\"\u001b[0m))\n",
       ")\n",
       "\u001b[36mres22_3\u001b[0m: \u001b[32mList\u001b[0m[(\u001b[32mString\u001b[0m, \u001b[32mList\u001b[0m[\u001b[32mString\u001b[0m])] = \u001b[33mList\u001b[0m(\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"00\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"ontrol\"\u001b[0m, \u001b[32m\"transfer\"\u001b[0m, \u001b[32m\"high\"\u001b[0m, \u001b[32m\"technologi\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"91\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"armi\"\u001b[0m, \u001b[32m\"acquisit\"\u001b[0m, \u001b[32m\"advanc\"\u001b[0m, \u001b[32m\"weapon\"\u001b[0m, \u001b[32m\"system\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"92\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"nternat\"\u001b[0m, \u001b[32m\"militari\"\u001b[0m, \u001b[32m\"equip\"\u001b[0m, \u001b[32m\"sale\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"93\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"hat\"\u001b[0m, \u001b[32m\"back\"\u001b[0m, \u001b[32m\"nation\"\u001b[0m, \u001b[32m\"rifl\"\u001b[0m, \u001b[32m\"associ\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"94\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"omput\"\u001b[0m, \u001b[32m\"aid\"\u001b[0m, \u001b[32m\"crime\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"95\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"omput\"\u001b[0m, \u001b[32m\"aid\"\u001b[0m, \u001b[32m\"crime\"\u001b[0m, \u001b[32m\"detect\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"96\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"omput\"\u001b[0m, \u001b[32m\"aid\"\u001b[0m, \u001b[32m\"medic\"\u001b[0m, \u001b[32m\"diagnosi\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"97\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"iber\"\u001b[0m, \u001b[32m\"optic\"\u001b[0m, \u001b[32m\"applic\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"98\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"iber\"\u001b[0m, \u001b[32m\"optic\"\u001b[0m, \u001b[32m\"equip\"\u001b[0m, \u001b[32m\"manufactur\"\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"99\"\u001b[0m, \u001b[33mList\u001b[0m(\u001b[32m\"ran\"\u001b[0m, \u001b[32m\"contra\"\u001b[0m, \u001b[32m\"affair\"\u001b[0m))\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Load the test queries. \n",
    "val test_title = Source.fromFile(files_path +\"test-questions.txt\").getLines().filter(_.startsWith(\"<title>\"))\n",
    "                .map(_.substring(16).trim).map(x => token_filter(x)).toList\n",
    "\n",
    "val test_num = Source.fromFile(files_path +\"test-questions.txt\").getLines().filter(_.startsWith(\"<num>\"))   \n",
    "                .map(x => numPattern.findFirstIn(x.toString).get.substring(1)).toList\n",
    "\n",
    "val test_query = test_num zip test_title\n",
    "test_query.sortBy(_._1) // the sorted order remains inherent to the object query (nice!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mobject \u001b[36mInspector\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// The object Inspector contains all functions required to calculate the evaluation metrics (Precision, Recall, \n",
    "// F1-Score and MAP (mean average precision))\n",
    "\n",
    "object Inspector\n",
    "{\n",
    "// calculates average precision for a given answer (returned result of query)\n",
    "def badass1(retriev2: List[String], relev: Array[String], bounded: Boolean=false): Double ={\n",
    "    val retriev = retriev2.map(_.replace(\"-\", \"\"))\n",
    "    // remember to remove the \"-\" hyphens from the prediction for comparison purposes\n",
    "    (retriev.map(relev.contains(_)) // produces a boolean list with true where element belongs to relevant\n",
    "        .scanLeft(0){case (sum, next) => if(next) sum + 1 else sum}.tail // creates cumulative count of the booleans\n",
    "        .zipWithIndex.map(x => x._1.toDouble / (x._2 + 1)) // calculates average precision for each element\n",
    "        .zip(retriev.map(relev.contains(_))) // combines average precision with the boolean list from the start\n",
    "        .filter(_._2) // to filter out the ones that are not relevant \n",
    "        .map(_._1).sum // calculates the numerator (sums up the precision for all elements that are relevant)\n",
    "        )/ (if (bounded) retriev.size else relev.size) // divides by numerator (depending on bounded or not)\n",
    "    }\n",
    "\n",
    "// calculates mean average precision over a set of queries. \n",
    "def badass2(retriev_all: Map[String, List[String]], relev_all: Map[String, Array[String]], \n",
    "            bounded: Boolean=false): Double = {\n",
    "    (retriev_all.map(x => Inspector.badass1(x._2,relev_all(x._1),bounded)) // calculate average precision for each query\n",
    "    .sum)/(retriev_all.size) // calculates mean average precision (average precision over all queries)    \n",
    "}\n",
    "// Classic Precision and Recall for a given query, not striclty necessary. \n",
    "def evaluate(retriev: List[String], relev: Array[String])={\n",
    "    val TP = retriev.filter(relev.contains(_)).size.toDouble\n",
    "    val precision = TP / retriev.size\n",
    "    val recall = TP / relev.size\n",
    "    (precision,recall)\n",
    "}\n",
    "def recall1(retriev2: List[String],relev: Array[String]): Double = {\n",
    "        val retriev = retriev2.map(_.replace(\"-\", \"\"))\n",
    "        val TP = relev.filter(retriev.contains(_)).size.toDouble\n",
    "        TP / relev.size\n",
    "}\n",
    "\n",
    "def recall2(retriev_all: Map[String, List[String]],relev_all: Map[String, Array[String]]) = {\n",
    "        retriev_all.map(x => Inspector.recall1(x._2,relev_all(x._1)))\n",
    "}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Term-Frequency Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction \u001b[36mhash_query\u001b[0m\n",
       "\u001b[36mcorpus_size\u001b[0m: \u001b[32mInt\u001b[0m = \u001b[32m100000\u001b[0m\n",
       "defined \u001b[32mfunction \u001b[36mget_idf\u001b[0m\n",
       "defined \u001b[32mfunction \u001b[36mget_tf\u001b[0m\n",
       "defined \u001b[32mfunction \u001b[36mget_log_tf\u001b[0m\n",
       "defined \u001b[32mfunction \u001b[36mget_tf_idf\u001b[0m\n",
       "\u001b[36maverage_doc_size\u001b[0m: \u001b[32mDouble\u001b[0m = \u001b[32m413.69858\u001b[0m\n",
       "defined \u001b[32mfunction \u001b[36mget_tf_okapi\u001b[0m\n",
       "defined \u001b[32mfunction \u001b[36mget_idf_okapi\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Define auxillary functions for the term-frequency model\n",
    "\n",
    "\n",
    "def hash_query(query: (String, List[String])) = {\n",
    "    (query._1, query._2.map(x => token_hash.getOrElse(x,-1)).filter(prun_htoken_id.keys.toSet.contains(_)).toSet)\n",
    "}\n",
    "\n",
    "val corpus_size = prun_id_htoken.size\n",
    "def get_idf(query: Set[Int]) = {\n",
    "    query.map(x => x -> Math.log(corpus_size / prun_htoken_id(x).size)).toMap\n",
    "}\n",
    "\n",
    "// get term frequency in a specific document (doc)\n",
    "def get_tf(query: Set[Int],doc: Int) = {\n",
    "    prun_id_htoken(doc).filter(query.contains(_)).groupBy(identity).mapValues(_.size)\n",
    "}\n",
    "\n",
    "// get log_tf\n",
    "def get_log_tf(query: Set[Int],doc: Int) = {\n",
    "    prun_id_htoken(doc).filter(query.contains(_)).groupBy(identity).mapValues(x => Math.log(x.size))\n",
    "}\n",
    "\n",
    "// get tf-idf is defined as tf-idf = tf * idf\n",
    "def get_tf_idf(query: Set[Int],doc:Int) = {\n",
    "    get_tf(query,doc).map(x => x._1 -> x._2 * get_idf(query).getOrElse(x._1,0.toDouble))\n",
    "}\n",
    "\n",
    "val average_doc_size = (prun_id_htoken.mapValues(x => x.size).foldLeft(0)(_+_._2).toDouble / prun_id_htoken.size)\n",
    "\n",
    "def get_tf_okapi(query: Set[Int],doc: Int,k: Double=1.2,b: Double=0.75) = {\n",
    "    val dsize = prun_id_htoken(doc).size\n",
    "    val avdsize = average_doc_size\n",
    "    get_tf(query,doc).mapValues(x => (x * (k+1)) / (x + k*(1 - b + (b*(dsize/avdsize)))))\n",
    "}\n",
    "\n",
    "def get_idf_okapi(query: Set[Int]) = {\n",
    "    query.map(x => x -> Math.log((corpus_size - prun_htoken_id(x).size + 0.5)/(prun_htoken_id(x).size+0.5))).toMap\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction \u001b[36mhandle\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Simple tf-idf model\n",
    "// Handle a Query --> take in a query, produce a ranking\n",
    "def handle(query: (String, List[String])) = {\n",
    "    val hashed_query = hash_query(query)\n",
    "    val doc_set = hashed_query._2.map(x => prun_htoken_id(x)).flatten.toSet\n",
    "    val ranking = doc_set.map(x => x -> get_tf_idf(hashed_query._2,x).values.sum).toSeq.sortBy(-_._2)\n",
    "                    .take(100).map(x => x._1).toList    \n",
    "    (query._1,ranking)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction \u001b[36mhandle_okapi\u001b[0m\n",
       "defined \u001b[32mfunction \u001b[36mhandle_okapi_no_index\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Okapi BM25 model \n",
    "// https://en.wikipedia.org/wiki/Okapi_BM25\n",
    "// Handle a Query --> take in a query, produce a ranking\n",
    "def handle_okapi(query: (String, List[String]),k: Double=1.2,b: Double=0.75) = {\n",
    "    val hashed_query = hash_query(query)\n",
    "    val idf = get_idf_okapi(hashed_query._2)\n",
    "    val doc_set = hashed_query._2.map(x => prun_htoken_id(x)).flatten.toSet\n",
    "    \n",
    "    val ranking = doc_set.map(x => x -> (get_tf_okapi(hashed_query._2,x,k,b).map(x => x._1 -> x._2 * idf(x._1))).values.sum)\n",
    "                    .toSeq.sortBy(-_._2)\n",
    "                    .take(100).map(x => x._1).toList    \n",
    "    (query._1,ranking)\n",
    "}\n",
    "\n",
    "// This is a modification of that function to faciliate the comparison of retrieval time when an inverted index is \n",
    "// used versus no inverted index is used // the results this model returns are the same obviuosly. \n",
    "def handle_okapi_no_index(query: (String, List[String]),k: Double=1.2,b: Double=0.75) = {\n",
    "    val hashed_query = hash_query(query)\n",
    "    val idf = get_idf_okapi(hashed_query._2)\n",
    "    val doc_set = prun_id_htoken.keySet\n",
    "    \n",
    "    val ranking = doc_set.map(x => x -> (get_tf_okapi(hashed_query._2,x,k,b).map(x => x._1 -> x._2 * idf(x._1))).values.sum)\n",
    "                    .toSeq.sortBy(-_._2)\n",
    "                    .take(100).map(x => x._1).toList    \n",
    "    (query._1,ranking)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF: The average time per query on the training set is: 0.07255479411458333\n",
      "TF: The bounded MAP on the training set is: 0.3212110318053528\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36manswers_train\u001b[0m: \u001b[32mMap\u001b[0m[\u001b[32mString\u001b[0m, \u001b[32mList\u001b[0m[\u001b[32mString\u001b[0m]] = \u001b[33mMap\u001b[0m(\n",
       "  \u001b[32m\"67\"\u001b[0m -> \u001b[33mList\u001b[0m(\n",
       "    \u001b[32m\"WSJ880727-0154\"\u001b[0m,\n",
       "    \u001b[32m\"WSJ870908-0085\"\u001b[0m,\n",
       "    \u001b[32m\"AP891013-0071\"\u001b[0m,\n",
       "    \u001b[32m\"WSJ911014-0122\"\u001b[0m,\n",
       "    \u001b[32m\"WSJ891101-0074\"\u001b[0m,\n",
       "    \u001b[32m\"AP880607-0214\"\u001b[0m,\n",
       "    \u001b[32m\"AP900116-0109\"\u001b[0m,\n",
       "    \u001b[32m\"AP881220-0009\"\u001b[0m,\n",
       "    \u001b[32m\"WSJ880413-0114\"\u001b[0m,\n",
       "    \u001b[32m\"WSJ880517-0073\"\u001b[0m,\n",
       "    \u001b[32m\"AP880427-0078\"\u001b[0m,\n",
       "    \u001b[32m\"WSJ890802-0116\"\u001b[0m,\n",
       "    \u001b[32m\"AP890531-0190\"\u001b[0m,\n",
       "    \u001b[32m\"AP881220-0046\"\u001b[0m,\n",
       "    \u001b[32m\"WSJ880425-0077\"\u001b[0m,\n",
       "    \u001b[32m\"AP880614-0123\"\u001b[0m,\n",
       "    \u001b[32m\"WSJ900518-0117\"\u001b[0m,\n",
       "    \u001b[32m\"WSJ861212-0091\"\u001b[0m,\n",
       "\u001b[33m...\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Consider the training set performance\n",
    "timeit.start\n",
    "val answers_train = query.map(x => handle_okapi(x)).toMap.mapValues(_.map(x => id_name(x)))\n",
    "\n",
    "println(\"TF: The average time per query on the training set is: \" + ((timeit.uptonow / 60.0)/answers_train.size))\n",
    "println(\"TF: The bounded MAP on the training set is: \" + Inspector.badass2(answers_train,truth.judgements,bounded=true))\n",
    "\n",
    "// TF: The average time per query on the training set is: 0.07255479411458333\n",
    "// TF: The bounded MAP on the training set is: 0.3212110318053528"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language model score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "// // collection tf\n",
    "val collection_size = prun_htoken_collectfreq.foldLeft(0.0)(_+_._2)\n",
    "val collection_size_log = prun_htoken_collectfreq.foldLeft(0.0)(\n",
    "    (res,value) => res + Math.log(1.0 + value._2.toDouble))\n",
    "\n",
    "val pruned_token_set = prun_htoken_collectfreq.keys.toSet\n",
    "\n",
    "// discards log scores (leaves ranked ID's and time)\n",
    "def unfold_name_time(score :(List[(String, Double)], Double)) = {\n",
    "    (score._1.unzip._1, score._2)\n",
    "}\n",
    "\n",
    "// returns id's of docs in which the most query tokens appear in\n",
    "def reduce_candidate_doc(query: (String, List[Int]), \n",
    "                         prun_htoken_id: MutHashMap[Int, List[Int]] = prun_htoken_id, \n",
    "                         candidate_size: Int = 100) = {\n",
    "    \n",
    "    // doc_id -> # of occurances\n",
    "    val doc_occurance = query._2.flatMap(token => prun_htoken_id(token)).\n",
    "        groupBy(identity).mapValues(_.size)\n",
    "    \n",
    "    // sorted iterator of # of occurances\n",
    "    val intersect_value = doc_occurance.values.toSet.toList.sorted.reverse.toIterator\n",
    "    var iter = intersect_value.next\n",
    "    \n",
    "    var cur_doc_occurance = doc_occurance.filter(_._2 > iter - 1)\n",
    "    \n",
    "    // looks for minimal set that satisfies minimal set size\n",
    "    while((cur_doc_occurance.size < candidate_size) & (intersect_value.hasNext)) {\n",
    "        iter = intersect_value.next\n",
    "        cur_doc_occurance = doc_occurance.filter(_._2 > iter - 1)\n",
    "    }\n",
    "    cur_doc_occurance.keys.toList\n",
    "}\n",
    "\n",
    "// (id, List(hashes_tokens))\n",
    "val query_hash = query.map{ \n",
    "    case (id, str) => (id, str.\n",
    "                       flatMap(x => token_hash.get(x)).filter(pruned_token_set.contains(_))\n",
    "                      )}\n",
    "\n",
    "val lambda = 0.01 // smoothing parameter\n",
    "\n",
    "// set of all non-empty documents (i.e. that contain tokens)\n",
    "val non_empty_id = prun_id_htoken.filter(_._2.size > 0).keys.toList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// operate on a single query\n",
    "def lang_query(query: (String, List[Int]),\n",
    "               method: String = \"index\", \n",
    "               log_opt: String = \"tf\",\n",
    "               prun_htoken_collectfreq: MutHashMap[Int, Int] = prun_htoken_collectfreq,\n",
    "               collection_size: Double = collection_size, \n",
    "               collection_size_log: Double = collection_size_log, \n",
    "               lambda: Double = lambda, \n",
    "               prun_id_htoken: MutHashMap[Int, List[Int]] = prun_id_htoken, \n",
    "               prun_htoken_id: MutHashMap[Int, List[Int]] = prun_htoken_id, \n",
    "               candidate_size: Int = 100, \n",
    "               take_k_results: Int = 100) = {\n",
    "    \n",
    "    // list of doc id's containing tokens in query\n",
    "    def candidate_doc(): List[Int] = method match {\n",
    "        case \"index\" => reduce_candidate_doc(query = query, candidate_size = candidate_size)\n",
    "        case \"no_index\" => non_empty_id\n",
    "//         case \"test\" => (1 to 2).toList\n",
    "        case _ => throw new Exception(\"Please choose either 'index' or 'no_index'\")\n",
    "    }\n",
    "        \n",
    "    // map of tokens to frequency in a given doc\n",
    "    def doc_tf_map(doc_id: Int) = log_opt match {\n",
    "        case \"tf\" => prun_id_htoken(doc_id).\n",
    "            groupBy(identity).mapValues(x => x.size.toDouble)\n",
    "        case \"log\" => prun_id_htoken(doc_id).\n",
    "            groupBy(identity).mapValues(x => Math.log(1.0+x.size))\n",
    "        case _ => throw new Exception(\"Please choose either 'log' or 'tf'\")\n",
    "    }\n",
    "    \n",
    "    // number of tokens in doc\n",
    "    def doc_size(doc_id: Int) = {\n",
    "        doc_tf_map(doc_id).values.sum\n",
    "    }\n",
    "\n",
    "    // list of (relative) frequency of query tokens in a given doc\n",
    "    def doc_query_tf(doc_id: Int) = {\n",
    "        query._2.map(token => \n",
    "                     doc_tf_map(doc_id).getOrElse(token, 0.0) / doc_size(doc_id))\n",
    "    }\n",
    "    \n",
    "    // list of (relative) frequency of query tokens in the collection\n",
    "    def query_cf() = log_opt match {\n",
    "        case \"tf\" => query._2.map(token => \n",
    "                                  prun_htoken_collectfreq(token).toDouble / collection_size)\n",
    "        case \"log\" => query._2.map(token => \n",
    "                                  Math.log(1.0 + prun_htoken_collectfreq(token)) / collection_size_log)\n",
    "    }\n",
    "        \n",
    "    // this only needs to be calculated once per query\n",
    "    // (wasteful to call function multiple times)\n",
    "    val cur_query_cf = query_cf()\n",
    "    \n",
    "    //\n",
    "    def smooth_prob(doc_id: Int) = {\n",
    "        doc_query_tf(doc_id).zip(cur_query_cf).\n",
    "            map{case (x, y) => (1 - lambda) * x + lambda * y}\n",
    "    }\n",
    "    \n",
    "    // sum log(x) elements of list\n",
    "    def doc_lang_score(doc_id: Int) = {\n",
    "        smooth_prob(doc_id).foldLeft(0.0)(_ + Math.log(_))\n",
    "    }\n",
    "    \n",
    "    val log_scores = candidate_doc().map(doc => \n",
    "                                         (id_name(doc), doc_lang_score(doc))\n",
    "                                         ).sortWith(_._2 > _._2)\n",
    "    \n",
    "    (log_scores.take(take_k_results), timeit.uptonow / 60.0)\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "// applies lang_query to all queries in the \"training set\"\n",
    "// reports MAP and precision scores\n",
    "def lang_model_results_MAP(query_hash: List[(String, List[Int])] = query_hash, \n",
    "                           method: String, \n",
    "                           log_opt: String, \n",
    "                           candidate_size: Int = 100, \n",
    "                           lambda: Double = lambda, \n",
    "                           truth: TipsterGroundTruth = truth) = {\n",
    "    \n",
    "    val lang_model_rank_time = query_hash.map(query => \n",
    "                           (query._1, unfold_name_time(\n",
    "                               lang_query(query, \n",
    "                                          method = method, \n",
    "                                          log_opt = log_opt, \n",
    "                                          candidate_size = candidate_size, \n",
    "                                          lambda = lambda))\n",
    "                           )\n",
    "                                             ).toMap\n",
    "    \n",
    "    val lang_model_time = average(lang_model_rank_time.values.map(x => x._2))\n",
    "    val lang_model_rank = lang_model_rank_time.mapValues(x => x._1)\n",
    "    println(f\"Average time per query is ${lang_model_time}%1.3f minutes\")\n",
    "    \n",
    "    val MAP_score = Inspector.badass2(lang_model_rank,truth.judgements,bounded=true)\n",
    "    println(f\"MAP score is ${MAP_score}%1.3f\")\n",
    "    \n",
    "    var precision = List[Double]()\n",
    "    \n",
    "    // (precision,recall)\n",
    "    for (key <- lang_model_rank.keys) {\n",
    "        var p_r = Inspector.evaluate(lang_model_rank(key).map(_.replace(\"-\", \"\")),truth.judgements(key))\n",
    "        precision ++= List(p_r._1)\n",
    "    //     recall ++= List(p_r._2)\n",
    "    }\n",
    "    println(f\"mean precision is ${average(precision)}%1.3f\")\n",
    "    \n",
    "    MAP_score\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// use example, with configurations chosen (after running the hyper parameter search)\n",
    "lang_model_results_MAP(method = \"index\", \n",
    "                       log_opt = \"tf\", \n",
    "                       candidate_size = 1000, \n",
    "                       lambda = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// searching through hyper-parameters and model configurations\n",
    "var lang_model_search = List[(String, Double, Int, Double)]() // method, lambda, candidate_size, MAP_score\n",
    "\n",
    "for (opt_iter <- Seq(\"log\", \"tf\")){\n",
    "    for (candidate_size_iter <- Seq(100, 1000)){\n",
    "        for (lambda_iter <- (0 to 4).map(x => Math.pow(10, -x))){\n",
    "            println(f\"applying $opt_iter method with candidate size of $candidate_size_iter, lambda is set to ${lambda_iter}\")\n",
    "            lang_model_search ++= List((opt_iter, \n",
    "                                        lambda_iter, \n",
    "                                        candidate_size_iter, \n",
    "                                        lang_model_results_MAP(query_hash = query_hash, \n",
    "                                                               method = \"index\", \n",
    "                                                               log_opt = opt_iter, \n",
    "                                                               candidate_size = candidate_size_iter, \n",
    "                                                               lambda = lambda_iter)\n",
    "                                        ))\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "// write results to file\n",
    "write_lang_model_search(lang_model_search, PATH_lang_model_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rank test queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "// load test queries\n",
    "val PATH_test_questions = \"test-questions.txt\"\n",
    "val test_title = Source.fromFile(files_path + PATH_test_questions).getLines().filter(_.startsWith(\"<title>\"))\n",
    "                .map(_.substring(16).trim).map(x => token_filter(x)).toList\n",
    "\n",
    "val test_num = Source.fromFile(files_path + PATH_test_questions).getLines().filter(_.startsWith(\"<num>\"))   \n",
    "                .map(x => numPattern.findFirstIn(x.toString).get.substring(1)).toList\n",
    "\n",
    "val test_query = test_num zip test_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF: Average time per query w indexing is: 0.14082328266833335\n",
      "TF: Average time per query w/o indexing is: 0.21297428135666668\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36manswers_test\u001b[0m: \u001b[32mMap\u001b[0m[\u001b[32mString\u001b[0m, \u001b[32mList\u001b[0m[\u001b[32mString\u001b[0m]] = \u001b[33mMap\u001b[0m(\n",
       "  \u001b[32m\"98\"\u001b[0m -> \u001b[33mList\u001b[0m(\n",
       "    \u001b[32m\"ZF109-452-641\"\u001b[0m,\n",
       "    \u001b[32m\"DOE1-25-0952\"\u001b[0m,\n",
       "    \u001b[32m\"ZF109-720-277\"\u001b[0m,\n",
       "    \u001b[32m\"ZF207-519-494\"\u001b[0m,\n",
       "    \u001b[32m\"ZF108-104-486\"\u001b[0m,\n",
       "    \u001b[32m\"FR88915-0002\"\u001b[0m,\n",
       "    \u001b[32m\"WSJ910528-0183\"\u001b[0m,\n",
       "    \u001b[32m\"WSJ880610-0075\"\u001b[0m,\n",
       "    \u001b[32m\"ZF108-719-392\"\u001b[0m,\n",
       "    \u001b[32m\"DOE2-48-0109\"\u001b[0m,\n",
       "    \u001b[32m\"FR89501-0011\"\u001b[0m,\n",
       "    \u001b[32m\"WSJ910715-0071\"\u001b[0m,\n",
       "    \u001b[32m\"WSJ890920-0035\"\u001b[0m,\n",
       "    \u001b[32m\"DOE1-23-0542\"\u001b[0m,\n",
       "    \u001b[32m\"ZF207-488-516\"\u001b[0m,\n",
       "    \u001b[32m\"WSJ870304-0091\"\u001b[0m,\n",
       "    \u001b[32m\"WSJ880607-0144\"\u001b[0m,\n",
       "    \u001b[32m\"ZF207-827-034\"\u001b[0m,\n",
       "\u001b[33m...\u001b[0m\n",
       "\u001b[36manswers_test_no_index\u001b[0m: \u001b[32mMap\u001b[0m[\u001b[32mString\u001b[0m, \u001b[32mList\u001b[0m[\u001b[32mString\u001b[0m]] = \u001b[33mMap\u001b[0m(\n",
       "  \u001b[32m\"98\"\u001b[0m -> \u001b[33mList\u001b[0m(\n",
       "    \u001b[32m\"ZF109-452-641\"\u001b[0m,\n",
       "    \u001b[32m\"DOE1-25-0952\"\u001b[0m,\n",
       "    \u001b[32m\"ZF109-720-277\"\u001b[0m,\n",
       "    \u001b[32m\"ZF207-519-494\"\u001b[0m,\n",
       "    \u001b[32m\"ZF108-104-486\"\u001b[0m,\n",
       "    \u001b[32m\"FR88915-0002\"\u001b[0m,\n",
       "    \u001b[32m\"WSJ910528-0183\"\u001b[0m,\n",
       "    \u001b[32m\"WSJ880610-0075\"\u001b[0m,\n",
       "    \u001b[32m\"ZF108-719-392\"\u001b[0m,\n",
       "    \u001b[32m\"DOE2-48-0109\"\u001b[0m,\n",
       "    \u001b[32m\"FR89501-0011\"\u001b[0m,\n",
       "    \u001b[32m\"WSJ910715-0071\"\u001b[0m,\n",
       "    \u001b[32m\"WSJ890920-0035\"\u001b[0m,\n",
       "    \u001b[32m\"DOE1-23-0542\"\u001b[0m,\n",
       "    \u001b[32m\"ZF207-488-516\"\u001b[0m,\n",
       "    \u001b[32m\"WSJ870304-0091\"\u001b[0m,\n",
       "    \u001b[32m\"WSJ880607-0144\"\u001b[0m,\n",
       "    \u001b[32m\"ZF207-827-034\"\u001b[0m,\n",
       "\u001b[33m...\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// tf model ranking\n",
    "\n",
    "// compare running time with and w/o indexing\n",
    "\n",
    "// w indexing\n",
    "timeit.start\n",
    "val answers_test = test_query.map(x => handle_okapi(x)).toMap.mapValues(_.map(x => id_name(x)))\n",
    "println(\"TF: Average time per query w indexing is: \" + ((timeit.uptonow / 60.0)/answers_test.size))\n",
    "\n",
    "// w/o indexing \n",
    "timeit.start\n",
    "val answers_test_no_index = test_query.map(x => handle_okapi_no_index(x)).toMap.mapValues(_.map(x => id_name(x)))\n",
    "println(\"TF: Average time per query w/o indexing is: \" + ((timeit.uptonow / 60.0)/answers_test_no_index.size))\n",
    "\n",
    "// TF: Average time per query w indexing is: 0.14082328266833335\n",
    "// TF: Average time per query w indexing is: 0.19855570774\n",
    "\n",
    "// TF: Average time per query w/o indexing is: 0.21297428135666668\n",
    "// TF: Average time per query w/o indexing is: 0.29179896611333334\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// saving to file\n",
    "// Produce test-set ranking for tf-model\n",
    "write_res(answers_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val test_query_hash = test_query.map{ \n",
    "    case (id, str) => (id, str.\n",
    "                       flatMap(x => token_hash.get(x)).filter(pruned_token_set.contains(_))\n",
    "                      )}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "// language model ranking\n",
    "// setting optimal config based on search\n",
    "// compare running time with and w/o indexing\n",
    "val lm_test_INDEX_rank_time = test_query_hash.map(query => \n",
    "                                                  (query._1, unfold_name_time(\n",
    "                                                      lang_query(query, \n",
    "                                                                 method = \"index\", \n",
    "                                                                 log_opt = \"tf\", \n",
    "                                                                 candidate_size = 1000, \n",
    "                                                                 lambda = 0.1))\n",
    "                                                  )\n",
    "                                                 ).toMap\n",
    "\n",
    "val lm_test_INDEX_time = lm_test_INDEX_rank_time.values.map(x => x._2)\n",
    "val lm_test_INDEX_rank = lm_test_INDEX_rank_time.mapValues(x => x._1)\n",
    "println(f\"Average time per query is ${average(lm_test_INDEX_time)}%1.3f seconds\")\n",
    "\n",
    "// w/o indexing\n",
    "val lm_test_NOINDEX_rank_time = test_query_hash.map(query => \n",
    "                                                  (query._1, unfold_name_time(\n",
    "                                                      lang_query(query, \n",
    "                                                                 method = \"no_index\", \n",
    "                                                                 log_opt = \"tf\", \n",
    "                                                                 candidate_size = 1000, \n",
    "                                                                 lambda = 0.1))\n",
    "                                                  )\n",
    "                                                 ).toMap\n",
    "\n",
    "val lm_test_NOINDEX_time = lm_test_NOINDEX_rank_time.values.map(x => x._2)\n",
    "val lm_test_NOINDEX_rank = lm_test_NOINDEX_rank_time.mapValues(x => x._1)\n",
    "println(f\"Average time per query is ${average(lm_test_NOINDEX_time)}%1.3f seconds\")\n",
    "\n",
    "// Average time per query is 14.816 seconds\n",
    "// Average time per query is 24.109 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// saving to file\n",
    "write_res(lm_test_INDEX_rank,\"l\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Scala 2.11",
   "language": "scala211",
   "name": "scala211"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
