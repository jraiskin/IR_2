{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classpath.addPath(\"tinyir-1.1.jar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36mscala.xml.XML\u001b[0m\n",
       "\u001b[32mimport \u001b[36mch.ethz.dal.tinyir._\u001b[0m\n",
       "\u001b[32mimport \u001b[36mcom.github.aztek.porterstemmer.PorterStemmer\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import scala.xml.XML\n",
    "import ch.ethz.dal.tinyir._\n",
    "import com.github.aztek.porterstemmer.PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36mjava.io._\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// import scala.io.Source  // for importing txt files\n",
    "import java.io._  // for saving txt files\n",
    "// import scala.collection.mutable.HashMap  //HashMap used for counting elements in linear time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36mscala.collection.mutable.{Map => MutMap}\u001b[0m\n",
       "\u001b[32mimport \u001b[36mscala.collection.mutable.ListBuffer\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// import scala.util.Random\n",
    "import scala.collection.mutable.{Map => MutMap}\n",
    "// enables \"mutable lists\"\n",
    "import scala.collection.mutable.ListBuffer  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class xml_doc (file_path: String) {\n",
    "    def get_doc(): xml.Elem = {\n",
    "        XML.loadFile(file_path: String)\n",
    "    }    \n",
    "    \n",
    "    def text() = {\n",
    "        (get_doc() \\\\ \"DOC\" \\\\ \"TEXT\").text\n",
    "    }\n",
    "    \n",
    "    def head() = {\n",
    "        (get_doc() \\\\ \"DOC\" \\\\ \"HEAD\").text\n",
    "    }\n",
    "\n",
    "    def id() = {\n",
    "        (get_doc() \\\\ \"DOC\" \\\\ \"DOCNO\").text.trim\n",
    "    }\n",
    "    \n",
    "    // tokenizes the text, remove stop words and stems\n",
    "    // based on ch.ethz.dal.tinyir.processing\n",
    "    // and on com.github.aztek.porterstemmer\n",
    "    def token_stem() = {\n",
    "        processing.StopWords.filterOutSW(                    // filter out any token which is a StopWord\n",
    "        processing.Tokenizer.tokenize(head() ++ text())     // create tokens from headline and body\n",
    "    ).map(x => PorterStemmer.stem(x).                       // apply stemming\n",
    "          replaceAll(\"\\\\P{L}+\", \"\")).filter(_.trim.nonEmpty)  // pattern matching to keep only alphabet\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def list_docs (path: String) = {  // : Array[java.io.File]\n",
    "        new java.io.File(path).listFiles.map(x => x.toString())\n",
    "    }\n",
    "val numPattern = \"[0-9]+\".r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write(data: MutMap[Double, (Double, Double, Double)],filename: String) = {\n",
    "\n",
    "    val bw = new BufferedWriter(new FileWriter(new File(filename)))\n",
    "    val iter = data.keys.iterator\n",
    "    while(iter.hasNext){\n",
    "        var elem = iter.next()\n",
    "        var values = data(elem).productIterator.toList\n",
    "        if(values.length>0){\n",
    "            bw.write(elem+\" \"+values.mkString(\" \"))\n",
    "            bw.newLine\n",
    "        }    \n",
    "    }   \n",
    "    bw.close()\n",
    "}\n",
    "\n",
    "def write_prediction(data: MutMap[Int, Set[String]],filename: String) = {\n",
    "\n",
    "    val bw = new BufferedWriter(new FileWriter(new File(filename)))\n",
    "    val iter = data.keys.iterator\n",
    "    while(iter.hasNext){\n",
    "        var elem = iter.next()\n",
    "        var values = data(elem).toList\n",
    "        if(values.length>0){\n",
    "            bw.write(elem+\" \"+values.mkString(\" \"))\n",
    "            bw.newLine\n",
    "        }    \n",
    "    }   \n",
    "    bw.close()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val train_list = list_docs(\"documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val doc_list = new ListBuffer[processing.StringDocument]()\n",
    "var counter = 0\n",
    "for (path <- train_list.take(3)){\n",
    "    counter += 1\n",
    "    var cur_doc = new xml_doc(path)\n",
    "    doc_list += (new processing.StringDocument(counter, cur_doc.token_stem.mkString(\" \")))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val doc_stream = doc_list.toStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val test_pos_index = new indexing.PosIndex(doc_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_pos_index.index\n",
    "// test_pos_index.postings(doc_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31mMain.scala:24: not found: value doc_stream",
      "doc_stream(2).body",
      "^\u001b[0m"
     ]
    }
   ],
   "source": [
    "doc_stream(2).body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "/*\n",
    "We want a DocStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "var t = io.DocStream.getStream(\"documents/AP880212-0006\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "var t = new java.io.FileInputStream(\"documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "public FileInputStream(FileDescriptor \"documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "var t = new java.io.File(\"documents\").listFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for (doc in t){\n",
    "    doc_list +=\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mt\u001b[0m: \u001b[32mio\u001b[0m.\u001b[32mDirStream\u001b[0m = ch.ethz.dal.tinyir.io.DirStream@37de0d29"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val t = new io.DirStream(\"documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres5\u001b[0m: \u001b[32mStream\u001b[0m[\u001b[32mInputStream\u001b[0m] = \u001b[33mStream\u001b[0m(\n",
       "  java.io.BufferedInputStream@5657b756,\n",
       "  java.io.BufferedInputStream@5334f8f6,\n",
       "  java.io.BufferedInputStream@2e99bdf0,\n",
       "  java.io.BufferedInputStream@4127f1ed,\n",
       "  java.io.BufferedInputStream@5dd8c1e0,\n",
       "  java.io.BufferedInputStream@295f71e5,\n",
       "  java.io.BufferedInputStream@4a4a3f03,\n",
       "  java.io.BufferedInputStream@341c073e,\n",
       "  java.io.BufferedInputStream@2f50079c,\n",
       "  java.io.BufferedInputStream@7d528bca,\n",
       "  java.io.BufferedInputStream@66fb0144,\n",
       "  java.io.BufferedInputStream@328acf52,\n",
       "  java.io.BufferedInputStream@1d495278,\n",
       "  java.io.BufferedInputStream@742c86c0,\n",
       "  java.io.BufferedInputStream@4ae35820,\n",
       "  java.io.BufferedInputStream@2ef7fe71,\n",
       "  java.io.BufferedInputStream@797aa345,\n",
       "  java.io.BufferedInputStream@6970202c,\n",
       "  java.io.BufferedInputStream@3902477a,\n",
       "\u001b[33m...\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t.stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres83\u001b[0m: \u001b[32mList\u001b[0m[\u001b[32mString\u001b[0m] = \u001b[33mList\u001b[0m(\n",
       "  \u001b[32m\"strategist\"\u001b[0m,\n",
       "  \u001b[32m\"jack\"\u001b[0m,\n",
       "  \u001b[32m\"kemp\"\u001b[0m,\n",
       "  \u001b[32m\"presidentialcampaign\"\u001b[0m,\n",
       "  \u001b[32m\"georg\"\u001b[0m,\n",
       "  \u001b[32m\"bush\"\u001b[0m,\n",
       "  \u001b[32m\"poor\"\u001b[0m,\n",
       "  \u001b[32m\"showe\"\u001b[0m,\n",
       "  \u001b[32m\"iowa\"\u001b[0m,\n",
       "  \u001b[32m\"coupl\"\u001b[0m,\n",
       "  \u001b[32m\"withkemp\"\u001b[0m,\n",
       "  \u001b[32m\"tough\"\u001b[0m,\n",
       "  \u001b[32m\"talk\"\u001b[0m,\n",
       "  \u001b[32m\"ad\"\u001b[0m,\n",
       "  \u001b[32m\"bob\"\u001b[0m,\n",
       "  \u001b[32m\"dole\"\u001b[0m,\n",
       "  \u001b[32m\"put\"\u001b[0m,\n",
       "  \u001b[32m\"kemp\"\u001b[0m,\n",
       "  \u001b[32m\"therun\"\u001b[0m,\n",
       "\u001b[33m...\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "collection(1).tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mcollection\u001b[0m: \u001b[32mStream\u001b[0m[\u001b[32mXMLDocument\u001b[0m] = \u001b[33mStream\u001b[0m(\n",
       "  ch.ethz.dal.tinyir.processing.TipsterParse@6c351c10,\n",
       "  ch.ethz.dal.tinyir.processing.TipsterParse@362cc1a,\n",
       "  ch.ethz.dal.tinyir.processing.TipsterParse@594290eb,\n",
       "  ch.ethz.dal.tinyir.processing.TipsterParse@776b5610,\n",
       "  ch.ethz.dal.tinyir.processing.TipsterParse@5d02e740,\n",
       "  ch.ethz.dal.tinyir.processing.TipsterParse@4ed8966e,\n",
       "  ch.ethz.dal.tinyir.processing.TipsterParse@4bbb5a4c,\n",
       "  ch.ethz.dal.tinyir.processing.TipsterParse@4311b1a2,\n",
       "  ch.ethz.dal.tinyir.processing.TipsterParse@67b73982,\n",
       "  ch.ethz.dal.tinyir.processing.TipsterParse@7d940d48,\n",
       "  ch.ethz.dal.tinyir.processing.TipsterParse@311c9cd6,\n",
       "  ch.ethz.dal.tinyir.processing.TipsterParse@55b48478,\n",
       "  ch.ethz.dal.tinyir.processing.TipsterParse@7ecf6600,\n",
       "  ch.ethz.dal.tinyir.processing.TipsterParse@34bd1776,\n",
       "  ch.ethz.dal.tinyir.processing.TipsterParse@6a05e212,\n",
       "  ch.ethz.dal.tinyir.processing.TipsterParse@1314337f,\n",
       "  ch.ethz.dal.tinyir.processing.TipsterParse@2a992b48,\n",
       "  ch.ethz.dal.tinyir.processing.TipsterParse@6f0cb4bd,\n",
       "  ch.ethz.dal.tinyir.processing.TipsterParse@62172133,\n",
       "\u001b[33m...\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val collection = new io.TipsterStream(\"resources\").stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mcollection2\u001b[0m: \u001b[32mStream\u001b[0m[\u001b[32mXMLDocument\u001b[0m] = \u001b[33mStream\u001b[0m(\n",
       "  cmd90$$user$YMParse@422aec92,\n",
       "  cmd90$$user$YMParse@7ae637a2,\n",
       "  cmd90$$user$YMParse@7d5aa0f9,\n",
       "  cmd90$$user$YMParse@262b106,\n",
       "  cmd90$$user$YMParse@7aa701f7,\n",
       "  cmd90$$user$YMParse@2cf85e54,\n",
       "  cmd90$$user$YMParse@479debe0,\n",
       "  cmd90$$user$YMParse@7ebe3bae,\n",
       "  cmd90$$user$YMParse@7f49446f,\n",
       "  cmd90$$user$YMParse@3dc03467,\n",
       "  cmd90$$user$YMParse@338aa4ff,\n",
       "  cmd90$$user$YMParse@7ee11a8a,\n",
       "  cmd90$$user$YMParse@6aa3341a,\n",
       "  cmd90$$user$YMParse@6760c017,\n",
       "  cmd90$$user$YMParse@4d9a0103,\n",
       "  cmd90$$user$YMParse@619df9fe,\n",
       "  cmd90$$user$YMParse@344af29,\n",
       "  cmd90$$user$YMParse@369f0720,\n",
       "  cmd90$$user$YMParse@22207762,\n",
       "\u001b[33m...\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val collection2 = new YMStream(\"resources\").stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres116\u001b[0m: \u001b[32mInt\u001b[0m = \u001b[32m473\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "collection(1).tokens.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres115\u001b[0m: \u001b[32mInt\u001b[0m = \u001b[32m291\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "collection2(1).tokens.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres22\u001b[0m: \u001b[32mList\u001b[0m[\u001b[32mString\u001b[0m] = \u001b[33mList\u001b[0m(\n",
       "  \u001b[32m\"strategist\"\u001b[0m,\n",
       "  \u001b[32m\"for\"\u001b[0m,\n",
       "  \u001b[32m\"jack\"\u001b[0m,\n",
       "  \u001b[32m\"kemp\"\u001b[0m,\n",
       "  \u001b[32m\"presidentialcampaign\"\u001b[0m,\n",
       "  \u001b[32m\"sai\"\u001b[0m,\n",
       "  \u001b[32m\"georg\"\u001b[0m,\n",
       "  \u001b[32m\"bush\"\u001b[0m,\n",
       "  \u001b[32m\"poor\"\u001b[0m,\n",
       "  \u001b[32m\"showe\"\u001b[0m,\n",
       "  \u001b[32m\"iowa\"\u001b[0m,\n",
       "  \u001b[32m\"coupl\"\u001b[0m,\n",
       "  \u001b[32m\"withkemp\"\u001b[0m,\n",
       "  \u001b[32m\"tough-talk\"\u001b[0m,\n",
       "  \u001b[32m\"ad\"\u001b[0m,\n",
       "  \u001b[32m\"against\"\u001b[0m,\n",
       "  \u001b[32m\"bob\"\u001b[0m,\n",
       "  \u001b[32m\"dole\"\u001b[0m,\n",
       "  \u001b[32m\"could\"\u001b[0m,\n",
       "\u001b[33m...\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Helper.stemTokens(collection(1).tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction \u001b[36mtoken_filter\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def token_filter(text_body: String) = {\n",
    "    processing.StopWords.filterOutSW(\n",
    "        processing.Tokenizer.tokenize(text_body.\n",
    "                                      replaceAll(\"\\\\P{L}+\", \" \"))\n",
    "    ).\n",
    "    map(x => PorterStemmer.stem(x)).filter(_.trim.nonEmpty).toList\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres75\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"   Strategists for Jack Kemp's presidentialcampaign say George Bush's poor showing in Iowa, coupled withKemp's tough-talking ads against Bob Dole, could put Kemp in therunning for the Republican nomination.   Before last Monday's Iowa caucuses, Kemp had been on a roll inNew Hampshire, using an effective advertising campaign and theendorsement of the influential Concord Monitor to help broadensupport.   But even as his bid to become the conservative alternative toDole and Bush took shape, the New York congressman faced thepossibility of being swept out of the race by former televisionevangelist Pat Robertson.   Using his second-place finish in Iowa _ Kemp came in fourth _Robertson was trying to move Kemp aside and take the mantle of trueconservative. A poll of New Hampshire voters by the Boston GlobeThursday night indicated Robertson had drawn even with Kemp amongpotential Republican voters in next Tuesday's primary.   Charles Douglas, Kemp's New Hampshire campaign manager, says theBuffalo congressman wants to finish second or third in a clusterwith Bush and Dole.   ``We want to be in the 28-30-31 percent range with them,''Douglas said.   He conceded that Kemp cannot afford another fourth-place finish,saying, ``We need to do well here so we can raise money andcontinue on to the South. We need to have at least a close thirdhere.''   Kemp spent most of the week in New Hampshire making his case atcoffee klatches, before high school students and in speeches to theparty faithful. He is hopeful that his energetic person-to-personstyle and strong conservative message will make him\u001b[33m...\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "collection(1).body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres100\u001b[0m: \u001b[32mInt\u001b[0m = \u001b[32m286\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "token_filter(collection(1).body).size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres73\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"   Strategists for Jack Kemp's presidentialcampaign say George Bush's poor showing in Iowa, coupled withKemp's tough-talking ads against Bob Dole, could put Kemp in therunning for the Republican nomination.   Before last Monday's Iowa caucuses, Kemp had been on a roll inNew Hampshire, using an effective advertising campaign and theendorsement of the influential Concord Monitor to help broadensupport.   But even as his bid to become the conservative alternative toDole and Bush took shape, the New York congressman faced thepossibility of being swept out of the race by former televisionevangelist Pat Robertson.   Using his second-place finish in Iowa _ Kemp came in fourth _Robertson was trying to move Kemp aside and take the mantle of trueconservative. A poll of New Hampshire voters by the Boston GlobeThursday night indicated Robertson had drawn even with Kemp amongpotential Republican voters in next Tuesday's primary.   Charles Douglas, Kemp's New Hampshire campaign manager, says theBuffalo congressman wants to finish second or third in a clusterwith Bush and Dole.   ``We want to be in the 28-30-31 percent range with them,''Douglas said.   He conceded that Kemp cannot afford another fourth-place finish,saying, ``We need to do well here so we can raise money andcontinue on to the South. We need to have at least a close thirdhere.''   Kemp spent most of the week in New Hampshire making his case atcoffee klatches, before high school students and in speeches to theparty faithful. He is hopeful that his energetic person-to-personstyle and strong conservative message will make him\u001b[33m...\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "collection(1).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mt\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"abcdefghijklmmnopq\"\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "var t = \"abcdefghijklmmnopq\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "java.lang.reflect.InvocationTargetException",
      "  sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
      "  sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
      "  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
      "  java.lang.reflect.Method.invoke(Method.java:483)",
      "  ammonite.Interpreter$$anonfun$evaluate$1$$anonfun$apply$9.apply(Interpreter.scala:325)",
      "  ammonite.Interpreter$.evaluating(Interpreter.scala:291)",
      "  ammonite.Interpreter$$anonfun$evaluate$1.apply(Interpreter.scala:325)",
      "  ammonite.Interpreter$$anonfun$evaluate$1.apply(Interpreter.scala:324)",
      "  ammonite.InterpreterAction$$anon$1.apply(Interpreter.scala:57)",
      "  ammonite.InterpreterAction$$anonfun$flatMap$1.apply(Interpreter.scala:44)",
      "  ammonite.InterpreterAction$$anonfun$flatMap$1.apply(Interpreter.scala:43)",
      "  ammonite.InterpreterAction$$anon$1.apply(Interpreter.scala:57)",
      "  ammonite.Interpreter$$anon$5$$anonfun$flatMap$5.apply(Interpreter.scala:303)",
      "  ammonite.Interpreter$$anon$5$$anonfun$flatMap$5.apply(Interpreter.scala:302)",
      "  ammonite.InterpreterAction$$anon$1.apply(Interpreter.scala:57)",
      "  ammonite.InterpreterAction$$anonfun$flatMap$1$$anonfun$apply$3.apply(Interpreter.scala:44)",
      "  ammonite.InterpreterAction$$anonfun$flatMap$1$$anonfun$apply$3.apply(Interpreter.scala:44)",
      "  scala.util.Either$RightProjection.flatMap(Either.scala:522)",
      "  ammonite.InterpreterAction$$anonfun$flatMap$1.apply(Interpreter.scala:44)",
      "  ammonite.InterpreterAction$$anonfun$flatMap$1.apply(Interpreter.scala:43)",
      "  ammonite.InterpreterAction$$anon$1.apply(Interpreter.scala:57)",
      "  ammonite.InterpreterAction$$anonfun$flatMap$1$$anonfun$apply$3.apply(Interpreter.scala:44)",
      "  ammonite.InterpreterAction$$anonfun$flatMap$1$$anonfun$apply$3.apply(Interpreter.scala:44)",
      "  scala.util.Either$RightProjection.flatMap(Either.scala:522)",
      "  ammonite.InterpreterAction$$anonfun$flatMap$1.apply(Interpreter.scala:44)",
      "  ammonite.InterpreterAction$$anonfun$flatMap$1.apply(Interpreter.scala:43)",
      "  ammonite.InterpreterAction$$anon$1.apply(Interpreter.scala:57)",
      "  ammonite.InterpreterAction$$anonfun$flatMap$1$$anonfun$apply$3.apply(Interpreter.scala:44)",
      "  ammonite.InterpreterAction$$anonfun$flatMap$1$$anonfun$apply$3.apply(Interpreter.scala:44)",
      "  scala.util.Either$RightProjection.flatMap(Either.scala:522)",
      "  ammonite.InterpreterAction$$anonfun$flatMap$1.apply(Interpreter.scala:44)",
      "  ammonite.InterpreterAction$$anonfun$flatMap$1.apply(Interpreter.scala:43)",
      "  ammonite.InterpreterAction$$anon$1.apply(Interpreter.scala:57)",
      "  ammonite.InterpreterAction$$anonfun$flatMap$1$$anonfun$apply$3.apply(Interpreter.scala:44)",
      "  ammonite.InterpreterAction$$anonfun$flatMap$1$$anonfun$apply$3.apply(Interpreter.scala:44)",
      "  scala.util.Either$RightProjection.flatMap(Either.scala:522)",
      "  ammonite.InterpreterAction$$anonfun$flatMap$1.apply(Interpreter.scala:44)",
      "  ammonite.InterpreterAction$$anonfun$flatMap$1.apply(Interpreter.scala:43)",
      "  ammonite.InterpreterAction$$anon$1.apply(Interpreter.scala:57)",
      "  ammonite.InterpreterAction$$anonfun$flatMap$1$$anonfun$apply$3.apply(Interpreter.scala:44)",
      "  ammonite.InterpreterAction$$anonfun$flatMap$1$$anonfun$apply$3.apply(Interpreter.scala:44)",
      "  scala.util.Either$RightProjection.flatMap(Either.scala:522)",
      "  ammonite.InterpreterAction$$anonfun$flatMap$1.apply(Interpreter.scala:44)",
      "  ammonite.InterpreterAction$$anonfun$flatMap$1.apply(Interpreter.scala:43)",
      "  ammonite.InterpreterAction$$anon$1.apply(Interpreter.scala:57)",
      "  ammonite.InterpreterAction$$anonfun$flatMap$1$$anonfun$apply$3.apply(Interpreter.scala:44)",
      "  ammonite.InterpreterAction$$anonfun$flatMap$1$$anonfun$apply$3.apply(Interpreter.scala:44)",
      "  scala.util.Either$RightProjection.flatMap(Either.scala:522)",
      "  ammonite.InterpreterAction$$anonfun$flatMap$1.apply(Interpreter.scala:44)",
      "  ammonite.InterpreterAction$$anonfun$flatMap$1.apply(Interpreter.scala:43)",
      "  ammonite.InterpreterAction$$anon$1.apply(Interpreter.scala:57)",
      "  ammonite.InterpreterAction$$anonfun$flatMap$1$$anonfun$apply$3.apply(Interpreter.scala:44)",
      "  ammonite.InterpreterAction$$anonfun$flatMap$1$$anonfun$apply$3.apply(Interpreter.scala:44)",
      "  scala.util.Either$RightProjection.flatMap(Either.scala:522)",
      "  ammonite.InterpreterAction$$anonfun$flatMap$1.apply(Interpreter.scala:44)",
      "  ammonite.InterpreterAction$$anonfun$flatMap$1.apply(Interpreter.scala:43)",
      "  ammonite.InterpreterAction$$anon$1.apply(Interpreter.scala:57)",
      "  ammonite.Interpreter$$anon$4$$anonfun$flatMap$4.apply(Interpreter.scala:246)",
      "  ammonite.Interpreter$$anon$4$$anonfun$flatMap$4.apply(Interpreter.scala:240)",
      "  ammonite.InterpreterAction$$anon$1.apply(Interpreter.scala:57)",
      "  ammonite.Interpreter$$anon$3$$anonfun$flatMap$3$$anonfun$apply$7.apply(Interpreter.scala:232)",
      "  ammonite.Interpreter$$anon$3$$anonfun$flatMap$3$$anonfun$apply$7.apply(Interpreter.scala:232)",
      "  ammonite.util.Capture$$anonfun$ammonite$util$Capture$$withErr$1.apply(Capture.scala:46)",
      "  scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)",
      "  scala.Console$.withErr(Console.scala:80)",
      "  ammonite.util.Capture$.ammonite$util$Capture$$withErr(Capture.scala:42)",
      "  ammonite.util.Capture$$anonfun$3.apply(Capture.scala:59)",
      "  ammonite.util.Capture$$anonfun$withOut$1.apply(Capture.scala:37)",
      "  scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)",
      "  scala.Console$.withOut(Console.scala:53)",
      "  ammonite.util.Capture$.withOut(Capture.scala:33)",
      "  ammonite.util.Capture$.withOutAndErr(Capture.scala:59)",
      "  ammonite.util.Capture$.apply(Capture.scala:106)",
      "  ammonite.Interpreter$$anon$3$$anonfun$flatMap$3.apply(Interpreter.scala:232)",
      "  ammonite.Interpreter$$anon$3$$anonfun$flatMap$3.apply(Interpreter.scala:231)",
      "  ammonite.InterpreterAction$$anon$1.apply(Interpreter.scala:57)",
      "  ammonite.InterpreterAction$$anonfun$flatMap$1$$anonfun$apply$3.apply(Interpreter.scala:44)",
      "  ammonite.InterpreterAction$$anonfun$flatMap$1$$anonfun$apply$3.apply(Interpreter.scala:44)",
      "  scala.util.Either$RightProjection.flatMap(Either.scala:522)",
      "  ammonite.InterpreterAction$$anonfun$flatMap$1.apply(Interpreter.scala:44)",
      "  ammonite.InterpreterAction$$anonfun$flatMap$1.apply(Interpreter.scala:43)",
      "  ammonite.InterpreterAction$$anon$1.apply(Interpreter.scala:57)",
      "  ammonite.Interpreter$$anon$2$$anonfun$flatMap$2.apply(Interpreter.scala:205)",
      "  ammonite.Interpreter$$anon$2$$anonfun$flatMap$2.apply(Interpreter.scala:204)",
      "  ammonite.InterpreterAction$$anon$1.apply(Interpreter.scala:57)",
      "  ammonite.InterpreterAction$$anonfun$flatMap$1.apply(Interpreter.scala:44)",
      "  ammonite.InterpreterAction$$anonfun$flatMap$1.apply(Interpreter.scala:43)",
      "  ammonite.InterpreterAction$$anon$1.apply(Interpreter.scala:57)",
      "  ammonite.InterpreterAction$$anonfun$flatMap$1$$anonfun$apply$3.apply(Interpreter.scala:44)",
      "  ammonite.InterpreterAction$$anonfun$flatMap$1$$anonfun$apply$3.apply(Interpreter.scala:44)",
      "  scala.util.Either$RightProjection.flatMap(Either.scala:522)",
      "  ammonite.InterpreterAction$$anonfun$flatMap$1.apply(Interpreter.scala:44)",
      "  ammonite.InterpreterAction$$anonfun$flatMap$1.apply(Interpreter.scala:43)",
      "  ammonite.InterpreterAction$$anon$1.apply(Interpreter.scala:57)",
      "  jupyter.scala.ScalaInterpreter$$anon$2.interpret(ScalaInterpreter.scala:218)",
      "  jupyter.kernel.interpreter.InterpreterHandler$$anonfun$execute$1$$anonfun$apply$6.apply(InterpreterHandler.scala:118)",
      "  jupyter.kernel.interpreter.InterpreterHandler$$anonfun$execute$1$$anonfun$apply$6.apply(InterpreterHandler.scala:100)",
      "  jupyter.kernel.interpreter.InterpreterHandler$$anonfun$jupyter$kernel$interpreter$InterpreterHandler$$publishing$1$$anonfun$2.apply(InterpreterHandler.scala:68)",
      "  jupyter.kernel.interpreter.InterpreterHandler$$anonfun$jupyter$kernel$interpreter$InterpreterHandler$$publishing$1$$anonfun$2.apply(InterpreterHandler.scala:68)",
      "  scalaz.concurrent.Task$.Try(Task.scala:386)",
      "  scalaz.concurrent.Task$$anonfun$unsafeStart$1.apply(Task.scala:295)",
      "  scalaz.concurrent.Task$$anonfun$unsafeStart$1.apply(Task.scala:295)",
      "  scalaz.concurrent.Future$$anonfun$apply$15$$anon$4.call(Future.scala:380)",
      "  scalaz.concurrent.Future$$anonfun$apply$15$$anon$4.call(Future.scala:380)",
      "  java.util.concurrent.FutureTask.run(FutureTask.java:266)",
      "  java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
      "  java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
      "  java.lang.Thread.run(Thread.java:745)",
      "java.lang.OutOfMemoryError: GC overhead limit exceeded (GC overhead limit exceeded)",
      "  java.util.Arrays.copyOf(Arrays.java:3332)",
      "  java.lang.AbstractStringBuilder.expandCapacity(AbstractStringBuilder.java:137)",
      "  java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:121)",
      "  java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:421)",
      "  java.lang.StringBuffer.append(StringBuffer.java:272)",
      "  com.sun.org.apache.xerces.internal.dom.NodeImpl.getTextContent(NodeImpl.java:1309)",
      "  com.sun.org.apache.xerces.internal.dom.ParentNode.getTextContent(ParentNode.java:645)",
      "  com.sun.org.apache.xerces.internal.dom.ParentNode.getTextContent(ParentNode.java:645)",
      "  com.sun.org.apache.xerces.internal.dom.ParentNode.getTextContent(ParentNode.java:634)",
      "  com.sun.org.apache.xerces.internal.dom.ParentNode.getTextContent(ParentNode.java:626)",
      "  ch.ethz.dal.tinyir.processing.XMLDocument$$anonfun$1.apply(XMLDocument.scala:42)",
      "  ch.ethz.dal.tinyir.processing.XMLDocument$$anonfun$1.apply(XMLDocument.scala:42)",
      "  scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)",
      "  scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)",
      "  scala.collection.immutable.Range.foreach(Range.scala:160)",
      "  scala.collection.TraversableLike$class.map(TraversableLike.scala:234)",
      "  scala.collection.AbstractTraversable.map(Traversable.scala:104)",
      "  ch.ethz.dal.tinyir.processing.XMLDocument.read(XMLDocument.scala:42)",
      "  ch.ethz.dal.tinyir.processing.TipsterParse.body(TipsterParse.scala:13)",
      "  ch.ethz.dal.tinyir.processing.TipsterParse.content(TipsterParse.scala:16)",
      "  ch.ethz.dal.tinyir.processing.Document.tokens(Document.scala:13)",
      "  ch.ethz.dal.tinyir.indexing.PosIndex$$anonfun$postings$1.apply(PosIndex.scala:36)",
      "  ch.ethz.dal.tinyir.indexing.PosIndex$$anonfun$postings$1.apply(PosIndex.scala:36)",
      "  scala.collection.immutable.Stream$$anonfun$flatMap$1.apply(Stream.scala:497)",
      "  scala.collection.immutable.Stream$$anonfun$flatMap$1.apply(Stream.scala:497)",
      "  scala.collection.immutable.Stream.append(Stream.scala:255)",
      "  scala.collection.immutable.Stream$$anonfun$append$1.apply(Stream.scala:255)",
      "  scala.collection.immutable.Stream$$anonfun$append$1.apply(Stream.scala:255)",
      "  scala.collection.immutable.Stream$Cons.tail(Stream.scala:1233)",
      "  scala.collection.immutable.Stream$Cons.tail(Stream.scala:1223)",
      "  scala.collection.generic.Growable$class.loop$1(Growable.scala:54)",
      "  scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:57)"
     ]
    }
   ],
   "source": [
    "val test_pos_index = new indexing.PosIndex(collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Memory:  1545\n",
      "Free Memory:  1509\n",
      "Total Memory: 3055\n",
      "Max Memory:   3641\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mmb\u001b[0m: \u001b[32mInt\u001b[0m = \u001b[32m1048576\u001b[0m\n",
       "\u001b[36mruntime\u001b[0m: \u001b[32mRuntime\u001b[0m = java.lang.Runtime@45e471e3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val mb = 1024*1024\n",
    "val runtime = Runtime.getRuntime\n",
    "println(s\"Used Memory:  \" + (runtime.totalMemory - runtime.freeMemory) / mb)\n",
    "println(s\"Free Memory:  \" + runtime.freeMemory / mb)\n",
    "println(s\"Total Memory: \" + runtime.totalMemory / mb)\n",
    "println(s\"Max Memory:   \" + runtime.maxMemory / mb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres52\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"\"\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "collection(1).title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass \u001b[36mYMParse\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class YMParse(is: InputStream) extends processing.TipsterParse(is) { \n",
    "  override def tokens: List[String] = token_filter(content)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36mch.ethz.dal.tinyir.processing.TipsterParse\u001b[0m\n",
       "\u001b[32mimport \u001b[36mch.ethz.dal.tinyir.processing.Tokenizer\u001b[0m\n",
       "\u001b[32mimport \u001b[36mch.ethz.dal.tinyir.processing.XMLDocument\u001b[0m\n",
       "defined \u001b[32mclass \u001b[36mYMStream\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ch.ethz.dal.tinyir.processing.TipsterParse\n",
    "import ch.ethz.dal.tinyir.processing.Tokenizer\n",
    "import ch.ethz.dal.tinyir.processing.XMLDocument\n",
    "\n",
    "class YMStream (path: String, ext: String = \"\") extends io.ParsedXMLStream(new io.ZipDirStream(path, \"\")){\n",
    "  def stream : Stream[XMLDocument] = unparsed.stream.map(is => new YMParse(is))\n",
    "  def length = unparsed.length \n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31mMain.scala:38: not found: value DocStream",
      "    val parse = new TipsterParse(DocStream.getStream(fname))",
      "                                 ^\u001b[0m"
     ]
    }
   ],
   "source": [
    "\n",
    "class TipsterParse(is: InputStream) extends XMLDocument(is) { \n",
    "  override def title  : String = \"\" \n",
    "  override def body   : String = read(doc.getElementsByTagName(\"TEXT\"))\n",
    "  override def name   : String = read(doc.getElementsByTagName(\"DOCNO\")).filter(_.isLetterOrDigit)\n",
    "  override def date   : String = \"\"\n",
    "  override def content: String = body  \n",
    "}\n",
    "\n",
    "object TipsterParse {\n",
    "  def main(args: Array[String]) {\n",
    "    val dirname = \"/Users/thofmann/Data/Tipster/sample\"\n",
    "    val fname = dirname + \"/DOE2-84-0001\"\n",
    "    val parse = new TipsterParse(DocStream.getStream(fname))\n",
    "    val name = parse.name\n",
    "    println(name)    \n",
    "    val content = parse.content \n",
    "    println(content.take(20) + \"...\" + content.takeRight(20))\n",
    "  }\n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "package ch.ethz.dal.tinyir.io\n",
    "\n",
    "import ch.ethz.dal.tinyir.processing.TipsterParse\n",
    "import ch.ethz.dal.tinyir.processing.Tokenizer\n",
    "import ch.ethz.dal.tinyir.processing.XMLDocument\n",
    "\n",
    "class TipsterStream (path: String, ext: String = \"\") \n",
    "extends ParsedXMLStream(new ZipDirStream(path, \"\")){\n",
    "  def stream : Stream[XMLDocument] = unparsed.stream.map(is => new TipsterParse(is))\n",
    "  def length = unparsed.length \n",
    "}\n",
    "\n",
    "object TipsterStream  {\n",
    "\n",
    "  def main(args: Array[String]) {\n",
    "    val tipster = new TipsterStream (\"/Users/thofmann/Data/Tipster/zips\")  \n",
    "    println(\"Number of files in zips = \" + tipster.length)\n",
    "    \n",
    "    var length : Long = 0 \n",
    "    var tokens : Long = 0\n",
    "    for (doc <- tipster.stream.take(10000)) { \n",
    "      length += doc.content.length          \n",
    "      tokens += doc.tokens.length\n",
    "    }\n",
    "    println(\"Final number of characters = \" + length)\n",
    "    println(\"Final number of tokens     = \" + tokens)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36mscala.collection.mutable.{Map => MutMap}\u001b[0m\n",
       "\u001b[32mimport \u001b[36mcom.github.aztek.porterstemmer.PorterStemmer\u001b[0m\n",
       "\u001b[32mimport \u001b[36mscala.collection.mutable.Iterable\u001b[0m\n",
       "\u001b[32mimport \u001b[36mjava.io.File\u001b[0m\n",
       "\u001b[32mimport \u001b[36mjava.io.FileWriter\u001b[0m\n",
       "defined \u001b[32mclass \u001b[36mHelper\u001b[0m\n",
       "defined \u001b[32mobject \u001b[36mHelper\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import scala.collection.mutable.{Map => MutMap}\n",
    "import com.github.aztek.porterstemmer.PorterStemmer\n",
    "import scala.collection.mutable.Iterable\n",
    "import java.io.File\n",
    "import java.io.FileWriter\n",
    "\n",
    "class Helper {\n",
    "}\n",
    "\n",
    "object Helper {\n",
    "  val ZIP_PATH = \"/zips\"\n",
    "  val QRELS_PATH = \"/qrels\"\n",
    "  val TOPIC_PATH = \"/topics\"\n",
    "    \n",
    "  val OUTPUT_FILE = \"/output/ranking-M-tony-beltramelli.run\"\n",
    "  \n",
    "  val RESULT_NUMBER = 100\n",
    "  var TOKEN_MAX_SIZE = 100000\n",
    "\n",
    "  val IS_DEBUG_MODE: Boolean = false\n",
    "\n",
    "  private var _rootPath = \"\"\n",
    "  \n",
    "  private var _i = 0\n",
    "  private var _time : Long = System.nanoTime()\n",
    "  \n",
    "  def getPath(r: String ) : String = {\n",
    "    _rootPath + r\n",
    "  }\n",
    "  \n",
    "  def setRootPath(r: String) {\n",
    "    _rootPath = r\n",
    "  }\n",
    "\n",
    "  def debug(s: Any) {\n",
    "    if (!IS_DEBUG_MODE) return\n",
    "    println(s)\n",
    "  }\n",
    "  \n",
    "  private val _stemStore : MutMap[String, String] = MutMap()\n",
    "  \n",
    "  def stemTokens(list: List[String]) : List[String] = \n",
    "  {\n",
    "    if(_stemStore.size > TOKEN_MAX_SIZE) _stemStore.clear\n",
    "\t  \n",
    "    list.map(t => t.toLowerCase()).map(v => _stemStore.getOrElseUpdate(v, PorterStemmer.stem(v)))\n",
    "  }\n",
    "  \n",
    "  def time {\n",
    "    println(\"time \"+_i+\" : \" + (System.nanoTime() - _time) / 1000000000.0 + \" seconds\")\n",
    "    _i += 1\n",
    "  }\n",
    "\t\n",
    "  def log2(x: Double) = Math.log10(x) / Math.log10(2.0)\n",
    "  \n",
    "  def flipDimensions(original : Iterable[(String, List[(Int, Double)])]) : Map[Int, Iterable[(String, Double)]] =\n",
    "  {\n",
    "    val flatten = for {\n",
    "      (s, v) <- original\n",
    "      (i, d) <- v\n",
    "    } yield (i, s, d)\n",
    "\t\n",
    "    implicit class RichTuple2[A, B, C](t: (A, B, C)) {\n",
    "      def tail: (B, C) = (t._2, t._3)\n",
    "    }\n",
    "\t\n",
    "    flatten.groupBy(_._1).mapValues(_.map(_.tail))\n",
    "  }\n",
    "  \n",
    "  def printToFile(results : Map[Int, List[(String, Double)]], topics : List[(String, Int)], useLanguageModel : Boolean)\n",
    "  {\n",
    "    val file = new File(_rootPath + OUTPUT_FILE.replace('M', if(useLanguageModel) 'l' else 't'))\n",
    "    file.getParentFile.mkdirs\n",
    "    \n",
    "    val fw = new FileWriter(file)\n",
    "    \n",
    "    results.foreach(r => r._2.zipWithIndex.foreach{case(l, i) => fw.write(topics(r._1)._2.toString + \" \" + (i + 1) + \" \" + l._1 + \"\\n\")})\n",
    "    \n",
    "    fw.close\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def HashMap_get_count(w: Seq[String]): scala.collection.immutable.Map[String, Int] = {\n",
    "    // map(token -> count)\n",
    "    // auxilary function to find duplicate elements\n",
    "    // and return their counts in a Seq\n",
    "        val map = scala.collection.immutable.HashMap[String, Int]().withDefaultValue(0)\n",
    "        w.foldLeft(map)((m, c) => m + (c -> (m(c) + 1)) )\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pruned_vocab(total_vocab: scala.collection.mutable.Map[String, Int], min_count: Int)={\n",
    "    val temp = total_vocab.clone\n",
    "    temp.retain((k,v) => v > min_count)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def total_vocab(file_list: Array[File])={\n",
    "    val docwords = MutMap[String, Int]()\n",
    "    for(iter<-0 to file_list.length - 1) {\n",
    "        var doc = new xml_doc(file_list(iter).toString)\n",
    "        var tokens = doc.token_stem\n",
    "        for(t <- tokens){\n",
    "            var temp = docwords.getOrElseUpdate(t, 0)  \n",
    "            docwords.update(t,temp+1)\n",
    "        }\n",
    "    }\n",
    "    docwords\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def assess(retrievedTopics: Set[String], expecedTopics: Set[String]) = {\n",
    "        val truePositive = (retrievedTopics & expecedTopics).size\n",
    "        val falsePositive = (retrievedTopics -- expecedTopics).size\n",
    "        val falseNegative = (expecedTopics -- retrievedTopics).size\n",
    "\n",
    "    val precision = truePositive.toDouble / (truePositive + falsePositive)\n",
    "    val recall = truePositive.toDouble / (truePositive + falseNegative)\n",
    "    var f1Score = 2 * (precision * recall) / (precision + recall)\n",
    "    f1Score = if (f1Score.isNaN) {0} else {f1Score}\n",
    "    \n",
    "    (f1Score, precision, recall)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val train_list = list_xml_files(\"data/train\")\n",
    "val val_list = list_xml_files(\"data/validation\")\n",
    "val test_list = list_xml_files(\"data/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document maps creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_doc_maps(file_list: Array[File],\n",
    "               iter_start: Int = 0,\n",
    "               iter_end: Int, pruned_vocab_set: collection.Set[String]) = {\n",
    "    \n",
    "    //document index -> ((term -> tfs), size, topics)\n",
    "    val _documents = MutMap[Int, \n",
    "                            (scala.collection.Map[String, Int], \n",
    "                             Int, Set[String])]()\n",
    "    \n",
    "    //class name -> document indexes\n",
    "    val _classesToDoc = MutMap[String, Set[Int]]()\n",
    "    \n",
    "//     var iter = iter_start\n",
    "    \n",
    "    for(iter <- iter_start to iter_end - 1){\n",
    "        \n",
    "        // get tokens\n",
    "        var path = file_list(iter).toString\n",
    "        var cur_doc = new xml_doc(path)\n",
    "        var tokens = cur_doc.token_stem\n",
    "        \n",
    "        var pruned_tokens = tokens.filter(token=>pruned_vocab_set(token))\n",
    "        \n",
    "        // get labels\n",
    "        var labels = cur_doc.labels.toSet\n",
    "        \n",
    "        var ID =  cur_doc.id\n",
    "\n",
    "        _documents += ID -> ((HashMap_get_count(pruned_tokens), pruned_tokens.length, labels))\n",
    "    \n",
    "        for(c <- labels){\n",
    "            val cl = _classesToDoc.getOrElseUpdate(c, Set[Int]())  \n",
    "            _classesToDoc.update(c, cl + ID)\n",
    "        }\n",
    "        \n",
    "        if( (iter + 1) % 200 == 0){\n",
    "            println(s\"Current iteration: ${iter + 1}\")\n",
    "        }\n",
    "    }\n",
    "(_documents, _classesToDoc)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val vocab = total_vocab(file_list)\n",
    "val pruned_vocab = get_pruned_vocab(vocab,1)//prune vocabulary to remove tokens with frequency 1\n",
    "val documents = get_doc_maps(train_list, iter_end = train_list.length, pruned_vocab_set=pruned_vocab.keys.toSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "//document index -> ((term -> tfs), size, topics)\n",
    "val documents_tf_map = documents._1\n",
    "\n",
    "//class name -> document indexes\n",
    "val class_to_doc = documents._2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "//term -> tf-idf\n",
    "def get_inverseFreq(\n",
    "    documents_tf_map: MutMap[Int, \n",
    "                            (Map[String, Int], \n",
    "                             Int, Set[String])]) = {\n",
    "    \n",
    "    var inverseFreq = MutMap[String, Double]()\n",
    "    \n",
    "    var iter = 0\n",
    "    for(d <- documents_tf_map.map(_._2._1)) {\n",
    "        for(t <- d) {\n",
    "            val v = inverseFreq.getOrElse(t._1, -1.0)\n",
    "\n",
    "            if(v >= 0) inverseFreq.update(t._1, t._2 + v) else inverseFreq += t._1 -> t._2\n",
    "        }\n",
    "        if( (iter + 1) % 50 == 0){\n",
    "            println(s\"Current iteration: ${iter + 1}\")\n",
    "            }\n",
    "        iter += 1\n",
    "    }\n",
    "    inverseFreq = inverseFreq.map(f => f._1 -> (Math.log(iter) - Math.log(f._2)))\n",
    "    inverseFreq\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val inverseFreq = get_inverseFreq(documents_tf_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "// helper functions\n",
    "\n",
    "def sigmoid(theta: MutMap[String, Double], documentFeatures: Map[String, Double]) = {\n",
    "    1.0 / (1.0 + Math.exp(-dot_product(documentFeatures, theta)))\n",
    "}\n",
    "\n",
    "def dot_product(vector1: Map[String, Double], vector2: MutMap[String, Double]) = {\n",
    "    vector1.map(v => v._2 * vector2.getOrElse(v._1, 0.0)).sum\n",
    "}\n",
    "\n",
    "def scalar_product(vector: Map[String, Double], scalar: Double) = {\n",
    "    vector.mapValues(v => v * scalar)\n",
    "}\n",
    "\n",
    "def scalar_product_mut(vector: MutMap[String, Double], scalar: Double) = {\n",
    "    vector.mapValues(v => v * scalar)\n",
    "}\n",
    "\n",
    "def add_map(vector1: Map[String, Double], vector2: MutMap[String, Double]) = {\n",
    "    vector1.map(v => v._1 -> (vector2.getOrElse(v._1, 0.0) + v._2))\n",
    "}\n",
    "\n",
    "def add_map_immut(vector1: Map[String, Double], vector2: scala.collection.Map[String,Double]) = {\n",
    "    vector1.map(v => v._1 -> (vector2.getOrElse(v._1, 0.0) + v._2))\n",
    "}\n",
    "\n",
    "def takeRandomN[A](n: Int, org_list: Set[A]) ={\n",
    "  scala.util.Random.shuffle(org_list).take(n)\n",
    "}\n",
    "\n",
    "def sum_square (some_map: MutMap[String, Double]) = {\n",
    "    some_map.values.foldLeft(0.0)(_ + Math.pow(_, 2))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "object Logistic_reg {\n",
    "    \n",
    "    // init placeholder vars\n",
    "    var _classesToDoc = MutMap[String, Set[Int]]() //class name -> document indexes\n",
    "    \n",
    "    var _documents = MutMap[Int, (Map[String, Int], Int, Set[String])]() //document index -> ((term -> tfs), size, topics)\n",
    "    \n",
    "    var _inverseFreq = MutMap[String, Double]()  //term -> tf-idf\n",
    "    \n",
    "    var corp_size = 0\n",
    "    \n",
    "    var label_to_weight = MutMap[String, MutMap[String, Double]]()  // label -> (token -> weight)\n",
    "    \n",
    "    var lambda = 0.0  // regularization constant\n",
    "    \n",
    "    var step_size = 1.0  // for weights update\n",
    "    \n",
    "    // set placeholder variables\n",
    "    def set_class_to_doc(value: MutMap[String, Set[Int]]) = {\n",
    "        _classesToDoc = value\n",
    "    }\n",
    "    \n",
    "    def set_documents_tf_map(value: MutMap[Int, (Map[String, Int], Int, Set[String])]) = {\n",
    "        _documents = value\n",
    "        corp_size = value.size\n",
    "    }\n",
    "    \n",
    "    def set_inverseFreq(value: MutMap[String, Double]) = {\n",
    "        _inverseFreq = value\n",
    "    }\n",
    "    \n",
    "    def set_lambda(value: Double) = {\n",
    "        lambda = value\n",
    "    }\n",
    "    \n",
    "    def set_step_size(value: Double) = {\n",
    "        step_size = value\n",
    "    }\n",
    "    \n",
    "    // generate prediction\n",
    "    def getProb(documentFeatures: Map[String, Double],\n",
    "               theta: MutMap[String, Double]) = {\n",
    "        sigmoid(theta, documentFeatures)\n",
    "    }\n",
    "    \n",
    "    // gradient calculates for a SINGLE LABEL\n",
    "    // update weights\n",
    "    def gradient(theta: MutMap[String, Double], \n",
    "                  documentFeatures: Map[String, Double], \n",
    "                  y: Boolean) : Map[String, Double] = {\n",
    "        val loss_contrib = if(y) 1 - sigmoid(theta, documentFeatures) else -sigmoid(theta, documentFeatures)\n",
    "        val grad = scalar_product(documentFeatures, loss_contrib * step_size)\n",
    "        val reg_shrink = scalar_product_mut(theta, -lambda * step_size)\n",
    "        \n",
    "        add_map(\n",
    "            add_map_immut(grad, reg_shrink)\n",
    "            ,theta)\n",
    "    }\n",
    "    \n",
    "    // train on a single training example (doc)\n",
    "    def train_on_label(label: String,\n",
    "                       documentFeatures: Map[String, Double], \n",
    "                       y: Boolean) =  {\n",
    "        \n",
    "        if (!(label_to_weight contains label)){\n",
    "            label_to_weight.update(label, MutMap[String, Double]().withDefaultValue(0.5))\n",
    "        }\n",
    "        label_to_weight(label) ++= gradient(label_to_weight(label), \n",
    "                                             documentFeatures, y)\n",
    "    }\n",
    "    \n",
    "    // train on a signle \"topic\" with a set of docs\n",
    "    def _train(label: String) = {\n",
    "        // train on a set of docs (identified by docIndex)\n",
    "        \n",
    "        val doc_collection = takeRandomN(800, RandomDocuments(label))\n",
    "        \n",
    "        for(docIndex <- doc_collection) {\n",
    "\n",
    "            val doc = _documents(docIndex)\n",
    "            val y = doc._3.contains(label)\n",
    "            train_on_label(label, doc._1.map(f => f._1 -> _inverseFreq(f._1)), y)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def trainAll = {\n",
    "        var iter = 1\n",
    "        for(classToDoc <- _classesToDoc) {\n",
    "            _train(classToDoc._1)\n",
    "            if(iter % 25 == 0){\n",
    "                println(s\"Current iteration: label #${iter}\")\n",
    "            }\n",
    "            iter += 1\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def RandomDocuments(trueTopic: String) = {\n",
    "        val random = new Random\n",
    "        var documents = _classesToDoc(trueTopic)\n",
    "\n",
    "        var sample_size = documents.size * 3\n",
    "        sample_size = if(sample_size > corp_size) {\n",
    "            corp_size} else {sample_size}\n",
    "\n",
    "        documents = documents ++ takeRandomN(sample_size, _documents.keys.toSet)\n",
    "\n",
    "        random.shuffle(documents)\n",
    "    }\n",
    "    \n",
    "    // predicting a set of classes, given a list of tokens\n",
    "    def predict(tokens: Set[String], threshold: Double, cut_num: Int = 7) : Set[String]= {\n",
    "        \n",
    "        var label_to_prob = MutMap[String, Double]()\n",
    "        \n",
    "        val documentFeatures = tokens.map(f => f -> _inverseFreq.getOrElse(f, 0.0)).filter(_._2 > 0.0).toMap\n",
    "        \n",
    "        for(classToDoc <- _classesToDoc) {\n",
    "            val label = classToDoc._1\n",
    "            val weights = label_to_weight(label)\n",
    "            // insert a (label -> probability) entry\n",
    "            label_to_prob(label) = getProb(documentFeatures,\n",
    "                                           weights)\n",
    "        }\n",
    "        \n",
    "        // keep labels with probability higher than threshold in a set\n",
    "        label_to_prob.filter({case (k,v) => v > threshold}).  // remove labels below threshold\n",
    "            toSeq.sortWith(_._2 > _._2).take(cut_num).map(x => x._1).toSet  // keep cut_num labels with max probability\n",
    "        \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def assess_files_logistic(file_list: Array[File],\n",
    "                         threshold: Double = 0.95) = {\n",
    "    // calculate F score for all docs in validation set\n",
    "    var f1_score = Set[Double]()\n",
    "    var precision = Set[Double]()\n",
    "    var recall = Set[Double]()\n",
    "\n",
    "    for(path <- file_list) {\n",
    "\n",
    "        var iter = 1\n",
    "        val current_doc = new xml_doc(path.toString)\n",
    "\n",
    "        var cur_scores = assess(\n",
    "            Logistic_reg.predict(current_doc.token_stem.toSet, threshold),\n",
    "            current_doc.labels.toSet)\n",
    "\n",
    "        f1_score += cur_scores._1\n",
    "        precision += cur_scores._2\n",
    "        recall += cur_scores._3\n",
    "\n",
    "        if( iter % 25 == 0){\n",
    "            println(s\"Current iteration: #${iter}, working on file ${path.toString}\")\n",
    "        }\n",
    "        iter += 1\n",
    "    }\n",
    "    (f1_score.sum / f1_score.size,\n",
    "     precision.sum / precision.size,\n",
    "     recall.sum / recall.size)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "// init map values\n",
    "Logistic_reg.set_class_to_doc(class_to_doc)\n",
    "Logistic_reg.set_documents_tf_map(documents_tf_map)\n",
    "Logistic_reg.set_inverseFreq(inverseFreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "var hyper_param_res = MutMap[Double, (Double, Double, Double)]()\n",
    "val hyper_param_list = List.tabulate(6)(x => Math.pow(10, -(x+1) )).tail\n",
    "val step_size_list = List.range(1,6).map(x => 1 / Math.sqrt(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for (hyp_param <- hyper_param_list) {\n",
    "    // set lambda value\n",
    "    Logistic_reg.set_lambda(hyp_param)\n",
    "    println(s\"Lambda set to $hyp_param\")\n",
    "    println(\"New training cycle starting\")\n",
    "    \n",
    "    // reset weights before training\n",
    "    Logistic_reg.label_to_weight = \n",
    "        MutMap[String, collection.mutable.Map[String, Double]]()\n",
    "    \n",
    "    // train epochs\n",
    "    for(step <- step_size_list) {\n",
    "        Logistic_reg.set_step_size(step)\n",
    "        println(s\"Step size set to $step\")\n",
    "        println()\n",
    "        Logistic_reg.trainAll\n",
    "    }\n",
    "\n",
    "    hyper_param_res(hyp_param) = assess_files_logistic(val_list)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write(hyper_param_res, \"logistic_reg_hyperparam.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "// train on expanded set and produce final prediction\n",
    "val expanded_train_list = train_list ++ val_list\n",
    "val log_reg_lambda = 0.001\n",
    "\n",
    "Logistic_reg.set_lambda(log_reg_lambda)\n",
    "// reset weights before training\n",
    "Logistic_reg.label_to_weight = \n",
    "    MutMap[String, collection.mutable.Map[String, Double]]()\n",
    "\n",
    "// train epochs\n",
    "for(step <- step_size_list) {\n",
    "    Logistic_reg.set_step_size(step)\n",
    "    println(s\"Step size set to $step\")\n",
    "    println()\n",
    "    Logistic_reg.trainAll\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "// predict on test set\n",
    "var log_reg_test_predict = MutMap[Int, Set[String]]()\n",
    "val numPattern = \"[0-9]+\".r\n",
    "\n",
    "var iter = 1\n",
    "\n",
    "for (path <- test_list) {\n",
    "    var ID = numPattern.findFirstIn(path.toString).get.toInt\n",
    "    var current_doc = new xml_doc(path.toString).token_stem.toSet\n",
    "    \n",
    "    log_reg_test_predict(ID) = Logistic_reg.predict(current_doc, 0.95)\n",
    "\n",
    "    if( iter % 100 == 0){\n",
    "        println(s\"Current iteration: #${iter}, predicting for file ${path.toString}\")\n",
    "    }\n",
    "    iter += 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// write predictions to file\n",
    "write_prediction(log_reg_test_predict, \"ir-2016-1-project-24-lr.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "object Svm {\n",
    "    // init placeholder vars\n",
    "    var _classesToDoc = MutMap[String, Set[Int]]() //class name -> document indexes\n",
    "    \n",
    "    var _documents = MutMap[Int, (Map[String, Int], Int, Set[String])]() //document index -> ((term -> tfs), size, topics)\n",
    "    \n",
    "    var _inverseFreq = MutMap[String, Double]()  //term -> tf-idf\n",
    "    \n",
    "    var corp_size = 0\n",
    "    \n",
    "    var label_to_weight = MutMap[String, MutMap[String, Double]]()  // label -> (token -> weight)\n",
    "    \n",
    "    var lambda = 0.1  // needed for projection, |theta|_2 <= 1/sqrt(lambda)\n",
    "    \n",
    "    var step_size = 1.0  // for weights update\n",
    "    \n",
    "    // set placeholder variables\n",
    "    def set_class_to_doc(value: MutMap[String, Set[Int]]) = {\n",
    "        _classesToDoc = value\n",
    "    }\n",
    "    \n",
    "    def set_documents_tf_map(value: MutMap[Int, (Map[String, Int], Int, Set[String])]) = {\n",
    "        _documents = value\n",
    "        corp_size = value.size\n",
    "    }\n",
    "    \n",
    "    def set_inverseFreq(value: MutMap[String, Double]) = {\n",
    "        _inverseFreq = value\n",
    "    }\n",
    "    \n",
    "    def set_lambda(value: Double) = {\n",
    "        lambda = value\n",
    "    }\n",
    "    \n",
    "    def set_step_size(value: Double) = {\n",
    "        step_size = value\n",
    "    }\n",
    "    \n",
    "    // generate prediction score (dot product)\n",
    "    def getScore(documentFeatures: Map[String, Double],\n",
    "               theta: MutMap[String, Double]) = {\n",
    "        dot_product(documentFeatures, theta)\n",
    "    }\n",
    "    \n",
    "    // gradient calculates for a SINGLE LABEL\n",
    "    // update weights\n",
    "    def gradient(theta: MutMap[String, Double], \n",
    "                  documentFeatures: Map[String, Double], \n",
    "                  y: Boolean) = {\n",
    "        \n",
    "        val y_label = if(y) 1 else -1\n",
    "        \n",
    "        val theta_regular = scalar_product_mut(theta, 1 - step_size * lambda)\n",
    "        \n",
    "        val hinge = 1.0 - y_label * dot_product(documentFeatures, theta)\n",
    "        \n",
    "        if(hinge <= 0) { theta_regular }\n",
    "        else {\n",
    "            val projection = Math.min(\n",
    "                1 , (1.0 / Math.sqrt(lambda + sum_square(theta)) )\n",
    "            )\n",
    "            scalar_product(add_map_immut(\n",
    "                scalar_product(documentFeatures, step_size * y_label),\n",
    "                theta_regular), \n",
    "                                  projection)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    // train on a single training example (doc)\n",
    "    def train_on_label(label: String,\n",
    "                       documentFeatures: Map[String, Double], \n",
    "                       y: Boolean) =  {\n",
    "        \n",
    "        if (!(label_to_weight contains label)){\n",
    "            label_to_weight.update(label, MutMap[String, Double]().withDefaultValue(0.5))\n",
    "        }\n",
    "        \n",
    "        label_to_weight(label) ++= gradient(label_to_weight(label), \n",
    "                                             documentFeatures, y)\n",
    "    }\n",
    "    \n",
    "    // train on a single \"topic\" with a set of docs\n",
    "    def _train(label: String) = {\n",
    "        // train on a set of docs (identified by docIndex)\n",
    "        val doc_collection = takeRandomN(800, RandomDocuments(label))\n",
    "\n",
    "        for(docIndex <- doc_collection) {\n",
    "\n",
    "            val doc = _documents(docIndex)\n",
    "            val y = doc._3.contains(label)\n",
    "            \n",
    "            train_on_label(label, doc._1.map(f => f._1 -> _inverseFreq(f._1)), y)\n",
    "\n",
    "        }\n",
    "    }\n",
    "\n",
    "    def RandomDocuments(trueTopic: String) = {\n",
    "        val random = new Random\n",
    "        var documents = _classesToDoc(trueTopic)\n",
    "\n",
    "        var sample_size = documents.size * 3\n",
    "        sample_size = if(sample_size > corp_size) {\n",
    "            corp_size} else {sample_size}\n",
    "\n",
    "        documents = documents ++ takeRandomN(sample_size, _documents.keys.toSet)\n",
    "\n",
    "        random.shuffle(documents)\n",
    "    }\n",
    "    \n",
    "    def trainAll = {\n",
    "        var iter = 1\n",
    "        \n",
    "        for(classToDoc <- _classesToDoc)\n",
    "        {\n",
    "            _train(classToDoc._1)\n",
    "            if( iter % 25 == 0){\n",
    "                println(s\"Current iteration: label #${iter}\")\n",
    "            }\n",
    "            iter += 1\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    // predicting a set of classes, given a set of tokens\n",
    "    def predict(tokens: Set[String], cut_num: Int = 7) : Set[String]= {\n",
    "        \n",
    "        var label_to_prob = MutMap[String, Double]()\n",
    "        \n",
    "        val documentFeatures = tokens.map(f => f -> _inverseFreq.getOrElse(f, 0.0)).filter(_._2 > 0.0).toMap\n",
    "        \n",
    "        for(classToDoc <- _classesToDoc) {\n",
    "            val label = classToDoc._1\n",
    "            val weights = label_to_weight(label)\n",
    "            // insert a (label -> probability) entry\n",
    "            label_to_prob(label) = getScore(documentFeatures,\n",
    "                                           weights)\n",
    "        }\n",
    "        \n",
    "        // keep labels with probability higher than threshold in a set\n",
    "        label_to_prob.filter({case (k,v) => v > 0}).  // remove labels below threshold\n",
    "            toSeq.sortWith(_._2 > _._2).take(cut_num).map(x => x._1).toSet  // keep cut_num labels with max probability\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def assess_files_svm(file_list: Array[File],\n",
    "                         threshold: Double = 0.95) = {\n",
    "    // calculate F score for all docs in validation set\n",
    "    var f1_score = Set[Double]()\n",
    "    var precision = Set[Double]()\n",
    "    var recall = Set[Double]()\n",
    "\n",
    "    for(path <- file_list) {\n",
    "\n",
    "        var iter = 1\n",
    "        val current_doc = new xml_doc(path.toString)\n",
    "\n",
    "        var cur_scores = assess(\n",
    "            Svm.predict(current_doc.token_stem.toSet),\n",
    "            current_doc.labels.toSet)\n",
    "\n",
    "        f1_score += cur_scores._1\n",
    "        precision += cur_scores._2\n",
    "        recall += cur_scores._3\n",
    "\n",
    "        if( iter % 25 == 0){\n",
    "            println(s\"Current iteration: #${iter}, working on file ${path.toString}\")\n",
    "        }\n",
    "        iter += 1\n",
    "    }\n",
    "    (f1_score.sum / f1_score.size,\n",
    "     precision.sum / precision.size,\n",
    "     recall.sum / recall.size)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// init map values\n",
    "Svm.set_class_to_doc(class_to_doc)\n",
    "Svm.set_documents_tf_map(documents_tf_map)\n",
    "Svm.set_inverseFreq(inverseFreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "var hyper_param_res = MutMap[Double, (Double, Double, Double)]()\n",
    "val hyper_param_list = List.tabulate(6)(x => Math.pow(10, -(x+1) )).tail\n",
    "val step_size_list = List.range(1,6).map(x => 1 / Math.sqrt(x))\n",
    "\n",
    "for (hyp_param <- hyper_param_list) {\n",
    "    // set lambda value\n",
    "    Svm.set_lambda(hyp_param)\n",
    "    println(s\"Lambda set to $hyp_param\")\n",
    "    println(\"New training cycle starting\")\n",
    "    \n",
    "    // reset weights before training\n",
    "    Svm.label_to_weight = \n",
    "        MutMap[String, collection.mutable.Map[String, Double]]()\n",
    "    \n",
    "    // train epochs\n",
    "    for(step <- step_size_list) {\n",
    "        Svm.set_step_size(step)\n",
    "        println(s\"Step size set to $step\")\n",
    "        println()\n",
    "        Svm.trainAll\n",
    "    }\n",
    "\n",
    "    hyper_param_res(hyp_param) = assess_files_svm(val_list)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write(hyper_param_res, \"svm_hyperparam.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "// train on expanded set and produce final prediction\n",
    "val expanded_train_list = train_list ++ val_list\n",
    "val svm_lambda = 0.01\n",
    "\n",
    "Svm.set_lambda(svm_lambda)\n",
    "// reset weights before training\n",
    "Svm.label_to_weight = \n",
    "    MutMap[String, collection.mutable.Map[String, Double]]()\n",
    "\n",
    "// train epochs\n",
    "for(step <- step_size_list) {\n",
    "    Svm.set_step_size(step)\n",
    "    println(s\"Step size set to $step\")\n",
    "    println()\n",
    "    Svm.trainAll\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// predict on test set\n",
    "var svm_test_predict = MutMap[Int, Set[String]]()\n",
    "val numPattern = \"[0-9]+\".r\n",
    "\n",
    "var iter = 1\n",
    "\n",
    "for (path <- test_list) {\n",
    "    var ID = numPattern.findFirstIn(path.toString).get.toInt\n",
    "    var current_doc = new xml_doc(path.toString).token_stem.toSet\n",
    "    \n",
    "    svm_test_predict(ID) = Svm.predict(current_doc)\n",
    "\n",
    "    if( iter % 100 == 0){\n",
    "        println(s\"Current iteration: #${iter}, predicting for file ${path.toString}\")\n",
    "    }\n",
    "    iter += 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// write predictions to file\n",
    "write_prediction(svm_test_predict, \"ir-2016-1-project-24-svm.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "/** Main class of Naive Bayes\n",
    "* Parameters: alpha - Laplace Smoothing Parameter\n",
    "              threshold - the threshold for the ratio of log probabilities for classification\n",
    "              filter_size - words with frequency less than the filter size for a given class are removed from the training set of that class\n",
    "              cut - maximum number of classes to take per document\n",
    "*/\n",
    "\n",
    "class NBayes(alpha: Double, threshold: Double, filter_size: Int, cut: Int){\n",
    "\n",
    "// Hyperparameters and intermediate representations (created and used)\n",
    "    val _alpha = alpha \n",
    "    val _threshold = threshold\n",
    "    val _cut = cut\n",
    "    val _filter_size = filter_size\n",
    "\n",
    "    val supermap = MutMap[String, (Map[String, Double], Double)]() // class -> term -> term.frequency map, size of all documents to that class\n",
    "    val prior = MutMap[String,Double]()\n",
    "    val c_inv = MutMap[String,Set[Int]]()\n",
    "    val prior_inv = MutMap[String,Double]()\n",
    "    val supermap_inv = MutMap[String, (Map[String, Double], Double)]() // class -> term -> term.frequency map, size of all documents to that class\n",
    "    var vocabulary_size = 0\n",
    "    var vocabulary = Set[String]()\n",
    "\n",
    "/** Function to train the classifier\n",
    "* Parameters: c - Map of classes to Set of Document IDs which have that class\n",
    "              d - Map of document ID to (map of (word to count),document size, set of cllass labels)\n",
    "  Creates a map, which is used to determine the prior probability values for each class and the conditional probabilities of the token given the class label\n",
    "*/\n",
    "    def train(c: scala.collection.Map[String,scala.collection.immutable.Set[Int]], d: scala.collection.Map[Int,(scala.collection.Map[String,Int], Int, Set[String])])\n",
    "    {\n",
    "        // ttfbyc is a map for each class from terms (across all documents in that class) to term-frequency in these documents\n",
    "        // cdocsize is the size of the documents labelled with that class\n",
    "        // supermap stores both these values\n",
    "        for (cl <- c)\n",
    "        {\n",
    "            prior += cl._1 -> Math.log((cl._2.size.toDouble / d.size))\n",
    "            val ttfbyc = c(cl._1).toList.flatMap(di => d(di)._1).groupBy(x=>x._1).mapValues(x=> x.map(x=>x._2).sum.toDouble).filter(_._2>_filter_size) \n",
    "            val cdocsize = ttfbyc.values.sum\n",
    "            supermap += cl._1 -> ((ttfbyc, cdocsize))\n",
    "            vocabulary = vocabulary ++ ttfbyc.keys.toSet\n",
    "        }   \n",
    "\n",
    "        // INVERSE TRAINING\n",
    "        // create the inverse class map, i.e. the mapping of each class to its complement document set (see report)\n",
    "        \n",
    "        val c_inv = c.mapValues(t => d.keys.toSet -- t)\n",
    "        println(\"created c_inv\")\n",
    "        for (cl <- c_inv)\n",
    "        {\n",
    "            prior_inv += cl._1 -> Math.log((cl._2.size.toDouble / d.size))\n",
    "            val ttfbyc = c_inv(cl._1).take(10000).toList.flatMap(di => d(di)._1).groupBy(x=>x._1).mapValues(x=> x.map(x=>x._2).sum.toDouble).filter(_._2>_filter_size) \n",
    "            val cdocsize = ttfbyc.values.sum\n",
    "            supermap_inv += cl._1 -> ((ttfbyc, cdocsize))\n",
    "            vocabulary = vocabulary ++ ttfbyc.keys.toSet\n",
    "        }   \n",
    "        vocabulary_size = vocabulary.size\n",
    "    }\n",
    "    \n",
    "/** Function to get the conditional probability of a token given the class\n",
    "* Parameters: term - the token\n",
    "              cl - the  class\n",
    "  Determines conditional probabilities of the token given the class label\n",
    "*/\n",
    "    \n",
    "        def getCondi(term: String, cl: String) =\n",
    "    {\n",
    "        // Using the supermap, we implement Laplace smoothing using hyperparameter provided\n",
    "        val sumTf = supermap(cl)._1.getOrElse(term,0.0) + alpha\n",
    "        val sumDocSize = supermap(cl)._2 + (alpha * vocabulary_size)\n",
    "        Math.log(sumTf / sumDocSize.toDouble)\n",
    "    }\n",
    "\n",
    "        \n",
    "        def getCondi_inv(term: String, cl: String) =\n",
    "    {\n",
    "        // the same is done for the complementary set of documents\n",
    "        val sumTf = supermap_inv(cl)._1.getOrElse(term,0.0) + alpha\n",
    "        val sumDocSize = supermap_inv(cl)._2 + (alpha * vocabulary_size)\n",
    "        Math.log(sumTf / sumDocSize.toDouble)\n",
    "    }\n",
    "\n",
    "\n",
    "/** Function to create a prediction\n",
    "* Parameters: c - Map of classes to Set of Document IDs which have that class\n",
    "              d - Map of document ID to (map of (word to count),document size, set of cllass labels)\n",
    "  \n",
    "*/\n",
    "\n",
    "    \n",
    "    // two functions to help with the prediction\n",
    "    // _getFreq just creates term --> termfq map for a given document to be classified\n",
    "    // _getLabels implements cut and threshold to produce a set of labels,\n",
    "    // after the predict function has worked its magic\n",
    "    \n",
    "    // IN: Bag of Words, OUT: Term -> Term-frequency\n",
    "    def _getFreq(doc: List[String]) = doc.groupBy(identity).mapValues(l => l.length).toMap\n",
    "    \n",
    "    // IN: Label -> Probability, OUT: Set of Labels as Prediction\n",
    "    def _getLabels(res: MutMap[String, Double], threshold: Double, cut: Int) =\n",
    "    {\n",
    "       res.mapValues(x => x).filter(_._2 >= threshold).toSeq.sortBy(-_._2).take(cut).map(_._1).toSet\n",
    "    }\n",
    "\n",
    "    \n",
    "    // The actual function called on a Bag of Words to produce a prediction\n",
    "     def predict(tokens: List[String]) =\n",
    "    {\n",
    "        val doc = _getFreq(tokens)\n",
    "        val terms = doc.keys\n",
    "        \n",
    "        var probs : MutMap[String, Double] = MutMap()\n",
    "        var probs_inv : MutMap[String, Double] = MutMap()\n",
    "\n",
    "\n",
    "        // for each class, this estimates the numerator\n",
    "        for(cl <- prior)\n",
    "            {\n",
    "            // gets the prior probability for that class\n",
    "            var prob = prior(cl._1)\n",
    "      \n",
    "            // for each term in my bag of words, get the probability of that term appearing in the document given class.\n",
    "            // add all the log-probabilities together\n",
    "            \n",
    "            for(term <- terms) prob += doc(term)*getCondi(term, cl._1)\n",
    "    \n",
    "                // add the log-probability to my final map for that\n",
    "                probs += cl._1 -> prob\n",
    "            }\n",
    "        \n",
    "        // for each class, this estimates the denominator\n",
    "        for(cl <- prior_inv)\n",
    "            {\n",
    "            // gets the prior probability for that class\n",
    "            var prob_inv = prior_inv(cl._1)\n",
    "      \n",
    "            // for each term in my bag of words, get theprobability of that term appearing in the document given class.\n",
    "            // add all the log-probabilitie together\n",
    "            \n",
    "            for(term <- terms) prob_inv += doc(term)*getCondi_inv(term, cl._1)\n",
    "    \n",
    "                // add the log-probability to my final map for tha\n",
    "                probs_inv += cl._1 -> prob_inv\n",
    "            }\n",
    "        \n",
    "        // this gives the estimated log-ratio of the probabilities for each class, is input to _getLabels\n",
    "        val probabil = probs.map(t => t._1 -> (t._2 - probs_inv(t._1)))\n",
    "        \n",
    "        //probabil\n",
    "        _getLabels(probabil, threshold, cut)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// initiate a new estimator, and train the new estimator.\n",
    "val estimator = new NBayes(0.5,5,1,5)\n",
    "// train the estimator\n",
    "estimator.train(class_to_doc,documents_tf_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// import bag of words to train classifier\n",
    "val bag = MutMap[Int,List[String]]()\n",
    "for (path <- test_list)\n",
    "{\n",
    "    var ID =  numPattern.findFirstIn(path.toString).get.toInt\n",
    "    val currentdoc = new xml_doc(path.toString)\n",
    "    val bowlist = currentdoc.token_stem.toList\n",
    "    bag(ID) = bowlist   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// compute predictions and write them to file\n",
    "val pred = bag.mapValues(x=>estimator.predict(x))\n",
    "write_prediction(pred,\"ir-2016-1-project-26-nb.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Use this cell to compute average_F1_scores on the validation set or any other set if you fancy\n",
    "val bag = MutMap[Int,List[String]]()\n",
    "val truth = MutMap[Int,Set[String]]()\n",
    "\n",
    "for (path <- val_list)\n",
    "{\n",
    "    var ID =  numPattern.findFirstIn(path.toString).get.toInt\n",
    "    val currentdoc = new xml_doc(path.toString)\n",
    "    val bowlist = currentdoc.token_stem.toList\n",
    "    val labels = currentdoc.labels.toSet\n",
    "    bag(ID) = bowlist   \n",
    "    truth(ID) = labels   \n",
    "}\n",
    "    \n",
    "val scores = bag.keySet.toList.map(x=>assess(estimator.predict(bag(x)),truth(x)))\n",
    "val f1_scores = scores.map(x=>x._1);\n",
    "val avg_f1_score = f1_scores.sum/f1_scores.size"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Scala 2.11",
   "language": "scala211",
   "name": "scala211"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
