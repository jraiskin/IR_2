{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classpath.addPath(\"tinyir-1.1.jar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36mscala.xml.XML\u001b[0m\n",
       "\u001b[32mimport \u001b[36mch.ethz.dal.tinyir._\u001b[0m\n",
       "\u001b[32mimport \u001b[36mcom.github.aztek.porterstemmer.PorterStemmer\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import scala.xml.XML\n",
    "import ch.ethz.dal.tinyir._\n",
    "import com.github.aztek.porterstemmer.PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36mjava.io._\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// import scala.io.Source  // for importing txt files\n",
    "import java.io._  // for saving txt files\n",
    "// import scala.collection.mutable.HashMap  //HashMap used for counting elements in linear time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36mscala.collection.mutable.{Map => MutMap}\u001b[0m\n",
       "\u001b[32mimport \u001b[36mscala.collection.mutable.ListBuffer\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// import scala.util.Random\n",
    "import scala.collection.mutable.{Map => MutMap}\n",
    "// enables \"mutable lists\"\n",
    "import scala.collection.mutable.ListBuffer  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass \u001b[36mxml_doc\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class xml_doc (file_path: String) {\n",
    "    def get_doc(): xml.Elem = {\n",
    "        XML.loadFile(file_path: String)\n",
    "    }    \n",
    "    \n",
    "    def text() = {\n",
    "        (get_doc() \\\\ \"DOC\" \\\\ \"TEXT\").text\n",
    "    }\n",
    "    \n",
    "    def head() = {\n",
    "        (get_doc() \\\\ \"DOC\" \\\\ \"HEAD\").text\n",
    "    }\n",
    "\n",
    "    def id() = {\n",
    "        (get_doc() \\\\ \"DOC\" \\\\ \"DOCNO\").text.trim\n",
    "    }\n",
    "    \n",
    "    // tokenizes the text, remove stop words and stems\n",
    "    // based on ch.ethz.dal.tinyir.processing\n",
    "    // and on com.github.aztek.porterstemmer\n",
    "    def token_stem() = {\n",
    "        processing.StopWords.filterOutSW(                    // filter out any token which is a StopWord\n",
    "        processing.Tokenizer.tokenize(head() ++ text())     // create tokens from headline and body\n",
    "    ).map(x => PorterStemmer.stem(x).                       // apply stemming\n",
    "          replaceAll(\"\\\\P{L}+\", \"\")).filter(_.trim.nonEmpty)  // pattern matching to keep only alphabet\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction \u001b[36mlist_docs\u001b[0m\n",
       "\u001b[36mnumPattern\u001b[0m: \u001b[32mscala\u001b[0m.\u001b[32mutil\u001b[0m.\u001b[32mmatching\u001b[0m.\u001b[32mRegex\u001b[0m = [0-9]+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def list_docs (path: String) = {  // : Array[java.io.File]\n",
    "        new java.io.File(path).listFiles.map(x => x.toString())\n",
    "    }\n",
    "val numPattern = \"[0-9]+\".r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction \u001b[36mwrite\u001b[0m\n",
       "defined \u001b[32mfunction \u001b[36mwrite_prediction\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def write(data: MutMap[Double, (Double, Double, Double)],filename: String) = {\n",
    "\n",
    "    val bw = new BufferedWriter(new FileWriter(new File(filename)))\n",
    "    val iter = data.keys.iterator\n",
    "    while(iter.hasNext){\n",
    "        var elem = iter.next()\n",
    "        var values = data(elem).productIterator.toList\n",
    "        if(values.length>0){\n",
    "            bw.write(elem+\" \"+values.mkString(\" \"))\n",
    "            bw.newLine\n",
    "        }    \n",
    "    }   \n",
    "    bw.close()\n",
    "}\n",
    "\n",
    "def write_prediction(data: MutMap[Int, Set[String]],filename: String) = {\n",
    "\n",
    "    val bw = new BufferedWriter(new FileWriter(new File(filename)))\n",
    "    val iter = data.keys.iterator\n",
    "    while(iter.hasNext){\n",
    "        var elem = iter.next()\n",
    "        var values = data(elem).toList\n",
    "        if(values.length>0){\n",
    "            bw.write(elem+\" \"+values.mkString(\" \"))\n",
    "            bw.newLine\n",
    "        }    \n",
    "    }   \n",
    "    bw.close()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mtrain_list\u001b[0m: \u001b[32mArray\u001b[0m[\u001b[32mString\u001b[0m] = \u001b[33mArray\u001b[0m(\n",
       "  \u001b[32m\"\"\"\n",
       "training\\AP880212-0006\n",
       "  \"\"\"\u001b[0m,\n",
       "  \u001b[32m\"\"\"\n",
       "training\\AP880212-0007\n",
       "  \"\"\"\u001b[0m,\n",
       "  \u001b[32m\"\"\"\n",
       "training\\AP880212-0009\n",
       "  \"\"\"\u001b[0m,\n",
       "  \u001b[32m\"\"\"\n",
       "training\\AP880212-0017\n",
       "  \"\"\"\u001b[0m,\n",
       "  \u001b[32m\"\"\"\n",
       "training\\AP880212-0018\n",
       "  \"\"\"\u001b[0m,\n",
       "  \u001b[32m\"\"\"\n",
       "training\\AP880212-0022\n",
       "  \"\"\"\u001b[0m,\n",
       "  \u001b[32m\"\"\"\n",
       "\u001b[33m...\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val train_list = list_docs(\"training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdoc_list\u001b[0m: \u001b[32mListBuffer\u001b[0m[\u001b[32mprocessing\u001b[0m.\u001b[32mStringDocument\u001b[0m] = \u001b[33mListBuffer\u001b[0m(\n",
       "  ch.ethz.dal.tinyir.processing.StringDocument@2822a04b,\n",
       "  ch.ethz.dal.tinyir.processing.StringDocument@38048203,\n",
       "  ch.ethz.dal.tinyir.processing.StringDocument@1ec3176e\n",
       ")\n",
       "\u001b[36mcounter\u001b[0m: \u001b[32mInt\u001b[0m = \u001b[32m3\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val doc_list = new ListBuffer[processing.StringDocument]()\n",
    "var counter = 0\n",
    "for (path <- train_list.take(3)){\n",
    "    counter += 1\n",
    "    var cur_doc = new xml_doc(path)\n",
    "    doc_list += (new processing.StringDocument(counter, cur_doc.token_stem.mkString(\" \")))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdoc_stream\u001b[0m: \u001b[32mStream\u001b[0m[\u001b[32mprocessing\u001b[0m.\u001b[32mStringDocument\u001b[0m] = \u001b[33mStream\u001b[0m(\n",
       "  ch.ethz.dal.tinyir.processing.StringDocument@2822a04b,\n",
       "  ch.ethz.dal.tinyir.processing.StringDocument@38048203,\n",
       "  ch.ethz.dal.tinyir.processing.StringDocument@1ec3176e\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val doc_stream = doc_list.toStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mtest_pos_index\u001b[0m: \u001b[32mindexing\u001b[0m.\u001b[32mPosIndex\u001b[0m = ch.ethz.dal.tinyir.indexing.PosIndex@1fdd1d78"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val test_pos_index = new indexing.PosIndex(doc_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres72\u001b[0m: \u001b[32mMap\u001b[0m[\u001b[32mString\u001b[0m, \u001b[32mtest_pos_index\u001b[0m.\u001b[32mPostList\u001b[0m] = \u001b[33mMap\u001b[0m(\n",
       "  \u001b[32m\"atcoffe\"\u001b[0m -> \u001b[33mList\u001b[0m(\u001b[33mPosPosting\u001b[0m(\u001b[32m2\u001b[0m, \u001b[32m112\u001b[0m)),\n",
       "  \u001b[32m\"eggproduc\"\u001b[0m -> \u001b[33mList\u001b[0m(\u001b[33mPosPosting\u001b[0m(\u001b[32m1\u001b[0m, \u001b[32m8\u001b[0m)),\n",
       "  \u001b[32m\"inher\"\u001b[0m -> \u001b[33mList\u001b[0m(\u001b[33mPosPosting\u001b[0m(\u001b[32m2\u001b[0m, \u001b[32m245\u001b[0m)),\n",
       "  \u001b[32m\"polic\"\u001b[0m -> \u001b[33mList\u001b[0m(\u001b[33mPosPosting\u001b[0m(\u001b[32m3\u001b[0m, \u001b[32m35\u001b[0m), \u001b[33mPosPosting\u001b[0m(\u001b[32m3\u001b[0m, \u001b[32m208\u001b[0m)),\n",
       "  \u001b[32m\"outlaw\"\u001b[0m -> \u001b[33mList\u001b[0m(\u001b[33mPosPosting\u001b[0m(\u001b[32m3\u001b[0m, \u001b[32m233\u001b[0m)),\n",
       "  \u001b[32m\"chief\"\u001b[0m -> \u001b[33mList\u001b[0m(\u001b[33mPosPosting\u001b[0m(\u001b[32m2\u001b[0m, \u001b[32m132\u001b[0m)),\n",
       "  \u001b[32m\"atough\"\u001b[0m -> \u001b[33mList\u001b[0m(\u001b[33mPosPosting\u001b[0m(\u001b[32m2\u001b[0m, \u001b[32m223\u001b[0m)),\n",
       "  \u001b[32m\"for\"\u001b[0m -> \u001b[33mList\u001b[0m(\u001b[33mPosPosting\u001b[0m(\u001b[32m2\u001b[0m, \u001b[32m251\u001b[0m)),\n",
       "  \u001b[32m\"internation\"\u001b[0m -> \u001b[33mList\u001b[0m(\u001b[33mPosPosting\u001b[0m(\u001b[32m3\u001b[0m, \u001b[32m125\u001b[0m)),\n",
       "  \u001b[32m\"school\"\u001b[0m -> \u001b[33mList\u001b[0m(\u001b[33mPosPosting\u001b[0m(\u001b[32m2\u001b[0m, \u001b[32m115\u001b[0m)),\n",
       "  \u001b[32m\"soldier\"\u001b[0m -> \u001b[33mList\u001b[0m(\n",
       "    \u001b[33mPosPosting\u001b[0m(\u001b[32m3\u001b[0m, \u001b[32m0\u001b[0m),\n",
       "    \u001b[33mPosPosting\u001b[0m(\u001b[32m3\u001b[0m, \u001b[32m6\u001b[0m),\n",
       "    \u001b[33mPosPosting\u001b[0m(\u001b[32m3\u001b[0m, \u001b[32m19\u001b[0m),\n",
       "    \u001b[33mPosPosting\u001b[0m(\u001b[32m3\u001b[0m, \u001b[32m34\u001b[0m),\n",
       "    \u001b[33mPosPosting\u001b[0m(\u001b[32m3\u001b[0m, \u001b[32m42\u001b[0m),\n",
       "    \u001b[33mPosPosting\u001b[0m(\u001b[32m3\u001b[0m, \u001b[32m72\u001b[0m),\n",
       "    \u001b[33mPosPosting\u001b[0m(\u001b[32m3\u001b[0m, \u001b[32m207\u001b[0m)\n",
       "  ),\n",
       "\u001b[33m...\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_pos_index.index\n",
    "// test_pos_index.postings(doc_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres60\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"soldier arrest coup attempt tribal homeland soldier beenarrest face death sentenc stem coupattempt bophuthatswana leader tribal homeland saidfridai rebel soldier stage takeov bid wednesdai detaininghomeland presid luca mangop top cabinet officialsfor hour south african soldier polic rush thehomeland rescu leader restor power soldier civilian di upris bophuthatswana minist justic godfrei mothib told anew confer arrest charg hightreason convict sentenc death theaccus court mondai arrest coup attempt asyoung troop senior warrant offic coup rebel soldier instal head state rockymalebanemets leader opposit progress peoplesparti malebanemets escap captur whereabout remainedunknown offici unsubstanti report hefl nearbi botswana warrant offic phiri mangop thecoup leader arrest fridai mmabatho capit thenomin independ homeland offici bophuthatswana popul million spreadov separ land block tribal homeland insouth africa half south africa million black livein homeland recogn internation henni riekert homeland defens minist southafrican troop remain bophuthatswana becomea perman presenc bophuthatswana foreign minist solomon ratheb defend southafrica intervent the fact south african govern invit assist drama peculiar tobophuthatswana ratheb but south africa mightask countri bophuthatswanaenjoi diplomat relat formal agreem mangop mutual defens treati homelandand south africa similar nato agreem refer toth atlant militari allianc elabor ask coup mangop we grantedpeopl freedom extent plan thing likethi upris began wednesdai rebel soldierstook mangop top minist home nationalsport stadium wedn\u001b[33m...\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc_stream(2).body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def HashMap_get_count(w: Seq[String]): scala.collection.immutable.Map[String, Int] = {\n",
    "    // map(token -> count)\n",
    "    // auxilary function to find duplicate elements\n",
    "    // and return their counts in a Seq\n",
    "        val map = scala.collection.immutable.HashMap[String, Int]().withDefaultValue(0)\n",
    "        w.foldLeft(map)((m, c) => m + (c -> (m(c) + 1)) )\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pruned_vocab(total_vocab: scala.collection.mutable.Map[String, Int], min_count: Int)={\n",
    "    val temp = total_vocab.clone\n",
    "    temp.retain((k,v) => v > min_count)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def total_vocab(file_list: Array[File])={\n",
    "    val docwords = MutMap[String, Int]()\n",
    "    for(iter<-0 to file_list.length - 1) {\n",
    "        var doc = new xml_doc(file_list(iter).toString)\n",
    "        var tokens = doc.token_stem\n",
    "        for(t <- tokens){\n",
    "            var temp = docwords.getOrElseUpdate(t, 0)  \n",
    "            docwords.update(t,temp+1)\n",
    "        }\n",
    "    }\n",
    "    docwords\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def assess(retrievedTopics: Set[String], expecedTopics: Set[String]) = {\n",
    "        val truePositive = (retrievedTopics & expecedTopics).size\n",
    "        val falsePositive = (retrievedTopics -- expecedTopics).size\n",
    "        val falseNegative = (expecedTopics -- retrievedTopics).size\n",
    "\n",
    "    val precision = truePositive.toDouble / (truePositive + falsePositive)\n",
    "    val recall = truePositive.toDouble / (truePositive + falseNegative)\n",
    "    var f1Score = 2 * (precision * recall) / (precision + recall)\n",
    "    f1Score = if (f1Score.isNaN) {0} else {f1Score}\n",
    "    \n",
    "    (f1Score, precision, recall)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val train_list = list_xml_files(\"data/train\")\n",
    "val val_list = list_xml_files(\"data/validation\")\n",
    "val test_list = list_xml_files(\"data/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document maps creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_doc_maps(file_list: Array[File],\n",
    "               iter_start: Int = 0,\n",
    "               iter_end: Int, pruned_vocab_set: collection.Set[String]) = {\n",
    "    \n",
    "    //document index -> ((term -> tfs), size, topics)\n",
    "    val _documents = MutMap[Int, \n",
    "                            (scala.collection.Map[String, Int], \n",
    "                             Int, Set[String])]()\n",
    "    \n",
    "    //class name -> document indexes\n",
    "    val _classesToDoc = MutMap[String, Set[Int]]()\n",
    "    \n",
    "//     var iter = iter_start\n",
    "    \n",
    "    for(iter <- iter_start to iter_end - 1){\n",
    "        \n",
    "        // get tokens\n",
    "        var path = file_list(iter).toString\n",
    "        var cur_doc = new xml_doc(path)\n",
    "        var tokens = cur_doc.token_stem\n",
    "        \n",
    "        var pruned_tokens = tokens.filter(token=>pruned_vocab_set(token))\n",
    "        \n",
    "        // get labels\n",
    "        var labels = cur_doc.labels.toSet\n",
    "        \n",
    "        var ID =  cur_doc.id\n",
    "\n",
    "        _documents += ID -> ((HashMap_get_count(pruned_tokens), pruned_tokens.length, labels))\n",
    "    \n",
    "        for(c <- labels){\n",
    "            val cl = _classesToDoc.getOrElseUpdate(c, Set[Int]())  \n",
    "            _classesToDoc.update(c, cl + ID)\n",
    "        }\n",
    "        \n",
    "        if( (iter + 1) % 200 == 0){\n",
    "            println(s\"Current iteration: ${iter + 1}\")\n",
    "        }\n",
    "    }\n",
    "(_documents, _classesToDoc)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val vocab = total_vocab(file_list)\n",
    "val pruned_vocab = get_pruned_vocab(vocab,1)//prune vocabulary to remove tokens with frequency 1\n",
    "val documents = get_doc_maps(train_list, iter_end = train_list.length, pruned_vocab_set=pruned_vocab.keys.toSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "//document index -> ((term -> tfs), size, topics)\n",
    "val documents_tf_map = documents._1\n",
    "\n",
    "//class name -> document indexes\n",
    "val class_to_doc = documents._2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "//term -> tf-idf\n",
    "def get_inverseFreq(\n",
    "    documents_tf_map: MutMap[Int, \n",
    "                            (Map[String, Int], \n",
    "                             Int, Set[String])]) = {\n",
    "    \n",
    "    var inverseFreq = MutMap[String, Double]()\n",
    "    \n",
    "    var iter = 0\n",
    "    for(d <- documents_tf_map.map(_._2._1)) {\n",
    "        for(t <- d) {\n",
    "            val v = inverseFreq.getOrElse(t._1, -1.0)\n",
    "\n",
    "            if(v >= 0) inverseFreq.update(t._1, t._2 + v) else inverseFreq += t._1 -> t._2\n",
    "        }\n",
    "        if( (iter + 1) % 50 == 0){\n",
    "            println(s\"Current iteration: ${iter + 1}\")\n",
    "            }\n",
    "        iter += 1\n",
    "    }\n",
    "    inverseFreq = inverseFreq.map(f => f._1 -> (Math.log(iter) - Math.log(f._2)))\n",
    "    inverseFreq\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val inverseFreq = get_inverseFreq(documents_tf_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "// helper functions\n",
    "\n",
    "def sigmoid(theta: MutMap[String, Double], documentFeatures: Map[String, Double]) = {\n",
    "    1.0 / (1.0 + Math.exp(-dot_product(documentFeatures, theta)))\n",
    "}\n",
    "\n",
    "def dot_product(vector1: Map[String, Double], vector2: MutMap[String, Double]) = {\n",
    "    vector1.map(v => v._2 * vector2.getOrElse(v._1, 0.0)).sum\n",
    "}\n",
    "\n",
    "def scalar_product(vector: Map[String, Double], scalar: Double) = {\n",
    "    vector.mapValues(v => v * scalar)\n",
    "}\n",
    "\n",
    "def scalar_product_mut(vector: MutMap[String, Double], scalar: Double) = {\n",
    "    vector.mapValues(v => v * scalar)\n",
    "}\n",
    "\n",
    "def add_map(vector1: Map[String, Double], vector2: MutMap[String, Double]) = {\n",
    "    vector1.map(v => v._1 -> (vector2.getOrElse(v._1, 0.0) + v._2))\n",
    "}\n",
    "\n",
    "def add_map_immut(vector1: Map[String, Double], vector2: scala.collection.Map[String,Double]) = {\n",
    "    vector1.map(v => v._1 -> (vector2.getOrElse(v._1, 0.0) + v._2))\n",
    "}\n",
    "\n",
    "def takeRandomN[A](n: Int, org_list: Set[A]) ={\n",
    "  scala.util.Random.shuffle(org_list).take(n)\n",
    "}\n",
    "\n",
    "def sum_square (some_map: MutMap[String, Double]) = {\n",
    "    some_map.values.foldLeft(0.0)(_ + Math.pow(_, 2))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "object Logistic_reg {\n",
    "    \n",
    "    // init placeholder vars\n",
    "    var _classesToDoc = MutMap[String, Set[Int]]() //class name -> document indexes\n",
    "    \n",
    "    var _documents = MutMap[Int, (Map[String, Int], Int, Set[String])]() //document index -> ((term -> tfs), size, topics)\n",
    "    \n",
    "    var _inverseFreq = MutMap[String, Double]()  //term -> tf-idf\n",
    "    \n",
    "    var corp_size = 0\n",
    "    \n",
    "    var label_to_weight = MutMap[String, MutMap[String, Double]]()  // label -> (token -> weight)\n",
    "    \n",
    "    var lambda = 0.0  // regularization constant\n",
    "    \n",
    "    var step_size = 1.0  // for weights update\n",
    "    \n",
    "    // set placeholder variables\n",
    "    def set_class_to_doc(value: MutMap[String, Set[Int]]) = {\n",
    "        _classesToDoc = value\n",
    "    }\n",
    "    \n",
    "    def set_documents_tf_map(value: MutMap[Int, (Map[String, Int], Int, Set[String])]) = {\n",
    "        _documents = value\n",
    "        corp_size = value.size\n",
    "    }\n",
    "    \n",
    "    def set_inverseFreq(value: MutMap[String, Double]) = {\n",
    "        _inverseFreq = value\n",
    "    }\n",
    "    \n",
    "    def set_lambda(value: Double) = {\n",
    "        lambda = value\n",
    "    }\n",
    "    \n",
    "    def set_step_size(value: Double) = {\n",
    "        step_size = value\n",
    "    }\n",
    "    \n",
    "    // generate prediction\n",
    "    def getProb(documentFeatures: Map[String, Double],\n",
    "               theta: MutMap[String, Double]) = {\n",
    "        sigmoid(theta, documentFeatures)\n",
    "    }\n",
    "    \n",
    "    // gradient calculates for a SINGLE LABEL\n",
    "    // update weights\n",
    "    def gradient(theta: MutMap[String, Double], \n",
    "                  documentFeatures: Map[String, Double], \n",
    "                  y: Boolean) : Map[String, Double] = {\n",
    "        val loss_contrib = if(y) 1 - sigmoid(theta, documentFeatures) else -sigmoid(theta, documentFeatures)\n",
    "        val grad = scalar_product(documentFeatures, loss_contrib * step_size)\n",
    "        val reg_shrink = scalar_product_mut(theta, -lambda * step_size)\n",
    "        \n",
    "        add_map(\n",
    "            add_map_immut(grad, reg_shrink)\n",
    "            ,theta)\n",
    "    }\n",
    "    \n",
    "    // train on a single training example (doc)\n",
    "    def train_on_label(label: String,\n",
    "                       documentFeatures: Map[String, Double], \n",
    "                       y: Boolean) =  {\n",
    "        \n",
    "        if (!(label_to_weight contains label)){\n",
    "            label_to_weight.update(label, MutMap[String, Double]().withDefaultValue(0.5))\n",
    "        }\n",
    "        label_to_weight(label) ++= gradient(label_to_weight(label), \n",
    "                                             documentFeatures, y)\n",
    "    }\n",
    "    \n",
    "    // train on a signle \"topic\" with a set of docs\n",
    "    def _train(label: String) = {\n",
    "        // train on a set of docs (identified by docIndex)\n",
    "        \n",
    "        val doc_collection = takeRandomN(800, RandomDocuments(label))\n",
    "        \n",
    "        for(docIndex <- doc_collection) {\n",
    "\n",
    "            val doc = _documents(docIndex)\n",
    "            val y = doc._3.contains(label)\n",
    "            train_on_label(label, doc._1.map(f => f._1 -> _inverseFreq(f._1)), y)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def trainAll = {\n",
    "        var iter = 1\n",
    "        for(classToDoc <- _classesToDoc) {\n",
    "            _train(classToDoc._1)\n",
    "            if(iter % 25 == 0){\n",
    "                println(s\"Current iteration: label #${iter}\")\n",
    "            }\n",
    "            iter += 1\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def RandomDocuments(trueTopic: String) = {\n",
    "        val random = new Random\n",
    "        var documents = _classesToDoc(trueTopic)\n",
    "\n",
    "        var sample_size = documents.size * 3\n",
    "        sample_size = if(sample_size > corp_size) {\n",
    "            corp_size} else {sample_size}\n",
    "\n",
    "        documents = documents ++ takeRandomN(sample_size, _documents.keys.toSet)\n",
    "\n",
    "        random.shuffle(documents)\n",
    "    }\n",
    "    \n",
    "    // predicting a set of classes, given a list of tokens\n",
    "    def predict(tokens: Set[String], threshold: Double, cut_num: Int = 7) : Set[String]= {\n",
    "        \n",
    "        var label_to_prob = MutMap[String, Double]()\n",
    "        \n",
    "        val documentFeatures = tokens.map(f => f -> _inverseFreq.getOrElse(f, 0.0)).filter(_._2 > 0.0).toMap\n",
    "        \n",
    "        for(classToDoc <- _classesToDoc) {\n",
    "            val label = classToDoc._1\n",
    "            val weights = label_to_weight(label)\n",
    "            // insert a (label -> probability) entry\n",
    "            label_to_prob(label) = getProb(documentFeatures,\n",
    "                                           weights)\n",
    "        }\n",
    "        \n",
    "        // keep labels with probability higher than threshold in a set\n",
    "        label_to_prob.filter({case (k,v) => v > threshold}).  // remove labels below threshold\n",
    "            toSeq.sortWith(_._2 > _._2).take(cut_num).map(x => x._1).toSet  // keep cut_num labels with max probability\n",
    "        \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def assess_files_logistic(file_list: Array[File],\n",
    "                         threshold: Double = 0.95) = {\n",
    "    // calculate F score for all docs in validation set\n",
    "    var f1_score = Set[Double]()\n",
    "    var precision = Set[Double]()\n",
    "    var recall = Set[Double]()\n",
    "\n",
    "    for(path <- file_list) {\n",
    "\n",
    "        var iter = 1\n",
    "        val current_doc = new xml_doc(path.toString)\n",
    "\n",
    "        var cur_scores = assess(\n",
    "            Logistic_reg.predict(current_doc.token_stem.toSet, threshold),\n",
    "            current_doc.labels.toSet)\n",
    "\n",
    "        f1_score += cur_scores._1\n",
    "        precision += cur_scores._2\n",
    "        recall += cur_scores._3\n",
    "\n",
    "        if( iter % 25 == 0){\n",
    "            println(s\"Current iteration: #${iter}, working on file ${path.toString}\")\n",
    "        }\n",
    "        iter += 1\n",
    "    }\n",
    "    (f1_score.sum / f1_score.size,\n",
    "     precision.sum / precision.size,\n",
    "     recall.sum / recall.size)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "// init map values\n",
    "Logistic_reg.set_class_to_doc(class_to_doc)\n",
    "Logistic_reg.set_documents_tf_map(documents_tf_map)\n",
    "Logistic_reg.set_inverseFreq(inverseFreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "var hyper_param_res = MutMap[Double, (Double, Double, Double)]()\n",
    "val hyper_param_list = List.tabulate(6)(x => Math.pow(10, -(x+1) )).tail\n",
    "val step_size_list = List.range(1,6).map(x => 1 / Math.sqrt(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for (hyp_param <- hyper_param_list) {\n",
    "    // set lambda value\n",
    "    Logistic_reg.set_lambda(hyp_param)\n",
    "    println(s\"Lambda set to $hyp_param\")\n",
    "    println(\"New training cycle starting\")\n",
    "    \n",
    "    // reset weights before training\n",
    "    Logistic_reg.label_to_weight = \n",
    "        MutMap[String, collection.mutable.Map[String, Double]]()\n",
    "    \n",
    "    // train epochs\n",
    "    for(step <- step_size_list) {\n",
    "        Logistic_reg.set_step_size(step)\n",
    "        println(s\"Step size set to $step\")\n",
    "        println()\n",
    "        Logistic_reg.trainAll\n",
    "    }\n",
    "\n",
    "    hyper_param_res(hyp_param) = assess_files_logistic(val_list)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write(hyper_param_res, \"logistic_reg_hyperparam.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "// train on expanded set and produce final prediction\n",
    "val expanded_train_list = train_list ++ val_list\n",
    "val log_reg_lambda = 0.001\n",
    "\n",
    "Logistic_reg.set_lambda(log_reg_lambda)\n",
    "// reset weights before training\n",
    "Logistic_reg.label_to_weight = \n",
    "    MutMap[String, collection.mutable.Map[String, Double]]()\n",
    "\n",
    "// train epochs\n",
    "for(step <- step_size_list) {\n",
    "    Logistic_reg.set_step_size(step)\n",
    "    println(s\"Step size set to $step\")\n",
    "    println()\n",
    "    Logistic_reg.trainAll\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "// predict on test set\n",
    "var log_reg_test_predict = MutMap[Int, Set[String]]()\n",
    "val numPattern = \"[0-9]+\".r\n",
    "\n",
    "var iter = 1\n",
    "\n",
    "for (path <- test_list) {\n",
    "    var ID = numPattern.findFirstIn(path.toString).get.toInt\n",
    "    var current_doc = new xml_doc(path.toString).token_stem.toSet\n",
    "    \n",
    "    log_reg_test_predict(ID) = Logistic_reg.predict(current_doc, 0.95)\n",
    "\n",
    "    if( iter % 100 == 0){\n",
    "        println(s\"Current iteration: #${iter}, predicting for file ${path.toString}\")\n",
    "    }\n",
    "    iter += 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// write predictions to file\n",
    "write_prediction(log_reg_test_predict, \"ir-2016-1-project-24-lr.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "object Svm {\n",
    "    // init placeholder vars\n",
    "    var _classesToDoc = MutMap[String, Set[Int]]() //class name -> document indexes\n",
    "    \n",
    "    var _documents = MutMap[Int, (Map[String, Int], Int, Set[String])]() //document index -> ((term -> tfs), size, topics)\n",
    "    \n",
    "    var _inverseFreq = MutMap[String, Double]()  //term -> tf-idf\n",
    "    \n",
    "    var corp_size = 0\n",
    "    \n",
    "    var label_to_weight = MutMap[String, MutMap[String, Double]]()  // label -> (token -> weight)\n",
    "    \n",
    "    var lambda = 0.1  // needed for projection, |theta|_2 <= 1/sqrt(lambda)\n",
    "    \n",
    "    var step_size = 1.0  // for weights update\n",
    "    \n",
    "    // set placeholder variables\n",
    "    def set_class_to_doc(value: MutMap[String, Set[Int]]) = {\n",
    "        _classesToDoc = value\n",
    "    }\n",
    "    \n",
    "    def set_documents_tf_map(value: MutMap[Int, (Map[String, Int], Int, Set[String])]) = {\n",
    "        _documents = value\n",
    "        corp_size = value.size\n",
    "    }\n",
    "    \n",
    "    def set_inverseFreq(value: MutMap[String, Double]) = {\n",
    "        _inverseFreq = value\n",
    "    }\n",
    "    \n",
    "    def set_lambda(value: Double) = {\n",
    "        lambda = value\n",
    "    }\n",
    "    \n",
    "    def set_step_size(value: Double) = {\n",
    "        step_size = value\n",
    "    }\n",
    "    \n",
    "    // generate prediction score (dot product)\n",
    "    def getScore(documentFeatures: Map[String, Double],\n",
    "               theta: MutMap[String, Double]) = {\n",
    "        dot_product(documentFeatures, theta)\n",
    "    }\n",
    "    \n",
    "    // gradient calculates for a SINGLE LABEL\n",
    "    // update weights\n",
    "    def gradient(theta: MutMap[String, Double], \n",
    "                  documentFeatures: Map[String, Double], \n",
    "                  y: Boolean) = {\n",
    "        \n",
    "        val y_label = if(y) 1 else -1\n",
    "        \n",
    "        val theta_regular = scalar_product_mut(theta, 1 - step_size * lambda)\n",
    "        \n",
    "        val hinge = 1.0 - y_label * dot_product(documentFeatures, theta)\n",
    "        \n",
    "        if(hinge <= 0) { theta_regular }\n",
    "        else {\n",
    "            val projection = Math.min(\n",
    "                1 , (1.0 / Math.sqrt(lambda + sum_square(theta)) )\n",
    "            )\n",
    "            scalar_product(add_map_immut(\n",
    "                scalar_product(documentFeatures, step_size * y_label),\n",
    "                theta_regular), \n",
    "                                  projection)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    // train on a single training example (doc)\n",
    "    def train_on_label(label: String,\n",
    "                       documentFeatures: Map[String, Double], \n",
    "                       y: Boolean) =  {\n",
    "        \n",
    "        if (!(label_to_weight contains label)){\n",
    "            label_to_weight.update(label, MutMap[String, Double]().withDefaultValue(0.5))\n",
    "        }\n",
    "        \n",
    "        label_to_weight(label) ++= gradient(label_to_weight(label), \n",
    "                                             documentFeatures, y)\n",
    "    }\n",
    "    \n",
    "    // train on a single \"topic\" with a set of docs\n",
    "    def _train(label: String) = {\n",
    "        // train on a set of docs (identified by docIndex)\n",
    "        val doc_collection = takeRandomN(800, RandomDocuments(label))\n",
    "\n",
    "        for(docIndex <- doc_collection) {\n",
    "\n",
    "            val doc = _documents(docIndex)\n",
    "            val y = doc._3.contains(label)\n",
    "            \n",
    "            train_on_label(label, doc._1.map(f => f._1 -> _inverseFreq(f._1)), y)\n",
    "\n",
    "        }\n",
    "    }\n",
    "\n",
    "    def RandomDocuments(trueTopic: String) = {\n",
    "        val random = new Random\n",
    "        var documents = _classesToDoc(trueTopic)\n",
    "\n",
    "        var sample_size = documents.size * 3\n",
    "        sample_size = if(sample_size > corp_size) {\n",
    "            corp_size} else {sample_size}\n",
    "\n",
    "        documents = documents ++ takeRandomN(sample_size, _documents.keys.toSet)\n",
    "\n",
    "        random.shuffle(documents)\n",
    "    }\n",
    "    \n",
    "    def trainAll = {\n",
    "        var iter = 1\n",
    "        \n",
    "        for(classToDoc <- _classesToDoc)\n",
    "        {\n",
    "            _train(classToDoc._1)\n",
    "            if( iter % 25 == 0){\n",
    "                println(s\"Current iteration: label #${iter}\")\n",
    "            }\n",
    "            iter += 1\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    // predicting a set of classes, given a set of tokens\n",
    "    def predict(tokens: Set[String], cut_num: Int = 7) : Set[String]= {\n",
    "        \n",
    "        var label_to_prob = MutMap[String, Double]()\n",
    "        \n",
    "        val documentFeatures = tokens.map(f => f -> _inverseFreq.getOrElse(f, 0.0)).filter(_._2 > 0.0).toMap\n",
    "        \n",
    "        for(classToDoc <- _classesToDoc) {\n",
    "            val label = classToDoc._1\n",
    "            val weights = label_to_weight(label)\n",
    "            // insert a (label -> probability) entry\n",
    "            label_to_prob(label) = getScore(documentFeatures,\n",
    "                                           weights)\n",
    "        }\n",
    "        \n",
    "        // keep labels with probability higher than threshold in a set\n",
    "        label_to_prob.filter({case (k,v) => v > 0}).  // remove labels below threshold\n",
    "            toSeq.sortWith(_._2 > _._2).take(cut_num).map(x => x._1).toSet  // keep cut_num labels with max probability\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def assess_files_svm(file_list: Array[File],\n",
    "                         threshold: Double = 0.95) = {\n",
    "    // calculate F score for all docs in validation set\n",
    "    var f1_score = Set[Double]()\n",
    "    var precision = Set[Double]()\n",
    "    var recall = Set[Double]()\n",
    "\n",
    "    for(path <- file_list) {\n",
    "\n",
    "        var iter = 1\n",
    "        val current_doc = new xml_doc(path.toString)\n",
    "\n",
    "        var cur_scores = assess(\n",
    "            Svm.predict(current_doc.token_stem.toSet),\n",
    "            current_doc.labels.toSet)\n",
    "\n",
    "        f1_score += cur_scores._1\n",
    "        precision += cur_scores._2\n",
    "        recall += cur_scores._3\n",
    "\n",
    "        if( iter % 25 == 0){\n",
    "            println(s\"Current iteration: #${iter}, working on file ${path.toString}\")\n",
    "        }\n",
    "        iter += 1\n",
    "    }\n",
    "    (f1_score.sum / f1_score.size,\n",
    "     precision.sum / precision.size,\n",
    "     recall.sum / recall.size)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// init map values\n",
    "Svm.set_class_to_doc(class_to_doc)\n",
    "Svm.set_documents_tf_map(documents_tf_map)\n",
    "Svm.set_inverseFreq(inverseFreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "var hyper_param_res = MutMap[Double, (Double, Double, Double)]()\n",
    "val hyper_param_list = List.tabulate(6)(x => Math.pow(10, -(x+1) )).tail\n",
    "val step_size_list = List.range(1,6).map(x => 1 / Math.sqrt(x))\n",
    "\n",
    "for (hyp_param <- hyper_param_list) {\n",
    "    // set lambda value\n",
    "    Svm.set_lambda(hyp_param)\n",
    "    println(s\"Lambda set to $hyp_param\")\n",
    "    println(\"New training cycle starting\")\n",
    "    \n",
    "    // reset weights before training\n",
    "    Svm.label_to_weight = \n",
    "        MutMap[String, collection.mutable.Map[String, Double]]()\n",
    "    \n",
    "    // train epochs\n",
    "    for(step <- step_size_list) {\n",
    "        Svm.set_step_size(step)\n",
    "        println(s\"Step size set to $step\")\n",
    "        println()\n",
    "        Svm.trainAll\n",
    "    }\n",
    "\n",
    "    hyper_param_res(hyp_param) = assess_files_svm(val_list)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write(hyper_param_res, \"svm_hyperparam.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "// train on expanded set and produce final prediction\n",
    "val expanded_train_list = train_list ++ val_list\n",
    "val svm_lambda = 0.01\n",
    "\n",
    "Svm.set_lambda(svm_lambda)\n",
    "// reset weights before training\n",
    "Svm.label_to_weight = \n",
    "    MutMap[String, collection.mutable.Map[String, Double]]()\n",
    "\n",
    "// train epochs\n",
    "for(step <- step_size_list) {\n",
    "    Svm.set_step_size(step)\n",
    "    println(s\"Step size set to $step\")\n",
    "    println()\n",
    "    Svm.trainAll\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// predict on test set\n",
    "var svm_test_predict = MutMap[Int, Set[String]]()\n",
    "val numPattern = \"[0-9]+\".r\n",
    "\n",
    "var iter = 1\n",
    "\n",
    "for (path <- test_list) {\n",
    "    var ID = numPattern.findFirstIn(path.toString).get.toInt\n",
    "    var current_doc = new xml_doc(path.toString).token_stem.toSet\n",
    "    \n",
    "    svm_test_predict(ID) = Svm.predict(current_doc)\n",
    "\n",
    "    if( iter % 100 == 0){\n",
    "        println(s\"Current iteration: #${iter}, predicting for file ${path.toString}\")\n",
    "    }\n",
    "    iter += 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// write predictions to file\n",
    "write_prediction(svm_test_predict, \"ir-2016-1-project-24-svm.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "/** Main class of Naive Bayes\n",
    "* Parameters: alpha - Laplace Smoothing Parameter\n",
    "              threshold - the threshold for the ratio of log probabilities for classification\n",
    "              filter_size - words with frequency less than the filter size for a given class are removed from the training set of that class\n",
    "              cut - maximum number of classes to take per document\n",
    "*/\n",
    "\n",
    "class NBayes(alpha: Double, threshold: Double, filter_size: Int, cut: Int){\n",
    "\n",
    "// Hyperparameters and intermediate representations (created and used)\n",
    "    val _alpha = alpha \n",
    "    val _threshold = threshold\n",
    "    val _cut = cut\n",
    "    val _filter_size = filter_size\n",
    "\n",
    "    val supermap = MutMap[String, (Map[String, Double], Double)]() // class -> term -> term.frequency map, size of all documents to that class\n",
    "    val prior = MutMap[String,Double]()\n",
    "    val c_inv = MutMap[String,Set[Int]]()\n",
    "    val prior_inv = MutMap[String,Double]()\n",
    "    val supermap_inv = MutMap[String, (Map[String, Double], Double)]() // class -> term -> term.frequency map, size of all documents to that class\n",
    "    var vocabulary_size = 0\n",
    "    var vocabulary = Set[String]()\n",
    "\n",
    "/** Function to train the classifier\n",
    "* Parameters: c - Map of classes to Set of Document IDs which have that class\n",
    "              d - Map of document ID to (map of (word to count),document size, set of cllass labels)\n",
    "  Creates a map, which is used to determine the prior probability values for each class and the conditional probabilities of the token given the class label\n",
    "*/\n",
    "    def train(c: scala.collection.Map[String,scala.collection.immutable.Set[Int]], d: scala.collection.Map[Int,(scala.collection.Map[String,Int], Int, Set[String])])\n",
    "    {\n",
    "        // ttfbyc is a map for each class from terms (across all documents in that class) to term-frequency in these documents\n",
    "        // cdocsize is the size of the documents labelled with that class\n",
    "        // supermap stores both these values\n",
    "        for (cl <- c)\n",
    "        {\n",
    "            prior += cl._1 -> Math.log((cl._2.size.toDouble / d.size))\n",
    "            val ttfbyc = c(cl._1).toList.flatMap(di => d(di)._1).groupBy(x=>x._1).mapValues(x=> x.map(x=>x._2).sum.toDouble).filter(_._2>_filter_size) \n",
    "            val cdocsize = ttfbyc.values.sum\n",
    "            supermap += cl._1 -> ((ttfbyc, cdocsize))\n",
    "            vocabulary = vocabulary ++ ttfbyc.keys.toSet\n",
    "        }   \n",
    "\n",
    "        // INVERSE TRAINING\n",
    "        // create the inverse class map, i.e. the mapping of each class to its complement document set (see report)\n",
    "        \n",
    "        val c_inv = c.mapValues(t => d.keys.toSet -- t)\n",
    "        println(\"created c_inv\")\n",
    "        for (cl <- c_inv)\n",
    "        {\n",
    "            prior_inv += cl._1 -> Math.log((cl._2.size.toDouble / d.size))\n",
    "            val ttfbyc = c_inv(cl._1).take(10000).toList.flatMap(di => d(di)._1).groupBy(x=>x._1).mapValues(x=> x.map(x=>x._2).sum.toDouble).filter(_._2>_filter_size) \n",
    "            val cdocsize = ttfbyc.values.sum\n",
    "            supermap_inv += cl._1 -> ((ttfbyc, cdocsize))\n",
    "            vocabulary = vocabulary ++ ttfbyc.keys.toSet\n",
    "        }   \n",
    "        vocabulary_size = vocabulary.size\n",
    "    }\n",
    "    \n",
    "/** Function to get the conditional probability of a token given the class\n",
    "* Parameters: term - the token\n",
    "              cl - the  class\n",
    "  Determines conditional probabilities of the token given the class label\n",
    "*/\n",
    "    \n",
    "        def getCondi(term: String, cl: String) =\n",
    "    {\n",
    "        // Using the supermap, we implement Laplace smoothing using hyperparameter provided\n",
    "        val sumTf = supermap(cl)._1.getOrElse(term,0.0) + alpha\n",
    "        val sumDocSize = supermap(cl)._2 + (alpha * vocabulary_size)\n",
    "        Math.log(sumTf / sumDocSize.toDouble)\n",
    "    }\n",
    "\n",
    "        \n",
    "        def getCondi_inv(term: String, cl: String) =\n",
    "    {\n",
    "        // the same is done for the complementary set of documents\n",
    "        val sumTf = supermap_inv(cl)._1.getOrElse(term,0.0) + alpha\n",
    "        val sumDocSize = supermap_inv(cl)._2 + (alpha * vocabulary_size)\n",
    "        Math.log(sumTf / sumDocSize.toDouble)\n",
    "    }\n",
    "\n",
    "\n",
    "/** Function to create a prediction\n",
    "* Parameters: c - Map of classes to Set of Document IDs which have that class\n",
    "              d - Map of document ID to (map of (word to count),document size, set of cllass labels)\n",
    "  \n",
    "*/\n",
    "\n",
    "    \n",
    "    // two functions to help with the prediction\n",
    "    // _getFreq just creates term --> termfq map for a given document to be classified\n",
    "    // _getLabels implements cut and threshold to produce a set of labels,\n",
    "    // after the predict function has worked its magic\n",
    "    \n",
    "    // IN: Bag of Words, OUT: Term -> Term-frequency\n",
    "    def _getFreq(doc: List[String]) = doc.groupBy(identity).mapValues(l => l.length).toMap\n",
    "    \n",
    "    // IN: Label -> Probability, OUT: Set of Labels as Prediction\n",
    "    def _getLabels(res: MutMap[String, Double], threshold: Double, cut: Int) =\n",
    "    {\n",
    "       res.mapValues(x => x).filter(_._2 >= threshold).toSeq.sortBy(-_._2).take(cut).map(_._1).toSet\n",
    "    }\n",
    "\n",
    "    \n",
    "    // The actual function called on a Bag of Words to produce a prediction\n",
    "     def predict(tokens: List[String]) =\n",
    "    {\n",
    "        val doc = _getFreq(tokens)\n",
    "        val terms = doc.keys\n",
    "        \n",
    "        var probs : MutMap[String, Double] = MutMap()\n",
    "        var probs_inv : MutMap[String, Double] = MutMap()\n",
    "\n",
    "\n",
    "        // for each class, this estimates the numerator\n",
    "        for(cl <- prior)\n",
    "            {\n",
    "            // gets the prior probability for that class\n",
    "            var prob = prior(cl._1)\n",
    "      \n",
    "            // for each term in my bag of words, get the probability of that term appearing in the document given class.\n",
    "            // add all the log-probabilities together\n",
    "            \n",
    "            for(term <- terms) prob += doc(term)*getCondi(term, cl._1)\n",
    "    \n",
    "                // add the log-probability to my final map for that\n",
    "                probs += cl._1 -> prob\n",
    "            }\n",
    "        \n",
    "        // for each class, this estimates the denominator\n",
    "        for(cl <- prior_inv)\n",
    "            {\n",
    "            // gets the prior probability for that class\n",
    "            var prob_inv = prior_inv(cl._1)\n",
    "      \n",
    "            // for each term in my bag of words, get theprobability of that term appearing in the document given class.\n",
    "            // add all the log-probabilitie together\n",
    "            \n",
    "            for(term <- terms) prob_inv += doc(term)*getCondi_inv(term, cl._1)\n",
    "    \n",
    "                // add the log-probability to my final map for tha\n",
    "                probs_inv += cl._1 -> prob_inv\n",
    "            }\n",
    "        \n",
    "        // this gives the estimated log-ratio of the probabilities for each class, is input to _getLabels\n",
    "        val probabil = probs.map(t => t._1 -> (t._2 - probs_inv(t._1)))\n",
    "        \n",
    "        //probabil\n",
    "        _getLabels(probabil, threshold, cut)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// initiate a new estimator, and train the new estimator.\n",
    "val estimator = new NBayes(0.5,5,1,5)\n",
    "// train the estimator\n",
    "estimator.train(class_to_doc,documents_tf_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// import bag of words to train classifier\n",
    "val bag = MutMap[Int,List[String]]()\n",
    "for (path <- test_list)\n",
    "{\n",
    "    var ID =  numPattern.findFirstIn(path.toString).get.toInt\n",
    "    val currentdoc = new xml_doc(path.toString)\n",
    "    val bowlist = currentdoc.token_stem.toList\n",
    "    bag(ID) = bowlist   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// compute predictions and write them to file\n",
    "val pred = bag.mapValues(x=>estimator.predict(x))\n",
    "write_prediction(pred,\"ir-2016-1-project-26-nb.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Use this cell to compute average_F1_scores on the validation set or any other set if you fancy\n",
    "val bag = MutMap[Int,List[String]]()\n",
    "val truth = MutMap[Int,Set[String]]()\n",
    "\n",
    "for (path <- val_list)\n",
    "{\n",
    "    var ID =  numPattern.findFirstIn(path.toString).get.toInt\n",
    "    val currentdoc = new xml_doc(path.toString)\n",
    "    val bowlist = currentdoc.token_stem.toList\n",
    "    val labels = currentdoc.labels.toSet\n",
    "    bag(ID) = bowlist   \n",
    "    truth(ID) = labels   \n",
    "}\n",
    "    \n",
    "val scores = bag.keySet.toList.map(x=>assess(estimator.predict(bag(x)),truth(x)))\n",
    "val f1_scores = scores.map(x=>x._1);\n",
    "val avg_f1_score = f1_scores.sum/f1_scores.size"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Scala 2.11",
   "language": "scala211",
   "name": "scala211"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
